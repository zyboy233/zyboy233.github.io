<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GitHub提速</title>
    <link href="undefined2019/08/04/2019-08-04-github%E6%8F%90%E9%80%9F/"/>
    <url>2019/08/04/2019-08-04-github%E6%8F%90%E9%80%9F/</url>
    
    <content type="html"><![CDATA[<blockquote><p>自从微软收购GitHub以来，GitHub仓库同步速度抽风减少。但是从昨天开始到现在GitHub仓库同步变得巨慢，联系中美贸易战以及最近的微软发布遵守美国法律的新闻消息等等，大家都懂，看来GitHub分发服务器DNS污染严重。那咱也不能放弃，那咱们就hosts文件直接解析域名指向ip地址绕过国内DNS解析。</p></blockquote><ol><li><a href="https://www.ipaddress.com" target="_blank" rel="noopener">https://www.ipaddress.com</a></li><li>查询下面3个网址的<code>IP</code></li></ol><pre><code>    a. github.com    b. assets-cdn.github.com    c. github.global.ssl.fastly.net</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>DNS</tag>
      
      <tag>GitHub</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SS服务器部署</title>
    <link href="undefined2019/07/05/2019-07-05-SS%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2/"/>
    <url>2019/07/05/2019-07-05-SS%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<blockquote><p>你好啊</p></blockquote><blockquote><p>太长了，我决定鸽😮</p></blockquote><blockquote><p>Building。。。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>ss</tag>
      
      <tag>vultr</tag>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>celery简单使用(搬运整理)</title>
    <link href="undefined2018/11/07/2018-11-07-celery%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
    <url>2018/11/07/2018-11-07-celery%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote></blockquote><h1 id="Celery"><a href="#Celery" class="headerlink" title="Celery"></a>Celery</h1><p>Celery是一个基于Python开发的分布式异步消息队列，可以轻松实现任务的异步处理。它的基本工作就是管理分配任务到不同的服务器，并且取得结果。至于说服务器之间是如何进行通信的？这个Celery本身不能解决。</p><a id="more"></a><p>Celery在执行任务时需要一个消息中间件来接收和发送任务消息，以及存储任务结果，一般使用RabbitMQ 或 Redis，我们这里只讨论Celery+RabbitMQ，其他的组合方式可以查阅更多资料。</p><p>RabbitMQ是一个由Erlang语言开发的AMQP的开源实现。AMQP即Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。</p><p>在Celery+RabbitMQ组合中，RabbitMQ作为一个消息队列管理工具被引入到和Celery集成，负责处理服务器之间的通信任务。</p><p>那么有一个疑问：RabbitMQ作为消息管理系统已经可以实现异步的发送消息，为什么还要使用Celery?</p><p>Celery相当于包装了一个现成的系统，可以方便的在项目中操作RabbitMQ这个消息队列介质，减少在RabbitMQ上编写脚本的任务。最直接的例子就是在Celery Python里，只需要config一下settings，然后就可以用decorator轻松使用消息队列，而不用在RabbitMQ上编写复杂的脚本。</p><p>当然，Celery也支持和Redis、MongoDB之类的组合，原因是RabbitMQ尽管足够强大，但对于一些相对简单的业务环境来说可能太多（复杂）了一些。</p><!-- more --><h1 id="Celery-RabbitMQ是如何工作的？"><a href="#Celery-RabbitMQ是如何工作的？" class="headerlink" title="Celery+RabbitMQ是如何工作的？"></a>Celery+RabbitMQ是如何工作的？</h1><p>关于Celery和RabbitMQ的协作方式，可以通过工作上的一些案例来说明：</p><p>假设A公司最近在开下年度工作会议，会议上要确定下一年的工作内容和计划，参会人员有老板（下发任务者）、部门主管（Celery分配任务者）、部门员工（工作者）、老板秘书（沟通协调者RabbitMQ）。</p><p>那么这场会议首先需要确定的是下一年的具体工作内容，这里就称之为“任务内容”。比如老板说我们下一年要开发出一款社交类APP产品，部门主管表示赞同，于是便愉快地定下了具体的工作任务（task），当然开发一款社交类APP产品是这个项目的总任务，其中可以细分成很多小的任务，比如业务流程是怎么样的？界面怎么设计等。</p><p>在确定了具体工作任务后，老板便把这个项目交给了部门主管（Celery），部门主管确定部门员工中谁去完成这项任务，于是指定某个人（Worker），也可以多个人。 </p><p>发布工作任务的人是老板（下发任务者），他指定了部门主管（Celery）什么时候去完成哪些任务，并要求获取反馈信息。但有一点需要注意，老板只管布置任务，不参与具体的任务分配，这个任务分配的工作是交给部门主管（Celery）去执行。 </p><p>项目之初，老板（下发任务者）通过公司会议将任务传递给部门主管（Celery），部门主管通过部门会议将任务分配给员工（Worker），过段时间再将任务结果反馈给老板。然而随着任务越来越多，部门主管发现任务太多，每个任务都要反馈结果，记不住，也容易弄乱，导致效率下降。 </p><p>在召开会议商量了一番后，老板秘书（沟通协调者RabbitMQ）站起来说：“我有个提议，老板每天将布置的任务写成一张纸条放到我这，然后部门主管每天早上来取并交给员工，至于纸条上的任务如何分配，部门主管决定就行，但是要将结果同样写一张纸条反馈给我，我再交给老板。这样老板只负责下发任务，我只负责保管任务纸条，部门主管只负责分配任务并获取反馈，员工只负责按任务工作。大家职责都很明确，效率肯定会更高。”至此，老板与员工的沟通问题也解决了。</p><h1 id="Celery介绍和基本使用"><a href="#Celery介绍和基本使用" class="headerlink" title="Celery介绍和基本使用"></a>Celery介绍和基本使用</h1><p>Celery是一个基于Python开发的分布式异步消息任务队列，它简单、灵活、可靠，是一个专注于实时处理的任务队列，同时也支持任务调度。通过它可以轻松的实现任务的异步处理，如果你的业务场景中需要用到异步任务，就可以考虑使用Celery。举几个适用场景：</p><p>1）可以在 Request-Response 循环之外执行的操作：发送邮件、推送消息。</p><p>2）耗时的操作：调用第三方 API、视频处理（前端通过 AJAX 展示进度和结果）。</p><p>3）周期性任务：取代 crontab。</p><h2 id="Celery有以下几个优点"><a href="#Celery有以下几个优点" class="headerlink" title="Celery有以下几个优点"></a>Celery有以下几个优点</h2><p>简单：一旦熟悉了Celery的工作流程后，配置和使用是比较简单的。</p><p>高可用：当任务执行失败或执行过程中发生连接中断，Celery 会自动尝试重新执行任务。</p><p>快速：一个单进程的Celery每分钟可处理上百万个任务。</p><p>灵活： Celery的大部分组件都可以被扩展及自定制。</p><h1 id="选择Broker"><a href="#选择Broker" class="headerlink" title="选择Broker"></a>选择Broker</h1><p>Celery的基本架构和工作流程如下图<br><img src="https://box.kancloud.cn/5d8cf93818f7bc7ad2c04306c4221354_1000x115.png" srcset="/img/loading.gif" alt=""></p><p>常用的Broker有RabbitMQ、Redis、数据库等，我们这里使用的是RabbitMQ</p><p><img src="https://box.kancloud.cn/155ab4d48f8625d34409f0f6a7e8da72_1000x768.png" srcset="/img/loading.gif" alt=""></p><h1 id="Celery安装使用"><a href="#Celery安装使用" class="headerlink" title="Celery安装使用"></a>Celery安装使用</h1><p>Celery是一个Python的应用，而且已经上传到了PyPi，所以可以使用pip或easy_install安装：</p><pre><code>pip install celerypip install eventlet</code></pre><h1 id="创建Application和Task"><a href="#创建Application和Task" class="headerlink" title="创建Application和Task"></a>创建Application和Task</h1><p>Celery的默认broker是RabbitMQ，仅需配置一行就可以：</p><pre><code>broker_url = &amp;#039;amqp://guest:guest@localhost:5672//&amp;#039;</code></pre><p>创建一个Celery Application用来定义任务列表。<br>实例化一个Celery对象app，然后通过@app.task 装饰器注册一个 task。任务文件就叫tasks.py：</p><pre><code>from celery import Celeryapp = Celery(__name__, broker=&amp;#039;amqp://guest:guest@localhost:5672//&amp;#039;)@app.taskdef add(x, y):           return x + y</code></pre><h1 id="运行-worker，启动Celery-Worker来开始监听并执行任务"><a href="#运行-worker，启动Celery-Worker来开始监听并执行任务" class="headerlink" title="运行 worker，启动Celery Worker来开始监听并执行任务"></a>运行 worker，启动Celery Worker来开始监听并执行任务</h1><p>在 tasks.py 文件所在目录运行</p><pre><code>celery worker -A tasks.app -l INFO  -P eventlet</code></pre><p>这个命令会开启一个在前台运行的 worker，解释这个命令的意义：</p><p>worker: 运行 worker 模块。</p><p>-A: –app=APP, 指定使用的 Celery 实例。</p><p>-l: –loglevel=INFO, 指定日志级别，可选：DEBUG, INFO, WARNING, ERROR, CRITICAL, FATAL</p><p>其它常用的选项：</p><p>-P: –pool=prefork, 并发模型，可选：prefork (默认，multiprocessing), eventlet, gevent, threads.</p><p>-c: –concurrency=10, 并发级别，prefork 模型下就是子进程数量，默认等于 CPU 核心数</p><p>完整的命令行选项可以这样查看：</p><pre><code>celery worker --help</code></pre><h1 id="调用Task"><a href="#调用Task" class="headerlink" title="调用Task"></a>调用Task</h1><p>再打开一个终端， 进行命令行模式，调用任务</p><pre><code>from tasks import addadd.delay(1,2)add.apply_async(args=(1,2))</code></pre><p>上面两种调用方式等价，delay() 方法是 apply_async() 方法的简写。这个调用会把 add 操作放入到队列里，然后立即返回一个 AsyncResult 对象。如果关心处理结果，需要给 app 配置 CELERY_RESULT_BACKEND，指定一个存储后端保存任务的返回值。</p><h1 id="在项目中的简单使用流程"><a href="#在项目中的简单使用流程" class="headerlink" title="在项目中的简单使用流程"></a>在项目中的简单使用流程</h1><p>1）RabbitMQ所在服务器，启动crontab设置  crontable -user user -e设置定时执行celery application应用。</p><pre><code>python tasks.py day </code></pre><p>在task.py文件里面启动一个叫做app的Celery Application，编写一个app.task函数来produce 任务到rabbitmq。</p><pre><code>app = Celery()app.config_from_object(celeryconfig)</code></pre><p>在每个worker里面通过命令启动worker消费任务</p><pre><code>celery worker -A tasks.app -l INFO  -P eventlet</code></pre><h1 id="Celery介绍和基本使用-1"><a href="#Celery介绍和基本使用-1" class="headerlink" title="Celery介绍和基本使用"></a>Celery介绍和基本使用</h1><p><strong>需求场景</strong></p><ol><li><p>对100台命令执行一条批量命令，命令执行需要很长时间，但是不想让主程序等着结果返回，而是给主程序返回一个任务ID，task_id<br>主程序过一段时间根据task_id，获取执行结果即可，再命令执行期间，主程序 可以继续做其他事情</p></li><li><p>定时任务，比如每天检测一下所有的客户资料，发现是客户的生日，发个祝福短信</p></li></ol><p><strong>解决方案</strong></p><ol><li>逻辑view 中启一个进程</li></ol><p>父进程结束，子进程跟着结束，子进程任务没有完成，不符合需求</p><p>父进程结束，等着子进程结束，父进程需等着结果返回，不符合需求</p><p>小结：该方案解决不了阻塞问题，即需要等待 </p><ol start="2"><li>启动 subprocess，任务托管给操作系统执行</li></ol><p>实现task_id，实现异步，解决阻塞</p><p>小结：大批量高并发，主服务器会出现问题，解决不了并发</p><ol start="3"><li>celery</li></ol><p>celery提供多子节点，解决并发问题</p><h2 id="celery介绍"><a href="#celery介绍" class="headerlink" title="celery介绍"></a>celery介绍</h2><p>celery是一个基于python开发的分布式异步消息队列，轻松实现任务的异步处理</p><p>celery在执行任务时需要一个消息中间件来接收和发送任务消息，以及存储任务结果，一般使用RabbitMQ 或 Redis</p><h2 id="celery优点"><a href="#celery优点" class="headerlink" title="celery优点"></a>celery优点</h2><p>简单：熟悉celery的工作流程后，配置使用简单</p><p>高可用：当任务执行失败或执行过程中发生连接中断，celery会自动尝试重新执行任务</p><p>快速：一个单进程的celery每分钟可处理上百万个任务</p><p>灵活：几乎celery的各个组件都可以被扩展及自定制</p><h2 id="celery基本工作流程"><a href="#celery基本工作流程" class="headerlink" title="celery基本工作流程"></a>celery基本工作流程</h2><p><img src="https://box.kancloud.cn/32d19c7420eb9c8c072f1b38ff0f8e41_670x319.png" srcset="/img/loading.gif" alt=""><br>其中中间队列用于分配任务以及存储执行结果</p><h2 id="celery安装及使用"><a href="#celery安装及使用" class="headerlink" title="celery安装及使用"></a>celery安装及使用</h2><ol><li><p>安装python模块</p><pre><code>pip3 install celerypip3 install redis</code></pre></li><li><p>安装redis服务</p><pre><code>wget  http://download.redis.io/releases/redis-3.2.8.tar.gztar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make</code></pre></li></ol><p>src/redis-server  # 启动redis 服务</p><pre><code>3.  创建一个celery application 用来定义任务列表创建一个任务 tasks.py</code></pre><p>from celery import Celery</p><p>app = Celery(&#039;TASK&#039;,<br>             broker=&#039;amqp://guest：guest@localhost:5672//&#039;,<br>             backend=&#039;redis://localhost&#039;)</p><p>@app.task<br>def add(x,y):<br>    print(&quot;running…&quot;,x,y)<br>    return x+y</p><pre><code> 4.  启动celery worker 来开始监听并执行任务</code></pre><p> celery -A tasks worker –loglevel=info</p><pre><code>tasks 任务文件名，worker 任务角色，--loglevel=info 任务日志级别5.  调用任务打开另外终端，进入命令行模式，调用任务![](https://box.kancloud.cn/ccf1e9529cc2e20728b3e0ab1c6baa08_573x167.png)6.  celery常用接口* tasks.add(4,6) ---&amp;gt; 本地执行* tasks.add.delay(3,4) --&amp;gt; worker执行* t=tasks.add.delay(3,4)  --&amp;gt; t.get()  获取结果，或卡住，阻塞* t.ready()---&amp;gt; False：未执行完，True：已执行完* t.get(propagate=False) 抛出简单异常，但程序不会停止* t.traceback 追踪完整异常# 项目中使用Celery## 1.  项目目录结构</code></pre><p>project<br>    |– <strong>init</strong>.py<br>    |– celery.py   # 配置文档<br>    |– tasks.py    # 任务函数<br>    |– tasks2.py   # 任务函数</p><pre><code>## 2.  项目文件project/celery.py</code></pre><h1 id="from-celery-import-Celery-默认当前路径，更改为绝对路径-当前路径有个celery-py文件啦"><a href="#from-celery-import-Celery-默认当前路径，更改为绝对路径-当前路径有个celery-py文件啦" class="headerlink" title="from celery import Celery 默认当前路径，更改为绝对路径(当前路径有个celery.py文件啦)"></a>from celery import Celery 默认当前路径，更改为绝对路径(当前路径有个celery.py文件啦)</h1><p>from celery import Celery</p><p>app = Celery(&#039;project&#039;,<br>             broker=&#039;redis://localhost&#039;,<br>             backend=&#039;redis://localhost&#039;,<br>             include=[&#039;project.tasks&#039;,&#039;project.tasks2&#039;])  # 配置文件和任务文件分开了，可以写多个任务文件</p><h1 id="app-扩展配置"><a href="#app-扩展配置" class="headerlink" title="app 扩展配置"></a>app 扩展配置</h1><p>app.conf.update(<br>    result_expires=3600,<br>)</p><p>if <strong>name</strong> == &#039;<strong>main</strong>&#039;:<br>    app.start()</p><pre><code>celery.py作用相当于配置文件project/tasks.py</code></pre><p>from .celery import app</p><p>@app.task<br>def add(x, y):<br>    return x + y</p><p>@app.task<br>def mul(x, y):<br>    return x * y</p><pre><code>project/tasks.py</code></pre><p>from .celery import app</p><p>@app.task<br>def hello():<br>    return &#039;Hello World&#039;</p><pre><code>3.  启动项目worker</code></pre><p>celery -A project worker -l info</p><pre><code>另启终端，与project同目录进入python3![](https://box.kancloud.cn/4abe47350dd74f51919d94120f8a98b4_585x245.png)4.  实现分布式 当启动多个时 celery -A project worker -l info，去broker去相应任务，实现分布式5.  后台启动woker</code></pre><p>celery multi start w1 -A project -l info<br>celery multi start w2 -A project -l info<br>celery multi start w3 -A project -l info</p><p>celery multi restart w1 -A project -l info<br>celery multi stop w1 w2 w3        # 任务立刻停止<br>celery multi stopwait w1 w2 w3    # 任务执行完，停止</p><pre><code>#  Celery定时任务celery支持定时任务，设定好任务的执行时间，celery就会定时帮你执行，这个定时任务模块叫 celery beat项目目录结构</code></pre><p>project<br>    |– <strong>init</strong>.py<br>    |– celery.py          # 配置文件<br>    |– periodic_task.py   # 定时任务文件</p><pre><code>脚本celery.py</code></pre><p>from celery import Celery</p><p>app = Celery(&#039;project&#039;,<br>             broker=&#039;redis://localhost&#039;,<br>             backend=&#039;redis://localhost&#039;,<br>             include=[&#039;project.periodic_task&#039;,])</p><p>app.conf.update(<br>    result_expires=3600,<br>)</p><p>if <strong>name</strong> == &#039;<strong>main</strong>&#039;:<br>    app.start()</p><pre><code>脚本periodic_task.py</code></pre><p>from .celery import app<br>from celery.schedules import crontab</p><p>@app.on_after_configure.connect<br>def setup_periodic_tasks(sender, **kwargs):<br>    # 每10s调用 test(&#039;hello&#039;)<br>    sender.add_periodic_task(10.0, test.s(&#039;hello&#039;), name=&#039;add every 10&#039;)</p><pre><code># 每20s调用 test(&amp;#039;world&amp;#039;)sender.add_periodic_task(20.0, test.s(&amp;#039;world&amp;#039;), expires=10)# 每周一早上7:30 执行 test(&amp;#039;Happy Mondays!&amp;#039;)sender.add_periodic_task(    crontab(hour=7, minute=30, day_of_week=1), # 可灵活修改    test.s(&amp;#039;Happy Mondays!&amp;#039;),)</code></pre><p>@app.task<br>def test(arg):<br>    print(arg)</p><pre><code>启动角色 worker  执行任务</code></pre><p>celery -A project worker -l info</p><pre><code>启动角色 beat 将定时任务放到队列中</code></pre><p>celery -A  project.periodic_task  beat  -l  debug</p><pre><code>也可以在配置文件celery.py 里添加定时任务</code></pre><p>app.conf.beat_schedule = {<br>    &#039;add-every-30-seconds&#039;: {<br>        &#039;task&#039;: &#039;project.tasks.add&#039;,<br>        &#039;schedule&#039;: 30.0,<br>        &#039;args&#039;: (16, 16)<br>    },<br>}<br>app.conf.timezone = &#039;UTC&#039;</p><pre><code># Celery与Django结合</code></pre><p>LearnCelery<br>   |– app1<br>        |– tasks.py<br>        |– models.py<br>   |– app2<br>        |– tasks.py<br>        |– models.py<br>   |– LearnCelery<br>        |– <strong>init</strong>.py<br>        |– celery.py<br>        |– settings.py</p><pre><code>脚本代码LearnCelery/app/tasks.py   # 必须叫这个名字</code></pre><p>from celery import shared_task<br>import time</p><h1 id="所有的app都可以调用"><a href="#所有的app都可以调用" class="headerlink" title="所有的app都可以调用"></a>所有的app都可以调用</h1><p>@shared_task<br>def add(x, y):<br>    time.sleep(10)<br>    return x + y</p><p>@shared_task<br>def mul(x, y):<br>    time.sleep(10)<br>    return x * y</p><pre><code>LearnCelery/LearnCelery/__init__.py</code></pre><h1 id="This-will-make-sure-the-app-is-always-imported-when"><a href="#This-will-make-sure-the-app-is-always-imported-when" class="headerlink" title="This will make sure the app is always imported when"></a>This will make sure the app is always imported when</h1><h1 id="Django-starts-so-that-shared-task-will-use-this-app"><a href="#Django-starts-so-that-shared-task-will-use-this-app" class="headerlink" title="Django starts so that shared_task will use this app."></a>Django starts so that shared_task will use this app.</h1><p>from .celery import app as celery_app</p><p><strong>all</strong> = [&#039;celery_app&#039;]</p><pre><code>LearnCelery/LearnCelery/celery.py</code></pre><p>import os<br>from celery import Celery</p><h1 id="单独脚本调用Django内容时，需配置脚本的环境变量"><a href="#单独脚本调用Django内容时，需配置脚本的环境变量" class="headerlink" title="单独脚本调用Django内容时，需配置脚本的环境变量"></a>单独脚本调用Django内容时，需配置脚本的环境变量</h1><p>os.environ.setdefault(&#039;DJANGO_SETTINGS_MODULE&#039;, &#039;mysite.settings&#039;)</p><p>app = Celery(&#039;mysite&#039;)</p><h1 id="CELERY-作为前缀，在settings中写配置"><a href="#CELERY-作为前缀，在settings中写配置" class="headerlink" title="CELERY_ 作为前缀，在settings中写配置"></a>CELERY_ 作为前缀，在settings中写配置</h1><p>app.config_from_object(&#039;django.conf:settings&#039;, namespace=&#039;CELERY&#039;)</p><h1 id="到Django各个app下，自动发现tasks-py-任务脚本"><a href="#到Django各个app下，自动发现tasks-py-任务脚本" class="headerlink" title="到Django各个app下，自动发现tasks.py 任务脚本"></a>到Django各个app下，自动发现tasks.py 任务脚本</h1><p>app.autodiscover_tasks()</p><p>@app.task(bind=True)<br>def debug_task(self):<br>    print(&#039;Request: {0!r}&#039;.format(self.request))</p><pre><code>LearnCelery/LearnCelery/settings.py</code></pre><h1 id="For-celery"><a href="#For-celery" class="headerlink" title="For celery"></a>For celery</h1><p>CELERY_BROKER_URL = &#039;redis://localhost&#039;<br>CELERY_RESULT_BACKEND = &#039;redis://localhost&#039;</p><pre><code>3.  启动celery</code></pre><p>celery -A LearnCelery worker -l debug</p><pre><code>4.  urls.py 视图处理</code></pre><p>urlpatterns = [<br>    url(r&#039;^celery_call/$&#039;, views.celery_call),<br>    url(r&#039;^celery_res/$&#039;, views.celery_res),<br>]</p><pre><code># django中使用计划任务1.  安装插件</code></pre><p>pip3 install django-celery-beat</p><pre><code>2.  修改配置 settings.py</code></pre><p>INSTALLED_APPS = [<br> &#039;django_celery_beat&#039;,<br>]</p><pre><code>3.数据库迁移</code></pre><p>python manage.py migrate</p><pre><code>4.  启动 celery beat</code></pre><p>celery -A LearnCelery beat -l info -S django</p><p>```<br>定时任务存到数据库里，启动beat定时取任务放到队列里执行</p><ol start="5"><li>admin管理<br><img src="https://box.kancloud.cn/33d360707ed8e3019c407402435e8eac_875x323.png" srcset="/img/loading.gif" alt=""></li></ol><p><img src="https://box.kancloud.cn/b884ec26e2cd1c98d8bf5c480e91a478_1680x1594.png" srcset="/img/loading.gif" alt=""></p><p>此时启动你的celery beat 和worker，会发现每隔2分钟，beat会发起一个任务消息让worker执行scp_task任务</p><p>注意，经测试，每添加或修改一个任务，celery beat都需要重启一次，要不然新的配置不会被celery beat进程读到</p>]]></content>
    
    
    
    <tags>
      
      <tag>web</tag>
      
      <tag>linux</tag>
      
      <tag>MQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis基本命令(更新)</title>
    <link href="undefined2018/11/06/2018-08-14-redis%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"/>
    <url>2018/11/06/2018-08-14-redis%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<p><strong>Redis 是一个速度非常快的非关系型数据库，使用内存作为主存储，内存中的数据也可以被持久化到硬盘。Redis以键值对形式(key-value)存储数据，其中值可以分为以下5中类型：</strong></p><ul><li><a href="#字符串">字符串(string)</a></li><li><a href="#列表">列表(list)</a></li><li><a href="#哈希">哈希(hash)</a></li><li><a href="#集合">集合(set)</a></li><li><a href="#有序集合">有序集合(zset)</a></li><li><a href="#key">key</a></li></ul><a id="more"></a><h3 id="Redis-基本命令"><a href="#Redis-基本命令" class="headerlink" title="Redis 基本命令"></a>Redis 基本命令</h3><h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><p>Redis的字符串(string)可以存储字符串、整数、浮点数,值最大存储512m。String命令及描述如下表所示：</p><table><thead><tr><th>String命令</th><th>描 述</th></tr></thead><tbody><tr><td>set key value</td><td>设置字符串key的值</td></tr><tr><td>get key</td><td>获取字符串key的值</td></tr><tr><td>del key</td><td>删除key</td></tr><tr><td>strlen key</td><td>获取值长度</td></tr><tr><td>mset key1 value1 key2 value2…</td><td>设置多个值</td></tr><tr><td>mget key1 key2…</td><td>获取多个值</td></tr><tr><td>append key value</td><td>追加值</td></tr><tr><td>incr key</td><td>将key对应的值加一</td></tr><tr><td>decr key</td><td>将key对应的值减一</td></tr><tr><td>incrby key intnum</td><td>将key对应的值加整数</td></tr><tr><td>decrby key intnum</td><td>将key对应的值减整数</td></tr></tbody></table><pre><code class="bash">127.0.0.1:6379&gt; mset age 30 sex manOK127.0.0.1:6379&gt; mget age sex1) &quot;30&quot;2) &quot;man&quot;127.0.0.1:6379&gt; append name lisi(integer) 4127.0.0.1:6379&gt; get name&quot;lisi&quot;127.0.0.1:6379&gt; strlen name(integer) 4127.0.0.1:6379&gt; incr age(integer) 31127.0.0.1:6379&gt; incrby age 10(integer) 41127.0.0.1:6379&gt; decr age(integer) 40127.0.0.1:6379&gt; decrby age 20(integer) 20127.0.0.1:6379&gt;</code></pre><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><p>Redis的列表(list)可以有序(按照插入顺序排序)地存储多个字符串。List命令及描述如下所示：</p><blockquote><p>索引值从0开始 ，可以为负数</p></blockquote><table><thead><tr><th>List命令</th><th>描 述</th></tr></thead><tbody><tr><td>lpush key1 value1 value2 …</td><td>在列表key左端插入一个或者多个值</td></tr><tr><td>rpush key1 value1 value2 …</td><td>在列表key又端插入一个或者多个值</td></tr><tr><td>linsert key before/after pivot value</td><td>在一个元素的前/后插入新的元素</td></tr><tr><td>lset key index value</td><td>设置指定索引的元素值</td></tr><tr><td>lpop key</td><td>在列表key左端弹出一个值</td></tr><tr><td>rpop key</td><td>在列表key右端弹出一个值</td></tr><tr><td>llen key</td><td>获取列表key的长度</td></tr><tr><td>lindex key index</td><td>获取列表key中index位置的值</td></tr><tr><td>lrange key start end</td><td>获取列表key中位置在[start,end]范围的值</td></tr><tr><td>ltrim key start end</td><td>裁剪列表，改为元集合的一个子集</td></tr></tbody></table><pre><code class="bash">127.0.0.1:6379&gt; lpush fond study game sing(integer) 3127.0.0.1:6379&gt; lrange fond 0 -11) &quot;sing&quot;2) &quot;game&quot;3) &quot;study&quot;127.0.0.1:6379&gt; rpush friends Jane Jack(integer) 2127.0.0.1:6379&gt; linsert friends after Jack kangkang(integer) 3127.0.0.1:6379&gt; lrange friends 0 -11) &quot;Jane&quot;2) &quot;Jack&quot;3) &quot;kangkang&quot;127.0.0.1:6379&gt; lset fond 2 swimOK127.0.0.1:6379&gt; lpop fond&quot;sing&quot;127.0.0.1:6379&gt; rpop friends&quot;kangkang&quot;127.0.0.1:6379&gt; lrange fond 0 -11) &quot;game&quot;2) &quot;swim&quot;127.0.0.1:6379&gt; llen fond(integer) 2127.0.0.1:6379&gt; lindex fond 0&quot;game&quot;127.0.0.1:6379&gt; ltrim friends 0 2OK127.0.0.1:6379&gt; lrange friends 0 -11) &quot;Jane&quot;2) &quot;Jack&quot;</code></pre><h4 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h4><p>Redis的哈希(Hash)可以存储多个键值对，其中的键和值都是字符串。Hash命令及描述如下表所示：</p><table><thead><tr><th>Hash命令</th><th>描述</th></tr></thead><tbody><tr><td>hset key field value</td><td>将哈希key的field字段赋值为value</td></tr><tr><td>hmset key field1 value1 field2 value2…</td><td>设置多个值</td></tr><tr><td>hdel key field1 field2…</td><td>删除哈希key的一个或多个字段</td></tr><tr><td>hget key field</td><td>获取哈希key的field字段的值</td></tr><tr><td>hmget key field1 field2…</td><td>获取多个属性的值</td></tr><tr><td>hgetall key</td><td>获取哈希key的所有字段和值</td></tr><tr><td>hkeys key</td><td>获取所有属性</td></tr><tr><td>hvals key</td><td>获取所有值</td></tr><tr><td>hlen  key</td><td>返回包含数据的个数</td></tr><tr><td>hexists key field</td><td>判断属性是否存在，存在返回1，不存在返回0</td></tr><tr><td>hstrlen key field</td><td>返回值的字符串长度</td></tr></tbody></table><pre><code class="bash">127.0.0.1:6379&gt; hset point x 10(integer) 1127.0.0.1:6379&gt; hmset point y 20OK127.0.0.1:6379&gt; hget point y&quot;20&quot;127.0.0.1:6379&gt; hmget point(error) ERR wrong number of argu127.0.0.1:6379&gt; hmget point x z1) &quot;10&quot;2) &quot;30&quot;127.0.0.1:6379&gt; hgetall point1) &quot;x&quot;2) &quot;10&quot;3) &quot;y&quot;4) &quot;20&quot;5) &quot;z&quot;6) &quot;30&quot;127.0.0.1:6379&gt; hkeys point1) &quot;x&quot;2) &quot;y&quot;3) &quot;z&quot;127.0.0.1:6379&gt; hvals point1) &quot;10&quot;2) &quot;20&quot;3) &quot;30&quot;127.0.0.1:6379&gt; hlen point(integer) 3127.0.0.1:6379&gt; hexists point w(integer) 0127.0.0.1:6379&gt; hstrlen point x(integer) 2127.0.0.1:6379&gt; hdel point x y z(integer) 3</code></pre><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><p>Redis中的集合(set)可以存储多个唯一的字符串,是string类型的无序集合。set命令及描述如下表所示：</p><table><thead><tr><th>set命令</th><th>描 述</th></tr></thead><tbody><tr><td>sadd key member1 member2…</td><td>向集合key中添加一个或多个成员</td></tr><tr><td>srem key member1 member2…</td><td>删除集合key中一个或者多个成员</td></tr><tr><td>smembers key</td><td>获取集合key中所有成员</td></tr><tr><td>scard key</td><td>获取集合key中成员数量</td></tr><tr><td>sismember key menber</td><td>判断member是否是集合key的成员</td></tr><tr><td>sinter key1 key2…</td><td>求多个集合的交集</td></tr><tr><td>sdiff  key1 key2…</td><td>求多个集合的差集</td></tr><tr><td>sunion key1 key2…</td><td>求多个集合的合集</td></tr></tbody></table><pre><code class="bash">127.0.0.1:6379&gt; sadd color red black blue whi(integer) 4127.0.0.1:6379&gt; smembers color1) &quot;black&quot;2) &quot;white&quot;3) &quot;blue&quot;4) &quot;red&quot;127.0.0.1:6379&gt; scard color(integer) 4127.0.0.1:6379&gt; sismember color yellow(integer) 0127.0.0.1:6379&gt; sadd colors red blue yellow p(integer) 4127.0.0.1:6379&gt; sinter color colors1) &quot;blue&quot;2) &quot;red&quot;127.0.0.1:6379&gt; sdiff color colors1) &quot;black&quot;2) &quot;white&quot;127.0.0.1:6379&gt; sunion color colors1) &quot;yellow&quot;2) &quot;black&quot;3) &quot;pink&quot;4) &quot;white&quot;5) &quot;blue&quot;6) &quot;red&quot;127.0.0.1:6379&gt; srem color red black(integer) 2</code></pre><h4 id="有序集合"><a href="#有序集合" class="headerlink" title="有序集合"></a>有序集合</h4><p>Redis中的有序集合(ZSet)与集合(Set)类似，可以存储多个唯一的字符串，但在有序集合中，每个成员都有一个分数(score)，所有成员按给定分数在集合中由小到大有序排列。zset中成员是唯一的,但是分数(score)可以相同.Zset命令及描述如下图所示：</p><table><thead><tr><th>Zset命令</th><th>描 述</th></tr></thead><tbody><tr><td>zadd key score1 member1 score2 member2…</td><td>向有序集合key中添加一个或多个成员</td></tr><tr><td>zrem key member member2 …</td><td>删除有序集合key中一个或多个成员</td></tr><tr><td>zrange key start stop</td><td>获取有序集合key中位置在[start,end]范围的所有成员</td></tr><tr><td>zrangebyscore key min max</td><td>获取有序集合key中分值在[min,max]范围的所有成员</td></tr><tr><td>zcount key key min max</td><td>获取有序集合key中分值在[min,max]范围的个数</td></tr><tr><td>zcard key</td><td>返回元素的个数</td></tr><tr><td>zscore key member</td><td>返回有序集合key中，成员member的分值</td></tr></tbody></table><pre><code class="bash">127.0.0.1:6379&gt; zadd country 1 China 2 Russia 3 India 4 France(integer) 5127.0.0.1:6379&gt; zrange country 0 -11) &quot;China&quot;2) &quot;Russia&quot;3) &quot;India&quot;4) &quot;France&quot;5) &quot;Italy&quot;127.0.0.1:6379&gt; zrangebyscore country 1 31) &quot;China&quot;2) &quot;Russia&quot;3) &quot;India&quot;127.0.0.1:6379&gt; zcount country 1 4(integer) 4127.0.0.1:6379&gt; zcard country(integer) 5127.0.0.1:6379&gt; zscore country China&quot;1&quot;</code></pre><h4 id="key"><a href="#key" class="headerlink" title="key"></a>key</h4><table><thead><tr><th>key命令</th><th>描 述</th></tr></thead><tbody><tr><td>keys pattern</td><td>查找键，参数支持正则</td></tr><tr><td>exists key</td><td>判断键是否存在，如果存在返回1，不存在返回0</td></tr><tr><td>type key</td><td>查看键及对应的值</td></tr><tr><td>del key1 key2 …</td><td>删除键及对应的值</td></tr><tr><td>expire key seconds</td><td>设置过期时间，以秒为单位</td></tr><tr><td>ttl key</td><td>查看有效时间，以秒为单位</td></tr></tbody></table><blockquote><ul><li>更多命令请参考官网 <a href="https://redis.io" target="_blank" rel="noopener">https://redis.io</a></li></ul></blockquote><!-- <meta http-equiv="refresh" content="3"> -->]]></content>
    
    
    
    <tags>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rabbitMQ多个接收端消费信息(搬运整理)</title>
    <link href="undefined2018/11/05/2018-11-05-rabbitMQ%E5%A4%9A%E4%B8%AA%E6%8E%A5%E6%94%B6%E7%AB%AF%E6%B6%88%E8%B4%B9%E4%BF%A1%E6%81%AF/"/>
    <url>2018/11/05/2018-11-05-rabbitMQ%E5%A4%9A%E4%B8%AA%E6%8E%A5%E6%94%B6%E7%AB%AF%E6%B6%88%E8%B4%B9%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="循环分发"><a href="#循环分发" class="headerlink" title="循环分发"></a>循环分发</h1><p>启动一个发送端往队列发消息，此时启动多个接收端。发送的消息会对接收端一个一个挨着发送消息。</p><p>这就是默认情况下，多个接收端轮流消费消息。队列发送给消费端后，就立即删除。那么问题来了，当某个消费者在处理消息的时候，异常终止了怎么办？此时，我们更希望这样：若是那个消费者挂掉了，消息自动转给另一个消费者处理。</p><a id="more"></a><p>幸好，rabbitmq就有效确认机制。消费者收到消息后，正常处理完成，此时才通知队列可以自由删除。那么问题又来了，消费者挂掉了连确认消息都发不出，该怎么办？rabbitmq维持了消费者的连接信息。消费者挂掉，与server的连接通道会关闭或tcp连接丢失。这时server知道了这个情况，就自动重发消息。</p><p>这里还有个问题，就是server挂掉了怎么办？<strong>注意： durable=True。这个就是，当server挂了队列还存在。delivery_mode=2：server挂了消息还存在。若是保证消息不丢，这两个参数都要设置。</strong></p><p>发送端</p><pre><code>import pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;localhost&amp;#039;))channel = connection.channel()# durable：server挂了队列仍然存在channel.queue_declare(queue=&amp;#039;task_queue&amp;#039;, durable=True)# 使用默认的交换机发送消息。exchange为空就使用默认的。delivery_mode=2：使消息持久化。和队列名称绑定routing_keymessage = &amp;#039; &amp;#039;.join(sys.argv[1:]) or &amp;quot;Hello World!&amp;quot;channel.basic_publish(exchange=&amp;#039;&amp;#039;,routing_key=&amp;#039;task_queue&amp;#039;,body=message,properties=pika.BasicProperties(delivery_mode=2,))print(&amp;quot; [x] Sent %r&amp;quot; % message)connection.close()</code></pre><p>接收端</p><pre><code>import pikaimport timeconnection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;localhost&amp;#039;))channel = connection.channel()channel.queue_declare(queue=&amp;#039;task_queue&amp;#039;, durable=True)print(&amp;#039; [*] Waiting for messages. To exit press CTRL+C&amp;#039;)def callback(ch, method, properties, body):    print(&amp;quot; [x] Received %r&amp;quot; % body)    time.sleep(body.count(b&amp;#039;.&amp;#039;))    print(&amp;quot; [x] Done&amp;quot;)    # 手动对消息进行确认    ch.basic_ack(delivery_tag=method.delivery_tag)# basic_consume：这个函数有no_ack参数。该参数默认为false。表示：需要对message进行确认。怎么理解：no设置成false，表示要确认channel.basic_consume(callback, queue=&amp;#039;task_queue&amp;#039;)channel.start_consuming()</code></pre><h1 id="公平派遣"><a href="#公平派遣" class="headerlink" title="公平派遣"></a>公平派遣</h1><p>此刻，我们已经知道如何保证消息不丢，那么问题又来了。有的消费干得快，有的干得慢。这样分发消息，有的累死有的没事干。这个问题如何解决？</p><p><img src="https://box.kancloud.cn/c967559388971bc1277b7021edfcf1c9_396x111.png" srcset="/img/loading.gif" alt=""></p><p>rabbitmq已经考虑到了。那就是：那个干完了，通知给server，server就发送给那个。<br>在上面的接收端的</p><pre><code>channel.basic_consume(callback, queue=&amp;#039;task_queue&amp;#039;)</code></pre><p>代码前加：</p><pre><code>channel.basic_qos(prefetch_count=1)</code></pre><h1 id="发布订阅模式"><a href="#发布订阅模式" class="headerlink" title="发布订阅模式"></a>发布订阅模式</h1><p>我们要将同一个消息发给多个客户端。</p><p>发送端：</p><pre><code>import pikaimport syscredentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))channel = connection.channel()# 原则上，消息，只能有交换机传到队列。就像我们家里面的交换机道理一样。# 有多个设备连接到交换机，那么，这个交换机把消息发给那个设备呢，就是根据# 交换机的类型来定。类型有：direct\topic\headers\fanout# fanout：这个就是，所有的设备都能收到消息，就是广播。# 此处定义一个名称为&amp;#039;logs&amp;#039;的&amp;#039;fanout&amp;#039;类型的exchangechannel.exchange_declare(exchange=&amp;#039;logs&amp;#039;,                         exchange_type=&amp;#039;fanout&amp;#039;)# 将消息发送到名为log的exchange中# 因为是fanout类型的exchange，所以无需指定routing_keymessage =  &amp;quot;info: Hello World!&amp;quot;channel.basic_publish(exchange=&amp;#039;logs&amp;#039;,                      routing_key=&amp;#039;&amp;#039;,                      body=message)print(&amp;quot; [x] Sent %r&amp;quot; % message)connection.close()</code></pre><p>接收端：</p><pre><code>import pikacredentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))channel = connection.channel()# 这里需要和发送端保持一致（习惯和要求）channel.exchange_declare(exchange=&amp;#039;logs&amp;#039;,                         exchange_type=&amp;#039;fanout&amp;#039;)# 类似的，比如log，我们其实最想看的，当连接上的时刻到消费者退出，这段时间的日志# 有些消息，过期了的对我们并没有什么用# 并且，一个终端，我们要收到队列的所有消息，比如：这个队列收到两个消息，一个终端收到一个。# 我们现在要做的是：两个终端都要收到两个# 那么，我们就只需做个临时队列。消费端断开后就自动删除result = channel.queue_declare(exclusive=True)# 取得队列名称queue_name = result.method.queue# 将队列和交换机绑定一起channel.queue_bind(exchange=&amp;#039;logs&amp;#039;,                   queue=queue_name)print(&amp;#039; [*] Waiting for logs. To exit press CTRL+C&amp;#039;)def callback(ch, method, properties, body):    print(&amp;quot; [x] %r&amp;quot; % body)# no_ack=True:此刻没必要回应了channel.basic_consume(callback,                      queue=queue_name,                      no_ack=True)channel.start_consuming()</code></pre><h1 id="根据类型订阅消息"><a href="#根据类型订阅消息" class="headerlink" title="根据类型订阅消息"></a>根据类型订阅消息</h1><p>一些消息，仍然发送给所有接收端。其中，某个接收端，只对其中某些消息感兴趣，它只想接收这一部分消息。如下图：C1，只对error感兴趣，C2对其他三种甚至对所有都感兴趣，我们该怎么搞呢？</p><p><img src="https://box.kancloud.cn/65d418b54973525e03432d314234141d_423x171.png" srcset="/img/loading.gif" alt=""></p><p>发送端：</p><pre><code>import pikaimport syscredentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))channel = connection.channel()# 创建一个交换机：direct_logs 类型是：directchannel.exchange_declare(exchange=&amp;#039;direct_logs&amp;#039;, exchange_type=&amp;#039;direct&amp;#039;)severity = sys.argv[1] if len(sys.argv) &amp;gt; 1 else &amp;#039;info&amp;#039;message = &amp;#039; &amp;#039;.join(sys.argv[2:]) or &amp;#039;Hello World!&amp;#039;# 向exchage按照设置的 routing_key=severity 发送messagechannel.basic_publish(exchange=&amp;#039;direct_logs&amp;#039;,                      routing_key=severity,                      body=message)print(&amp;quot; [x] Sent %r:%r&amp;quot; % (severity, message))connection.close()</code></pre><p>接收端：</p><pre><code>import pikaimport syscredentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))channel = connection.channel()# 跟发送端一致channel.exchange_declare(exchange=&amp;#039;direct_logs&amp;#039;,                         exchange_type=&amp;#039;direct&amp;#039;)# 还是声明临时队列result = channel.queue_declare(exclusive=True)queue_name = result.method.queueseverities = sys.argv[1:]if not severities:    sys.stderr.write(&amp;quot;Usage: %s [info] [warning] [error]\n&amp;quot; % sys.argv[0])    sys.exit(1)# 使用routing_key绑定交换机和队列。广播类型，无需使用这个# direct类型：会对消息进行精确匹配# 对个队列使用相同路由key是可以的for severity in severities:    channel.queue_bind(exchange=&amp;#039;direct_logs&amp;#039;,                       queue=queue_name,                       routing_key=severity)print(&amp;#039; [*] Waiting for logs. To exit press CTRL+C&amp;#039;)def callback(ch, method, properties, body):    print(&amp;quot; [x] %r:%r&amp;quot; % (method.routing_key, body))channel.basic_consume(callback,                      queue=queue_name,                      no_ack=True)channel.start_consuming()</code></pre><h1 id="进行RPC调用"><a href="#进行RPC调用" class="headerlink" title="进行RPC调用"></a>进行RPC调用</h1><p>RPC：是远程过程调用。简单点说：比如，我们在本地的代码中调用一个函数，那么这个函数不一定有返回值, 但一定有返回。若是在分布式环境中，前面的例子，发送消息出去后，发送端是不清楚客户端处理完后的结果的。由于rabbitmq的响应机制，顶多能获取到客户端的处理状态，但并不能获取处理结果。那么，我们想像本地调用那样，需要客户端处理后返回结果该怎么办呢。就是如下图：<br><img src="https://box.kancloud.cn/2e78d61bca855f6c743a3b7fe0e26eaf_576x200.png" srcset="/img/loading.gif" alt=""></p><p>client发送请求，同时告诉server处理完后要发送消息给：回调队列的ID：correlation_id=abc，并调用replay_to回调队列对应的回调函数。</p><h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><p>客户端：发消息也收消息</p><pre><code>import pikaimport uuidclass FibonacciRpcClient(object):    def __init__(self):        # 创建连接        credentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)        self.connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))        self.channel = self.connection.channel()        # 创建回调队列        result = self.channel.queue_declare(exclusive=True)        self.callback_queue = result.method.queue        # 这里：这个是消息发送方，当要执行回调的时候，它又是接收方        # 使用callback_queue 实现消息接收。即是回调。注意：这里的回调        # 不需要对消息进行确认。反复确认，没玩没了就成了死循环        #这里设置回调        self.channel.basic_consume(self.on_response, no_ack=True,                                   queue=self.callback_queue)    # 定义回调的响应函数。    # 判断：若是当前的回调ID和响应的回调ID相同，即表示，是本次请求的回调    # 原因：若是发起上百个请求，发送端总得知道回来的对应的是哪一个发送的    def on_response(self, ch, method, props, body):        if self.corr_id == props.correlation_id:            self.response = body    def call(self, n):        # 设置响应和回调通道的ID        self.response = None        self.corr_id = str(uuid.uuid4())        # properties中指定replay_to：表示回调要调用那个函数        # 指定correlation_id：表示回调返回的请求ID是那个        # body：是要交给接收端的参数        self.channel.basic_publish(exchange=&amp;#039;&amp;#039;,                                   routing_key=&amp;#039;rpc_queue&amp;#039;,                                   properties=pika.BasicProperties(reply_to=self.callback_queue,                                                                   correlation_id=self.corr_id,),                                   body=str(n))        # 监听回调        while self.response is None:            self.connection.process_data_events()        # 返回的结果是整数，这里进行强制转换        return int(self.response)fibonacci_rpc = FibonacciRpcClient()print(&amp;quot; [x] Requesting fib(30)&amp;quot;)response = fibonacci_rpc.call(30)print(&amp;quot; [.] Got %r&amp;quot; % response)</code></pre><p>服务端：</p><pre><code>import pikacredentials = pika.PlainCredentials(&amp;quot;yang&amp;quot;, &amp;quot;123456&amp;quot;)connection = pika.BlockingConnection(pika.ConnectionParameters(host=&amp;#039;192.168.2.176&amp;#039;, credentials=credentials))channel = connection.channel()channel.queue_declare(queue=&amp;#039;rpc_queue&amp;#039;)def fib(n):    if n == 0:        return 0    elif n == 1:        return 1    else:        return fib(n - 1) + fib(n - 2)def on_request(ch, method, props, body):    #收到的消息    n = int(body)    print(&amp;quot; [.] fib(%s)&amp;quot; % n)    #要处理的任务    response = fib(n)    #发布消息。通知到客户端    ch.basic_publish(exchange=&amp;#039;&amp;#039;,                     routing_key=props.reply_to,                     properties=pika.BasicProperties(correlation_id= props.correlation_id),                     body=str(response))    #手动响应    ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_qos(prefetch_count=1)channel.basic_consume(on_request, queue=&amp;#039;rpc_queue&amp;#039;)print(&amp;quot; [x] Awaiting RPC requests&amp;quot;)channel.start_consuming()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>web</tag>
      
      <tag>linux</tag>
      
      <tag>MQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RabbitMQ概述和运行机制(搬运整理)</title>
    <link href="undefined2018/11/03/2018-11-2-RabbitMQ%E6%A6%82%E8%BF%B0%E5%92%8C%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/"/>
    <url>2018/11/03/2018-11-2-RabbitMQ%E6%A6%82%E8%BF%B0%E5%92%8C%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<!-- #  RabbitMQ概述和运行机制 --><h1 id="RabbitMQ概述-搬运"><a href="#RabbitMQ概述-搬运" class="headerlink" title="RabbitMQ概述[搬运]"></a>RabbitMQ概述<code>[搬运]</code></h1><p><strong>RabbitMQ概述</strong>：RabbitMQ是使用最广泛的开源消息代理。RabbitMQ轻量级，易于在集群内部和云平台中部署。它支持多种消息传递协议。 它可以满足企业高规模，高可用性的要求。RabbitMQ使用Erlang语言开发的。</p><a id="more"></a><p><strong>MQ概述</strong>：全称为Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。<br><strong>MQ运行机制</strong>： P表示生产者，C表示消费者，红色部分为消息队列</p><p><img src="https://box.kancloud.cn/a02211c868ca3192b4f7f8d9af4bbd7c_536x86.png" srcset="/img/loading.gif" alt=""></p><h2 id="MQ实战场景："><a href="#MQ实战场景：" class="headerlink" title="MQ实战场景："></a>MQ实战场景：</h2><p>1.我们在双11的时候，当我们凌晨大量的秒杀和抢购商品，然后去结算的时候，就会发现，界面会提醒我们，让我们稍等，以及一些友好的图片文字提醒。而不是像前几年的时代，动不动就页面卡死，报错等来呈现给用户。在这个业务场景中，我们就可以采用队列的机制来处理，因为同时结算就只能达到这么多。</p><p><img src="https://box.kancloud.cn/5534033654975dec4f1aa1f746452ee6_869x420.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://box.kancloud.cn/a642a8065db7f6b250bbd8ba04073ab0_680x344.jpg" srcset="/img/loading.gif" alt=""></p><p>2.在我们平时的超市中购物也是一样，当我们在结算的时候，并不会一窝蜂一样涌入收银台，而是排队结算。这也是队列机制。一个接着一个的处理，不能插队。</p><p><img src="https://box.kancloud.cn/0df7fc413ae1f796d04d0f6f052c3a11_1024x683.jpg" srcset="/img/loading.gif" alt=""></p><hr><p><strong>RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。</strong></p><p>RabbitMQ是AMQP服务器的一种。<br><strong>AMQP简介</strong>：AMQP，即Advanced Message Queuing Protocol，<strong>高级消息队列协议</strong>，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦，消息的发送者无需知道消息使用者的存在，反之亦然。 AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。<br>它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，<strong>并不受产品、开发语言等条件的限制。</strong></p><p>AMQP 里主要要说两个组件：Exchange 和 Queue （在 AMQP 1.0 里还会有变动），如下图所示，绿色的 X 就是 Exchange ，红色的是 Queue ，这两者都在 Server 端，又称作 Broker ，这部分是 RabbitMQ 实现的，而蓝色的则是客户端，通常有 Producer（生产者） 和 Consumer（消费者） 两种类型：<br><img src="https://box.kancloud.cn/d55ac336d3c298d60fcf5e26e6c6fdf1_600x450.png" srcset="/img/loading.gif" alt=""><br>Publisher<br>消息的生产者，也是一个向交换器发布消息的客户端应用程序。<br>Exchange<br>交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。<br>Queue<br>消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。<br>Consumer<br>消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。<br>Broker<br>表示消息队列服务器实体。</p><p>RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：</p><ul><li><p>可靠性（Reliability）<br>RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。</p></li><li><p>灵活的路由（Flexible Routing）<br>在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。</p></li><li><p>消息集群（Clustering）<br>多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。</p></li><li><p>高可用（Highly Available Queues）<br>队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。</p></li><li><p>多种协议（Multi-protocol）<br>RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。</p></li><li><p>多语言客户端（Many Clients）<br>RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。</p></li><li><p>管理界面（Management UI）<br>RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</p></li><li><p>跟踪机制（Tracing）<br>如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。</p></li><li><p>插件机制（Plugin System）<br>RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</p></li></ul><h1 id="RabbitMQ-安装"><a href="#RabbitMQ-安装" class="headerlink" title="RabbitMQ 安装"></a>RabbitMQ 安装</h1><p>RabbitMQ是用erlang语言编写的，所以我们先安装erlang语言环境<br>配置erlang语言环境</p><pre><code>vim /etc/yum.repos.d/rabbitmq-erlang.repo# 在rabbitmq-erlang.repo 文件中加入下面的代码[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq/rpm/erlang/20/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascenabled=1# 执行导入keyrpm --import  https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.asc# 安装erlangyum install erlang -y  #安装erlang</code></pre><p><strong>安装rabbitmq服务</strong></p><p>下载rabbitmq  地址：<a href="http://www.rabbitmq.com/download.html" target="_blank" rel="noopener">http://www.rabbitmq.com/download.html</a></p><pre><code>wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.8/rabbitmq-server-3.7.8-1.el7.noarch.rpmyum install  rabbitmq-server-3.7.8-1.el7.noarch.rpm</code></pre><p>启用RabbitMQ的web插件 ，方便后期管理界面</p><pre><code>rabbitmq-plugins enable rabbitmq_management</code></pre><pre><code>The following plugins have been configured:  rabbitmq_management  rabbitmq_management_agent  rabbitmq_web_dispatchApplying plugin configuration to rabbit@localhost...The following plugins have been enabled:  rabbitmq_management  rabbitmq_management_agent  rabbitmq_web_dispatchset 3 plugins.Offline change; changes will take effect at broker restart.</code></pre><p>设置开机启动</p><pre><code>systemctl enable rabbitmq-server.service</code></pre><p>启动服务</p><pre><code>systemctl start rabbitmq-server</code></pre><p>rabbitmq配置文件位置</p><pre><code> ls /var/lib/rabbitmq/mnesia</code></pre><p>访问控制台<br>默认用户名和密码： guest/guest 。guest用户仅允许从在服务器以localhost或127.0.0.1作为ip登录</p><p>如果远程登录，如：<a href="http://192.168.1.63:15672/" target="_blank" rel="noopener">http://192.168.1.63:15672/</a>, 则会提示错误，登录不了。</p><p>为RabbitMQ创建用户并赋权。</p><pre><code>rabbitmqctl add_user root 123456  #添加用户rabbitmqctl set_user_tags root administrator #设置用户权限为administrator</code></pre><p>到此，已经搭建成功。</p><h1 id="RabbitMQ使用方法"><a href="#RabbitMQ使用方法" class="headerlink" title="RabbitMQ使用方法"></a>RabbitMQ使用方法</h1><p><strong>RabbitMQ查看相关的命令</strong></p><pre><code> rabbitmqctl list_connections   #用于查看当前的连接rabbitmqctl list_queues   #会列出所有队列名称，后边可能还会带着这个队列当前消息数 rabbitmqctl status        #查看当前队列信息</code></pre><h2 id="RabbitMQ的vhost管理"><a href="#RabbitMQ的vhost管理" class="headerlink" title="RabbitMQ的vhost管理"></a>RabbitMQ的vhost管理</h2><p>当我们在创建用户时，会指定用户能访问一个虚拟机，并且该用户只能访问该虚拟机下的队列和交换机，如果没有指定，默认的是”/”;一个rabbitmq服务器上可以运行多个vhost，以便于适用不同的业务需要，这样做既可以满足权限配置的要求，也可以避免不同业务之间队列、交换机的命名冲突问题，因为不同vhost之间是隔离的。</p><p>添加yang-web和yang-bbs两个虚拟机来管理网站和论坛的队列</p><pre><code>rabbitmqctl add_vhost yang-webrabbitmqctl add_vhost yang-bbs</code></pre><p>查看创建的虚拟主机 网页查看</p><p>删除bbs虚拟机</p><pre><code>rabbitmqctl delete_vhost yang-bbs</code></pre><p>查看虚拟机列表</p><pre><code>rabbitmqctl list_vhosts</code></pre><p><strong>“/”是rabbitmq默认的虚拟机，之前默认连接的都是它</strong></p><h1 id="RabbitMQ管理用户、角色和权限管理"><a href="#RabbitMQ管理用户、角色和权限管理" class="headerlink" title="RabbitMQ管理用户、角色和权限管理"></a>RabbitMQ管理用户、角色和权限管理</h1><p>1、用户管理语法<br>添加用户：rabbitmqctl add_user {username} {password}<br>删除用户：rabbitmqctl delete_user {username}<br>修改密码：rabbitmqctl change_password {username} {newpassword} </p><p>2、角色权限分配<br> 设置用户角色语法：rabbitmqctl set_user_tags {username} {tag}<br>RabbitMQ的tag用户角色分类：none、management、policymaker、monitoring、administrator<br>tag常用角色为：administrator, monitoring, management </p><h3 id="RabbitMQ各类角色描述："><a href="#RabbitMQ各类角色描述：" class="headerlink" title="RabbitMQ各类角色描述："></a>RabbitMQ各类角色描述：</h3><p>（1）、none角色权限 ：不能访问 management plugin<br>（2）、management角色权限：<br>列出自己可以通过AMQP登入的virtual hosts<br>查看自己的virtual hosts中的queues, exchanges 和 bindings<br>查看和关闭自己的channels 和 connections<br>查看有关自己的virtual hosts的“全局”的统计信息，包含其他用户在这些virtual hosts中的活动。</p><p>（3）、policymaker角色权限   #policymaker  [&#039;pɒləsɪmeɪkə(r)]  决策者<br>拥有management的所有权限，还拥有查看、创建和删除自己的virtual hosts所属的policies（策略）和parameters（[pəˈræmɪtə(r)] 参数 ）<br>（4）、monitoring 角色权限<br>拥有management的所有权限，还拥有：<br>列出所有virtual hosts，包括他们不能登录的virtual hosts<br>查看其他用户的connections和channels<br>查看节点级别的数据如clustering和memory使用情况<br>查看真正的关于所有virtual hosts的全局的统计信息<br>（5）、administrator角色权限<br>拥有policymaker和monitoring的所有权限，还拥有：<br>创建和删除virtual hosts<br>查看、创建和删除users<br>查看创建和删除permissions<br>关闭其他用户的connections</p><p>TODO</p><h1 id="使用python调用rabbitmq服务器"><a href="#使用python调用rabbitmq服务器" class="headerlink" title="使用python调用rabbitmq服务器"></a>使用python调用rabbitmq服务器</h1><p>pip install pika    #安装pika模块。python用pika模块调用rabbitmq。<br><strong>注：</strong> rabbitmq本质是一个生产者和消费者的模型结构。生产者-&gt;rabbitmq-&gt;消费者，即生产者产生消息，给到rabbitmq存储，消费者从rabbitmq中读取数据。</p><p>创建生产者代码send.py </p><pre><code>import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters(        host=&amp;#039;localhost&amp;#039;))channel = connection.channel()channel.queue_declare(queue=&amp;#039;hello&amp;#039;)channel.basic_publish(exchange=&amp;#039;&amp;#039;,                      routing_key=&amp;#039;hello&amp;#039;,                      body=&amp;#039;Hello World!&amp;#039;)print(&amp;quot; [x] Sent &amp;#039;Hello World!&amp;#039;&amp;quot;)connection.close()注：declare  [dɪˈkleə(r)]  声明  ； consuming [kənˈsju:mɪŋ]  消费；publish  [ˈpʌblɪʃ]  颁布</code></pre><p>创建消耗者代码receive.py </p><pre><code>import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters(        host=&amp;#039;localhost&amp;#039;))channel = connection.channel()channel.queue_declare(queue=&amp;#039;hello&amp;#039;)def callback(ch, method, properties, body):    print(&amp;quot; [x] Received %r&amp;quot; % body)channel.basic_consume(callback,                      queue=&amp;#039;hello&amp;#039;,                      no_ack=True)print(&amp;#039; [*] Waiting for messages. To exit press CTRL+C&amp;#039;)channel.start_consuming()</code></pre><h2 id="开始测试队列"><a href="#开始测试队列" class="headerlink" title="开始测试队列"></a>开始测试队列</h2><p>python send.py   #此命令执行两次，产生两个名字叫hello的消息<br>rabbitmqctl list_queues  #查看消息队列为2</p><p>在web界面查看消息队列，发现在2个消息队列等待处理：<br> python receive.py   #消费或处理这2个消息<br>rabbitmqctl list_queues   #查看队列，已经为0<br>在web界面，查看队列，也为0了</p>]]></content>
    
    
    
    <tags>
      
      <tag>web</tag>
      
      <tag>linux</tag>
      
      <tag>MQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos 7 下部署Django + uWSGI + Nginx</title>
    <link href="undefined2018/10/16/2018-10-16-Centos7%E9%83%A8%E7%BD%B2Django/"/>
    <url>2018/10/16/2018-10-16-Centos7%E9%83%A8%E7%BD%B2Django/</url>
    
    <content type="html"><![CDATA[<!-- # Centos 7 下部署Django + uWSGI + Nginx --><h2 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h2><p>Python: 3.6</p><p>Django: 2.1</p><p>OS: CentOS 7 x86_64</p><p>uwsgi: 2.0.17</p><a id="more"></a><h2 id="目录"><a href="#目录" class="headerlink" title="目录 :"></a>目录 :</h2><ul><li><a href="#安装Python3.6">安装Python3.6</a><ul><li><a href="#创建虚拟环境">创建虚拟环境</a></li><li><a href="#安装uWSGI">安装uWSGI</a></li></ul></li><li><a href="#安装Nginx">安装Nginx</a></li><li><a href="#配置项目">配置项目</a><ul><li><a href="#安装django项目依赖的包">安装django项目依赖的包</a></li><li><a href="#在项目目录下配置uwsgi启动django的参数">在项目目录下配置uwsgi启动django的参数</a></li><li><a href="#配置项目文件">配置项目文件</a></li></ul></li><li><a href="#配置nginx">配置nginx</a></li><li><a href="#重启Nginx">重启Nginx</a></li><li><a href="#uwsgi启动django">uwsgi启动django</a></li><li><a href="#代理">代理</a></li><li><a href="#问题汇总">问题汇总</a></li></ul><h2 id="安装Python3-6"><a href="#安装Python3-6" class="headerlink" title="安装Python3.6"></a>安装Python3.6</h2><ul><li>不要删除自带的python2.7，否则会出问题，因为centos许多软件需要依赖系统自带python</li><li>安装依赖工具 yum install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel</li><li>下载 wget <a href="https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz</a></li><li>解压 tar -zxvf Python-3.6.5.tgz</li><li>移动至规范的放软件的目录下 mv Python-3.6.5 /usr/local</li><li>安装：<pre><code class="bash">cd /usr/local/Python-3.6.5/</code></pre></li></ul><p>./configure</p><p>make &amp; make install</p><pre><code>* 验证* python -V### 创建虚拟环境* python3.6 -m venv /home/cosmic/py3.6env* source /home/cosmic/py3.6env/bin/activate   进入虚拟环境### 安装uWSGI* 首先进入虚拟环境,在虚拟环境下安装uwsgi* 安装 pip install uwsgi * 验证 ```bashdef application(env, start_response):    start_response(&#39;200 OK&#39;, [(&#39;Content-Type&#39;,&#39;text/html&#39;)])    return [b&quot;Hello Django&quot;]</code></pre><pre><code class="bash">uwsgi --http :8001 --wsgi-file test.py</code></pre><p>浏览器访问，网页能显示 Hello Django 那么就没问题</p><ul><li>如果安装失败(可能未安装依赖环境python-devel)</li><li>deactivate 退出虚拟环境</li><li>yum install -y python-devel </li><li>easy_install uwsgi</li></ul><h2 id="安装Nginx"><a href="#安装Nginx" class="headerlink" title="安装Nginx"></a>安装Nginx</h2><ul><li><p>配置源<br>  vi /etc/yum.repos.d/nginx.repo 添加下面内容</p><pre><code class="bash">  [nginx]  name=nginx repo  baseurl=http://nginx.org/packages/mainline/centos/7/x86_64/  gpgcheck=0  enabled=1</code></pre><p> gpkcheck=0 表示对从这个源下载的rpm包不进行校验；<br> enable=1 表示启用这个源。</p><ul><li><p>yum install nginx</p></li><li><p>启动nginx：<br>systemctl start nginx</p></li><li><p>修改默认端口号（默认为80）</p><pre><code class="bash">vim /etc/nginx/conf.d/default.conf</code></pre></li></ul></li></ul><pre><code>server {    listen       8089;    listen [::]:8089;    ...    ...}```</code></pre><ul><li>systemctl restart nginx 重启nginx，直接访问<a href="http://ip:8089" target="_blank" rel="noopener">http://ip:8089</a> 能看到nginx的欢迎界面即可。</li></ul><h2 id="配置项目"><a href="#配置项目" class="headerlink" title="配置项目"></a>配置项目</h2><h3 id="安装django项目依赖的包"><a href="#安装django项目依赖的包" class="headerlink" title="安装django项目依赖的包"></a>安装django项目依赖的包</h3><ul><li>虚拟环境下</li></ul><pre><code>    Django==2.1.2    django-haystack==2.8.1    mysqlclient==1.3.13    pytz==2018.5    uWSGI==2.0.17.1    Whoosh==2.7.4    blablabla...</code></pre><h3 id="在项目目录下配置uwsgi启动django的参数"><a href="#在项目目录下配置uwsgi启动django的参数" class="headerlink" title="在项目目录下配置uwsgi启动django的参数"></a>在项目目录下配置uwsgi启动django的参数</h3><pre><code class="bash">vim django_uwsgi.ini # 名称可自定义,用于启动django项目[uwsgi]# 通过uwsgi访问django需要配置成http# 通过nginx请求uwsgi来访问django 需要配置成socket# 9000 是django的端口号socket = 0.0.0.0:9000# web项目根目录chdir = /home/root/pydj/django_one# module指定项目自带的的wsgi配置文件位置module = django_one.wsgi# 允许存在主进程master = true# 开启进程数量processes = 3# 服务器退出时自动清理环境vacuum = true</code></pre><h3 id="配置项目文件"><a href="#配置项目文件" class="headerlink" title="配置项目文件"></a>配置项目文件</h3><blockquote><p>配置项目下的<code>settings.py</code>文件</p></blockquote><pre><code class="python">...DEBUG = False  # 关闭调试模式ALLOWED_HOSTS = [&quot;*&quot;]  # 允许访问的主机...# 数据库配置DATABASES = {    &#39;default&#39;: {        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;,        &#39;NAME&#39;: &#39;fresh&#39;,        &#39;HOST&#39;:&#39;192.168.98.1&#39;,  #数据库服务器的地址        &#39;USER&#39;:&#39;root&#39;,        &#39;PASSWORD&#39;:&#39;123456&#39;,        &#39;PORT&#39;:3306    }}...# 静态资源配置STATIC_URL = &#39;/static/&#39;   # 默认# STATIC_ROOT用于收集项目下静态资源,STATICFILES_DIRS和STATIC_ROOT不能共存,注销STATICFILES_DIRS#STATICFILES_DIRS = [#     os.path.join(BASE_DIR,&#39;static&#39;)# ]STATIC_ROOT = os.path.join(BASE_DIR, &quot;static/&quot;)...</code></pre><blockquote><p>配置完settings.py文件后在项目根目录下执行: python manage.py collectstatic<br>用于收集静态文件,执行后在项目根目录下的static文件夹下生成一个admin的文件夹</p></blockquote><h2 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h2><pre><code class="bash">vi /etc/nginx/nginx.confuser root ; # 以root用户启动nginx,否则可能会因为权限问题而丢失所有样式...</code></pre><pre><code class="bash">vi /etc/nginx/conf.d/default.conf# 在文件最后，新加一个server 或者在同级目录下添加xxx.conf文件,在其中添加server# 可查看/etc/nginx/nginx.conf便于理解server {    listen       8089;    listen      [::]:8089;    server_name 127.0.0.1 192.168.10.114;     location / {        include /etc/nginx/uwsgi_params;        uwsgi_pass 127.0.0.1:9000;    }    location /static{        alias /home/root/pydj/django_one/sign/static;    }}</code></pre><ul><li>8089 是对外的端口号</li><li>server_name nginx代理uwsgi对外的ip,192.168.10.114为nginx服务器所在的ip</li><li>127.0.0.1:9000 即当nginx服务器收到8089端口的请求时，直接将请求转发给 127.0.0.1:9000</li></ul><h2 id="重启Nginx"><a href="#重启Nginx" class="headerlink" title="重启Nginx"></a>重启Nginx</h2><p>systemctl restart nginx</p><p>secrtcrt<br>filiza<br>xshell</p><h2 id="uwsgi启动django"><a href="#uwsgi启动django" class="headerlink" title="uwsgi启动django"></a>uwsgi启动django</h2><blockquote><p>临时关闭防火墙</p></blockquote><pre><code class="bash">    systemctl stop firewalld.service</code></pre><blockquote><p>如果在虚拟机中部署测试项目时,主机访问时可能需要在控制面板中关闭防火墙</p></blockquote><blockquote><p>注意先进入虚拟环境下,这样创建的venv虚拟环境才能生效</p></blockquote><pre><code class="bash"># 进入项目根目录/home/root/pydj/django_one# 启动uwsgi --ini django_uwsgi.ini</code></pre><h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><h3 id="1-轮询"><a href="#1-轮询" class="headerlink" title="1. 轮询"></a>1. 轮询</h3><blockquote><p>配置nginx配置文件,在<code>/etc/nginx/conf.d/default.conf</code>追加如下内容:</p></blockquote><pre><code class="bash">upstream fresh{        # 设置多个uwsgi代理端口        server 192.168.52.193:9000;        server 192.168.52.210:9000;        server 127.0.0.1:9000;}server{        listen  8009;  # nginx监听端口        location / {                include /etc/nginx/uwsgi_params;                # 应用uwsgi代理端口                uwsgi_pass fresh;        }        location /static{                alias /home/zy/fresh/fresh/static;        }}</code></pre><ul><li>然后访问’nginx服务器ip:8009/…’,端口为nginx服务监听的端口,此时轮询每个uwsgi服务.</li></ul><h3 id="2-指定权重"><a href="#2-指定权重" class="headerlink" title="2. 指定权重"></a>2. 指定权重</h3><blockquote><p>指定轮询比重,weight和访问的比率成正比,常用于后端服务器性能不均时</p></blockquote><pre><code class="bash">upstream fresh{        # 设置多个uwsgi代理端口        server 192.168.52.193:9000 weight=10;        server 192.168.52.210 weight=10;        server 127.0.0.1:9000;}</code></pre><h3 id="3-IP绑定ip-hash"><a href="#3-IP绑定ip-hash" class="headerlink" title="3. IP绑定ip_hash"></a>3. IP绑定ip_hash</h3><blockquote><p>每个请求按访问ip的hasg结果分配,这样每个访客固定访问一个后端服务器,用于解决session会话问题</p></blockquote><pre><code class="bash">upstream fresh{        # 设置多个uwsgi代理端口        ip_hash;        server 192.168.52.193:9000;        server 192.168.52.210;        server 127.0.0.1:9000;}</code></pre><h3 id="4-fair-第三方"><a href="#4-fair-第三方" class="headerlink" title="4. fair(第三方)"></a>4. fair(第三方)</h3><blockquote><p>按后端服务器的响应时间来分配请求,响应时间短的优先分配</p></blockquote><pre><code class="bash">upstream fresh{        # 设置多个uwsgi代理端口        server 192.168.52.193:9000;        server 192.168.52.210;        server 127.0.0.1:9000;        fair;}</code></pre><h3 id="5-url-hash"><a href="#5-url-hash" class="headerlink" title="5. url_hash"></a>5. url_hash</h3><blockquote><p>按访问url的hash结果来分配请求,是每个url定向到同一个后端服务器,后端服务器为缓存时比较有效</p></blockquote><pre><code class="bash">upstream fresh{        # 设置多个uwsgi代理端口        server 192.168.52.193:9000;        server 192.168.52.210;        server 127.0.0.1:9000;        hash $request_url;        hash_method crc32;}</code></pre><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><blockquote><p>启动时切换root用户提权</p></blockquote><pre><code>su     # 切换root用户su - zy  # 切换普通用户</code></pre><blockquote><p>无法连接数据库,可能数据库未正确安装</p></blockquote><pre><code>yum install mysql-devel gcc gcc-devel python-develsemanage port -l | grep http_port_t</code></pre><blockquote><p>配置mysql可以被局域网任意主机访问; 1130错误 提示主机没有访问数据库权限; 1045错误 提示主机拒绝访问</p></blockquote><pre><code>mysql&amp;gt; use mysql; mysql&amp;gt; update user set host = &amp;#039;%&amp;#039; where user = &amp;#039;root&amp;#039;; mysql&amp;gt; select host, user from user; mysql&amp;gt; flush privileges;</code></pre><blockquote><p>[emerg] 31879#31879: bind() to 0.0.0.0:8089 failed (13: Permission denied)</p></blockquote><p>绑定端口失败,端口占用或者不支持或者selinux权限控制</p><pre><code>sudo semanage port -l | grep http_port_t      # 查看可用端口sudo semanage port -a -t http_port_t  -p tcp 8024    # 可将自定义端口加入其中netstat -lnp|grep 88 , lsof -i : 8000  , ps --help    # 查看端口状态kill -9 1777     # 指定pid杀掉进程</code></pre><blockquote><p>样式文件失效</p></blockquote><p>Selinux 控制访问权限,默认严格模式,通过以下命令临时放宽访问权限</p><pre><code># 临时关闭:[root@localhost ~]# getenforceEnforcing[root@localhost ~]# setenforce 0[root@localhost ~]# getenforcePermissive# 永久关闭[root@localhost ~]# vim /etc/sysconfig/selinuxSELINUX=enforcing 改为 SELINUX=disabled重启服务reboot</code></pre><blockquote><p>部署大致流程</p></blockquote><ul><li>修改setting文件之后</li><li>python manage.py collectstatic 收集admin静态文件</li><li>修改uwsgi.ini</li><li>启动</li><li>uwsgi –ini django_uwsgi.ini   –buffer-size 32768</li><li>添加nginx配置文件</li><li>重启nginx</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>web</tag>
      
      <tag>Django</tag>
      
      <tag>nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编码规范</title>
    <link href="undefined2018/09/04/2018-09-04-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    <url>2018/09/04/2018-09-04-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/</url>
    
    <content type="html"><![CDATA[<h1 id="PEP8规范"><a href="#PEP8规范" class="headerlink" title="PEP8规范"></a>PEP8规范</h1><p>（一） 代码的编排<br>1、 缩进。每行需要4个空格的缩进，不要使用Tap键，更不能混合使用Tap键和空格。<br>2 、每行最大长度79，换行可以使用反斜杠，但最好使用圆括号。换行点要在操作符的后边敲回车。<br>3 、类和top-level函数定义之间空两行；类中的方法定义之间空一行；函数内逻辑无关段落之间空一行；其他地方尽量不要再空行。</p><a id="more"></a><p>（二 ）文档的编排<br>1、 模块内容的顺序：模块说明和docstring—import—globals&amp;constants—其他定义。其中import部分，又按标准、第三方和自己编写顺序依次排放，之间空一行。</p><p>from django.db import models<br>from datetime import datetime</p><p>from django.contrib.auth.models import AbstractUser</p><h1 id="自己定义代码放置处"><a href="#自己定义代码放置处" class="headerlink" title="自己定义代码放置处"></a>自己定义代码放置处</h1><p>2 、不要在一行import多个库，比如import os, sys，虽说没有错误但是并不推荐。<br>3、 如果采用from xx import xx的方式来引用某个库，可以省略module.，但是可能会出现命名的冲突，所以这时就要采用import xx的方式。</p><p>（三）空格的使用<br>总体原则，避免不必要的空格。<br>1、 各种右括号前不要加空格。<br>2、 逗号、冒号、分号前不要加空格。<br>3 、函数的左括号前不要加空格。如function(1)。<br>4 、序列的左括号前不要加空格。如list[2]。<br>5、操作符左右各加一个空格，不要为了对齐增加空格。<br>6 、函数默认参数使用的赋值符左右省略空格。<br>7、不要将多句语句写在同一行，尽管使用；允许。<br>8、 if/for/while语句中，即使执行语句只有一句，也必须另起一行。</p><p>（四）命名的规范<br>总体原则，新编代码必须按下面命名风格进行，现有库的编码尽量保持风格。<br>1 、尽量单独使用小写字母‘l’，大写字母‘O’等容易混淆的字母。<br>2、 模块命名尽量短小，使用全部小写的方式，可以使用下划线。<br>3、 包命名尽量短小，使用全部小写的方式，不可以使用下划线。<br>4、 类的命名使用CapWords的方式，模块内部使用的类采用_CapWords的方式。<br>5、 异常命名使用CapWords+Error后缀的方式。<br>6 、全局变量尽量只在模块内有效，类似C语言中的static。实现方法有两种，一是all机制;二是前缀一个下划线。<br>7 、函数命名使用全部小写的方式，可以使用下划线。<br>8 、常量命名使用全部大写的方式，可以使用下划线。<br>9 、类的属性（方法和变量）命名使用全部小写的方式，可以使用下划线。<br>10、类的属性有3种作用域public、non-public和subclass API，可以理解成C++中的public、private、protected，non-public属性前，前缀一条下划线。<br>11 、类的属性若与关键字名字冲突，后缀一下划线，尽量不要使用缩略等其他方式。<br>12 、为避免与子类属性命名冲突，在类的一些属性前，前缀两条下划线。比如：类Foo中声明<strong>a,访问时，只能通过`Foo._Foo</strong>a`，避免歧义。如果子类也叫Foo，那就无能为力了。<br>13 、类的方法第一个参数必须是self，而静态方法第一个参数必须是cls。</p><p>（五）编码的建议<br>1、编码中考虑到其他python实现的效率等问题，比如运算符‘+’在CPython（Python）中效率很高，都是Jython中却非常低，所以应该采用.join()的方式。<br>2 、尽可能使用‘is’‘is not’取代‘==’，比如if x is not None 要优于if x。<br>3 、使用基于类的异常，每个模块或包都有自己的异常类，此异常类继承自Exception。<br>4 、异常中不要使用裸露的except，except后跟具体的exceptions。<br>5 、异常中try的代码尽可能少。比如：</p><pre><code>try:value = collection[key]except KeyError:return key_not_found(key)else:return handle_value(value)</code></pre><p>要优于</p><pre><code>try:# Too broad!return handle_value(collection[key])except KeyError:# Will also catch KeyError raised by handle_value()return key_not_found(key)</code></pre><p>6 、使用startswith() and endswith()代替切片进行序列前缀或后缀的检查。比如:<br>Yes: if foo.startswith(‘bar’):优于No: if foo[:3] == ‘bar’:<br>7 、使用isinstance()比较对象的类型。比如:<br>Yes: if isinstance(obj, int):优于No: if type(obj) is type(1):<br>8、 判断序列空或不空，有如下规则:</p><p>Yes: if not seq:<br>if seq:<br>优于</p><p>No: if len(seq)<br>if not len(seq)<br>9 、字符串不要以空格收尾。<br>10、二进制数据判断使用 if boolvalue的方式。</p><p>如果你想获得更多关于PEP8的信息，可以查阅这篇信息PEP8 Python 编码规范整理或者官方文档PEP8的官方文档</p>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>web</tag>
      
      <tag>Django</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django使用</title>
    <link href="undefined2018/09/04/2018-09-04-Django%E4%BD%BF%E7%94%A8/"/>
    <url>2018/09/04/2018-09-04-Django%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本篇内容需具备上篇<a href="https://zyboy.top/2018/09/03/2018-09-03-Django/">Django简介</a>基础</p></blockquote><a id="more"></a><h3 id="数据模型models-py"><a href="#数据模型models-py" class="headerlink" title="数据模型models.py"></a>数据模型<code>models.py</code></h3><blockquote><p>通过命令<code>django-admin startapp myApp</code>创建<code>myApp</code>后即可向<code>models.py</code>添加内容:</p></blockquote><pre><code class="python">from django.db import models# Create your models here.class Table1(models.Model):    name = models.CharField(max_length=20,unique=True)    age = models.IntegerField()    # 自定义显示查询结果的字段 否则为object对象     def __str__(self):        return self.name+&#39;,&#39;+self.age</code></pre><blockquote><p>在终端中切换目录到项目目录下执行命令:</p></blockquote><pre><code>\&gt; python manage.py makemigrations</code></pre><blockquote><p>查看在<code>migrations</code>文件夹下生成的迁移文件:</p></blockquote><pre><code class="python"># Generated by Django 2.1.1 on 2018-09-04 12:34from django.db import migrations, modelsclass Migration(migrations.Migration):    initial = True    dependencies = [    ]    operations = [        migrations.CreateModel(            name=&#39;Table1&#39;,            fields=[                (&#39;id&#39;, models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=&#39;ID&#39;)),                (&#39;name&#39;, models.CharField(max_length=20, unique=True)),                (&#39;age&#39;, models.IntegerField()),            ],        ),    ]</code></pre><blockquote><p>迁移文件:</p></blockquote><pre><code>\&gt; python manage.py migrate</code></pre><blockquote><p>此时数据库可用</p></blockquote><blockquote><p>测试数据库可在命令交互界面进行:</p></blockquote><pre><code>\&gt; python manage.py shell\&gt;[]: from myApp.models import First   -- 引入数据模型:&lt;!-- 添加数据的方式1 --&gt;\&gt;[]: first = First(name=&quot;lisi&quot;,des=&quot;my first friend&quot;)\&gt;[]: first.save()&lt;!-- 方式2 --&gt;\&gt;[]: second = First.objects.create(name=&quot;hello&quot;,des=&quot;world&quot;)&lt;!-- 获取数据 --&gt;\&gt;[]: First.objects.all() ----查看所有数据,增加了def __str__()方法后能友好显示\&gt;[]: user = First.objects.get(id=&quot;&quot;/name=&quot;&quot;/des=&quot;&quot;)\&gt;[]: user.name\&gt;[]: user.des\&gt;[]: user.id&lt;!-- 退出shell --&gt;\&gt;[]: exit()</code></pre><blockquote><p>查看保存的数据,须在<code>views.py</code>中设置,方法同上:</p><blockquote><p>1.引入模型 获取所有数据<code>First.objects.all()</code><br>2.return</p></blockquote></blockquote><blockquote><p><a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/9.04/Django-models/myModels" target="_blank" rel="noopener" title="点击进入">参考源码</a></p></blockquote><h3 id="urls-py"><a href="#urls-py" class="headerlink" title="urls.py"></a>urls.py</h3><blockquote><p>1.项目文件夹下的<code>urls.py</code></p></blockquote><pre><code class="python">from django.contrib import adminfrom django.urls import pathfrom django.urls.conf import include# 导入app中的视图文件from myFirst import viewsurlpatterns = [    path(&#39;admin/&#39;, admin.site.urls),    # path(&#39;regist/&#39;,views.regist) ,    # path(&#39;login/&#39;,views.login),    # 在localhost端口后面添加一个路径first    # 该路径下面还有其他两个路径  regist和login    path(&#39;first/&#39;,include(&#39;myFirst.urls&#39;)),    path(&#39;second/&#39;,include(&#39;mySecond.urls&#39;)),    path(r&#39;third/&#39;,include(&#39;myThird.urls&#39;))]</code></pre><blockquote><p>2.应用<code>myApp</code>中的<code>urls.py</code></p></blockquote><pre><code class="python">from django.urls import pathfrom django.conf.urls import urlfrom . import views# 对应项目下的应用myFirsturlpatterns = [    path(r&#39;regist/&#39;,views.regist),    path(r&#39;login/&#39;,views.login)]# 对应项目下的应用mySecondurlpatterns=[    url(r&#39;add/&#39;,views.add),    url(r&#39;addTwo/&#39;,views.addTwo)]# 对应项目下的应用myThirdurlpatterns = [    url(r&#39;red/&#39;,views.myFirst),    url(r&#39;black/&#39;,views.mySecond)]</code></pre><blockquote><p>在每个应用下的<code>views.py</code>文件中添加渲染代码,如应用中的<code>myFirst</code>下的<code>views.py</code>文件</p></blockquote><blockquote><p>此时整个项目的目录结构:</p></blockquote><pre><code class="cmd">ddjangoUrls│  │  __init__.py│  ││  └─myUrls│      │  db.sqlite3│      │  manage.py│      │  __init__.py│      ││      ├─myFirst│      │  │  admin.py│      │  │  apps.py│      │  │  models.py│      │  │  tests.py│      │  │  urls.py│      │  │  views.py│      │  │  __init__.py│      │  ││      │  ├─migrations│      │  │  │  __init__.py│      │  │  ││      │  │  └─__pycache__│      │  │          __init__.cpython-36.pyc│      │  ││      │  └─__pycache__│      │          ...│      ││      ├─mySecond│      │          ...│      ││      ├─myThird│      │  │       ...│      └─myUrls│          │  settings.py│          │  urls.py│          │  wsgi.py│          │  __init__.py│          ││          └─__pycache__│                  ...</code></pre><pre><code class="python">from django.shortcuts import renderfrom django.http import HttpResponse# Create your views here.def regist(request):    return HttpResponse(&#39;myFirst里面的注册页面&#39;)def login(request):    return HttpResponse(&#39;myFirst里面的登录页面&#39;)</code></pre><blockquote><p><a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/9.04/3.django-argument/myArgu" target="_blank" rel="noopener" title="点击进入">参考源码</a></p></blockquote><h3 id="传递参数-argument"><a href="#传递参数-argument" class="headerlink" title="传递参数(argument)"></a>传递参数(argument)</h3><h3 id="模板使用-templates"><a href="#模板使用-templates" class="headerlink" title="模板使用(templates)"></a>模板使用(templates)</h3><h3 id="模板继承"><a href="#模板继承" class="headerlink" title="模板继承"></a>模板继承</h3><h3 id="表单-forms"><a href="#表单-forms" class="headerlink" title="表单(forms)"></a>表单(forms)</h3><h3 id="邮件发送"><a href="#邮件发送" class="headerlink" title="邮件发送"></a>邮件发送</h3>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>web</tag>
      
      <tag>Django</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django简介</title>
    <link href="undefined2018/09/03/2018-09-03-Django/"/>
    <url>2018/09/03/2018-09-03-Django/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="1-安装Django"><a href="#1-安装Django" class="headerlink" title="1.安装Django:"></a>1.安装Django:</h3><blockquote><p>pip install django</p></blockquote><pre><code>        安装指定版本 pip install django==1.8</code></pre><blockquote><p>换源: </p></blockquote><pre><code>        pip install -i https://pypi.tuna.tsinghua.edu.cn/simple django </code></pre><blockquote><p>查看安装是否成功:</p></blockquote><pre><code>        django-admin --version</code></pre><h3 id="2-创建项目"><a href="#2-创建项目" class="headerlink" title="2.创建项目"></a>2.创建项目</h3><blockquote><p>创建任意目录(django-first)</p></blockquote><blockquote><p>进入django-first目录创建项目myFirst:</p></blockquote><pre><code>django-admin startproject myFirst</code></pre><blockquote><p>此时目录结构为: </p></blockquote><p><img src="/img/Django/1.png" srcset="/img/loading.gif" alt="img" title="img"></p><h3 id="3-启动项目-一定要进入项目文件夹下执行命令"><a href="#3-启动项目-一定要进入项目文件夹下执行命令" class="headerlink" title="3.启动项目(一定要进入项目文件夹下执行命令)"></a>3.启动项目(一定要进入项目文件夹下执行命令)</h3><blockquote><p>进入myFirst目录:</p></blockquote><pre><code>cd myFirst</code></pre><blockquote><p>执行命令: </p></blockquote><pre><code>python manage.py runserver</code></pre><blockquote><p>如图显示 启动成功.</p></blockquote><p><img src="/img/Django/2.png" srcset="/img/loading.gif" alt="img" title="img"></p><blockquote><p>修改默认端口号: </p></blockquote><pre><code>python manage.py runserver 8080</code></pre><h3 id="4-myFirst目录下创建app"><a href="#4-myFirst目录下创建app" class="headerlink" title="4.myFirst目录下创建app:"></a>4.myFirst目录下创建app:</h3><pre><code>django-admin startapp myApp</code></pre><h3 id="5-渲染views-py-HttpResponse"><a href="#5-渲染views-py-HttpResponse" class="headerlink" title="5.渲染views.py  HttpResponse"></a>5.渲染views.py  HttpResponse</h3><pre><code class="python">from django.shortcuts import renderfrom django.http import HttpResponse# Create your views here.def firstPage(request):    # 注意 此处需要接受一个请求对象 并且返回一个响应对象    return HttpResponse(&#39;hello world&#39;)</code></pre><h3 id="6-urls-py-当中设置路径"><a href="#6-urls-py-当中设置路径" class="headerlink" title="6.urls.py 当中设置路径"></a>6.urls.py 当中设置路径</h3><pre><code class="python">&quot;&quot;&quot;myFirst URL ConfigurationThe `urlpatterns` list routes URLs to views. For more information please see:    https://docs.djangoproject.com/en/2.1/topics/http/urls/Examples:Function views    1. Add an import:  from my_app import views    2. Add a URL to urlpatterns:  path(&#39;&#39;, views.home, name=&#39;home&#39;)Class-based views    1. Add an import:  from other_app.views import Home    2. Add a URL to urlpatterns:  path(&#39;&#39;, Home.as_view(), name=&#39;home&#39;)Including another URLconf    1. Import the include() function: from django.urls import include, path    2. Add a URL to urlpatterns:  path(&#39;blog/&#39;, include(&#39;blog.urls&#39;))&quot;&quot;&quot;from django.contrib import adminfrom django.urls import pathfrom myApp import viewsurlpatterns = [    path(&#39;admin/&#39;, admin.site.urls),    path(r&#39;first/&#39;,views.firstPage,name=&#39;first&#39;)]</code></pre><blockquote><p>此时浏览器中输入<code>http://127.0.0.1:8080/first</code>访问</p></blockquote><h3 id="7-创建templates模板-views-py中进行渲染-settings-py中的TEMPLATES进行设置"><a href="#7-创建templates模板-views-py中进行渲染-settings-py中的TEMPLATES进行设置" class="headerlink" title="7.创建templates模板, views.py中进行渲染,settings.py中的TEMPLATES进行设置"></a>7.创建templates模板, views.py中进行渲染,settings.py中的TEMPLATES进行设置</h3><blockquote><p>在myFirst项目下创建templates模板文件夹,向其中添加first.html文件:</p></blockquote><p><img src="/img/Django/3.png" srcset="/img/loading.gif" alt="img" title="img"></p><blockquote><p>views.py中进行渲染,修改views.py文件:</p></blockquote><pre><code class="python">from django.shortcuts import renderfrom django.http import HttpResponse# Create your views here.def firstPage(request):    # 注意 此处需要接受一个请求对象 并且返回一个响应对象    # return HttpResponse(&#39;hello world&#39;)    return render(request,&#39;first.html&#39;)</code></pre><blockquote><p>settings.py中设置</p></blockquote><pre><code class="python">TEMPLATES = [    {        ...        # 将templates文件夹路径添加进来        &#39;DIRS&#39;: [os.path.join(BASE_DIR,&#39;templates&#39;)],        ...            ],        },    },]INSTALLED_APPS = [    ...    # 将创建的app添加进来    &#39;myFirst&#39;,    &#39;mySecond&#39;,    &#39;myThird&#39;]</code></pre><blockquote><p>templates中first.html内容:</p></blockquote><pre><code class="html">&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;Django第一个页面&lt;/title&gt;    &lt;style&gt;        h1{            color: red;        }    &lt;/style&gt;&lt;/head&gt;&lt;body&gt;    &lt;h1&gt;这是我学习Django的第一个页面&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><blockquote><p>再次访问<code>http://127.0.0.1:8080/first</code></p></blockquote><p><img src="/img/Django/4.png" srcset="/img/loading.gif" alt="img" title="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>web</tag>
      
      <tag>Django</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>node.js进阶</title>
    <link href="undefined2018/08/30/2018-8-30-node.js%E8%BF%9B%E9%98%B6/"/>
    <url>2018/08/30/2018-8-30-node.js%E8%BF%9B%E9%98%B6/</url>
    
    <content type="html"><![CDATA[<blockquote><ul><li><p><a href="#get方法">get方法</a></p></li><li><p><a href="#post方法">post</a></p></li><li><p><a href="#xhr">xhr(XMLHttpRequest)</a></p></li><li><p><a href="#forms表单">forms表单</a></p></li><li><p><a href="#fs">fs(文件读写)</a></p></li><li><p><a href="#小项目练习">小项目练习</a></p></li></ul></blockquote><a id="more"></a><h3 id="get方法"><a href="#get方法" class="headerlink" title="get方法"></a>get方法</h3><blockquote><p><code>index.html</code>,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/form" target="_blank" rel="noopener" title="点击查看">参考源码</a></p></blockquote><pre><code class="html">...&lt;form action=&quot;/regist&quot;&gt;    &lt;h1&gt;regist&lt;/h1&gt;    &lt;label for=&quot;&quot;&gt;账号:&lt;/label&gt;    &lt;input name=&quot;user&quot; type=&quot;text&quot; placeholder=&quot;请输入账号&quot;&gt;    &lt;br&gt;    &lt;label for=&quot;&quot;&gt;密码:&lt;/label&gt;    &lt;input name=&quot;psw&quot; type=&quot;password&quot; placeholder=&quot;请输入密码&quot;&gt;    &lt;br&gt;    &lt;label for=&quot;&quot;&gt;重复密码:&lt;/label&gt;    &lt;input name=&quot;pswa&quot; type=&quot;password&quot; placeholder=&quot;请重复密码&quot;&gt;    &lt;br&gt;    &lt;input type=&quot;submit&quot; value=&quot;注册&quot;&gt;&lt;/form&gt;...</code></pre><blockquote><p><code>index.js</code></p></blockquote><pre><code class="js">var express = require(&#39;express&#39;)// post请求方式会将参数放入到请求体当中// 所以需要引入解析请求体的模块var bodyParser = require(&#39;body-parser&#39;)var web = express()web.use(express.static(&#39;public&#39;))// 设置对url进行编码 并且不允许url进行扩展// 如果设置为false 那么参数只能为数值或者字符串// 如果设置为True 那么参数为任意类型web.use(bodyParser.urlencoded({extended:false}))// 存储注册成功以后的账号和密码account = &#39;&#39;psw = &#39;&#39;web.get(&#39;/regist&#39;,function(req,res){    var password = req.query.psw    var password2 = req.query.pswa    var user = req.query.user    if(user != account &amp;&amp; password == password2){        account = user        psw = password        res.send(&#39;恭喜注册登陆成功! 账号是&#39; + user + &#39;,密码是&#39; + password + &#39;,请妥善保管&#39;)    }    else{        res.send(&#39;注册失败,账号已经注册或者密码不一致&#39;)    }    console.log(password)    console.log(password2)})</code></pre><h3 id="post方法"><a href="#post方法" class="headerlink" title="post方法"></a>post方法</h3><blockquote><p><code>index.html</code>,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/form" target="_blank" rel="noopener" title="点击查看">参考源码</a></p></blockquote><pre><code class="html">&lt;form action=&quot;/login&quot; method=&quot;POST&quot;&gt;    &lt;h1&gt;login&lt;/h1&gt;    &lt;label for=&quot;&quot;&gt;账号:&lt;/label&gt;    &lt;input name=&quot;user&quot; type=&quot;text&quot; placeholder=&quot;请输入账号&quot;&gt;    &lt;br&gt;    &lt;label for=&quot;&quot;&gt;密码:&lt;/label&gt;    &lt;input name=&quot;password&quot; type=&quot;text&quot; placeholder=&quot;请输入密码&quot;&gt;    &lt;br&gt;    &lt;input type=&quot;submit&quot; value=&quot;登陆&quot;&gt;&lt;/form&gt;</code></pre><blockquote><p><code>index.js</code></p></blockquote><pre><code class="js">web.post(&#39;/login&#39;,function(req,res){    var name = req.body.user    var password = req.body.password    if(name == account &amp;&amp; password == psw){        res.send(&#39;恭喜登陆成功&#39;)    }    else{        res.send(&#39;登陆失败,请检查账号和密码&#39;)    }})</code></pre><h3 id="xhr"><a href="#xhr" class="headerlink" title="xhr"></a>xhr</h3><blockquote><p><code>index.html</code>,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/xhr" target="_blank" rel="noopener" title="点击查看">参考源码</a></p></blockquote><pre><code class="html">&lt;!-- &lt;form action=&quot;/login&quot;&gt;        &lt;input type=&quot;submit&quot;&gt;    &lt;/form&gt; --&gt;    &lt;!--     xhr    XMLHTTPRequest 是ajax的核心    ajax是数据请求方式的一种    特点:    1.前端可以发送数据到后端    2.可以接受从后端发送过来的数据    3.可以解析从后端传过来的数据    3.可以对页面进行局部刷新操作    --&gt;    &lt;button onclick=&quot;get()&quot;&gt;xhr之get请求&lt;/button&gt;    &lt;button onclick=&quot;post()&quot;&gt;xhr之post请求&lt;/button&gt;    &lt;script&gt;        function get(){            // 创建一个数据请求的实例化对象            var xhr =new XMLHttpRequest()            // on 在...时候 ready 准备 state 状态 change 改变            // 在xhr的准备状态发生改变时 调用该方法            xhr.onreadystatechange = function(){                // 判断xhr的准备状态                switch(xhr.readyState){                    case 0:{                        console.log(&#39;open方法已经调用, 但是send方法没有调用&#39;)                        break                    }                    case 1:{                        console.log(&#39;send方法已经调用, 但是服务器并没有返回数据&#39;)                        break                    }                    case 2:{                        console.log(&#39;send方法已经调用, 服务器开始返回数据&#39;)                        break                    }                    case 3:{                        console.log(&#39;服务器已经返回部分数据&#39;)                        break                    }                    case 4:{                        console.log(&#39;服务器已经返回全部数据&#39;)                        console.log(xhr.response)                        console.log(xhr.responseText)                        console.log(xhr.responseURL)                        console.log(xhr.status)                        console.log(xhr.statusText)                        console.log(xhr.getAllResponseHeaders())                        break                    }                }            }            // open方法里面两个参数            // 参数1:数据请求方式 get post            // 参数2:请求的接口 参数才接口后面进行拼接,地址栏方式传参            xhr.open(&#39;get&#39;,&#39;/getTest?name=妹子UI&amp;des=前端app框架&#39;)            // 发送数据到后端            xhr.send()        }        function post(){            var xhr = new XMLHttpRequest()            // post请求方式 接口后面不能追加参数            xhr.open(&#39;post&#39;,&#39;/postTest&#39;)            // 如果使用post请求方式 而且参数是以key=value这种方式提交的            // 那么需要设置请求头的类型            xhr.setRequestHeader(&#39;Content-Type&#39;,&#39;application/x-www-form-urlencoded&#39;)            xhr.send(&#39;star=1&amp;des=东西很好,价格公道,但是差评,人生就是这么奇妙&#39;)            xhr.onreadystatechange = function(){                if(xhr.readyState == 4){                    console.log(xhr.responseText)                }            }        }    &lt;/script&gt;</code></pre><blockquote><p><code>index.js</code></p></blockquote><pre><code class="js">var express = require(&#39;express&#39;)var bodyParser = require(&#39;body-parser&#39;)var web = express()web.use(express.static(&#39;public&#39;))web.use(bodyParser.urlencoded({extended:false}))web.get(&#39;/getTest&#39;,function(req,res){    var name = req.query.name    var des = req.query.des    setTimeout(function(){        res.send(&#39;听说有一种&#39; + des + &quot;非常厉害,叫做&quot; + name)    },2000)})web.post(&#39;/postTest&#39;,function(req,res){    var star = req.body.star    var des = req.body.des    setTimeout(function(){        res.send(&#39;商品评价成功&#39;)    },2000)})web.listen(&#39;8080&#39;,function(){    console.log(&#39;服务器启动成功...&#39;)})</code></pre><h3 id="form表单"><a href="#form表单" class="headerlink" title="form表单"></a>form表单</h3><blockquote><p><code>public/index.html</code>,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/form" target="_blank" rel="noopener" title="点击查看">参考源码</a></p></blockquote><pre><code class="html">&lt;button onclick=&quot;form1()&quot;&gt;xhr之form请求&lt;/button&gt;    &lt;script&gt;        function form1() {            var xhr = new XMLHttpRequest()            // 初始化一个form对象            var form = new FormData()            form.append(&#39;name&#39;, &#39;小青年&#39;)            form.append(&#39;age&#39;, &#39;17&#39;)            form.append(&#39;fond&#39;, &#39;玩&#39;)            xhr.open(&#39;post&#39;, &#39;/test&#39;)            xhr.send(form)            xhr.onreadystatechange = function () {                if (xhr.readyState == 4) {                    console.log(xhr.responseText)                }            }        }    &lt;/script&gt;</code></pre><blockquote><p><code>index.js</code></p></blockquote><pre><code class="js">var express = require(&#39;express&#39;)var bodyParser = require(&#39;body-parser&#39;)var multer = require(&#39;multer&#39;)var web = express()var form = multer()web.use(express.static(&#39;public&#39;))web.use(bodyParser.urlencoded({extended:false}))// 如果使用的时FormData这种数据提交方式的话// 那么需要multer里面的array()方法进行数据剥离web.post(&#39;/test&#39;,form.array(),function(req,res){    name = req.body.name    age = req.body.age    fond = req.body.fond    res.send(&#39;姓名是&#39;+name + &quot;,年龄是&quot; + age + &#39;,爱好是&#39;+fond)})</code></pre><h3 id="fs"><a href="#fs" class="headerlink" title="fs"></a>fs</h3><blockquote><p><code>index.js</code> 源码参考(<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/fs" target="_blank" rel="noopener">https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/fs</a> “点击查看”)</p></blockquote><pre><code class="js">// 引入文件读写模块 系统默认包含fs模块 所以无需下载var fs = require(&#39;fs&#39;)var gameRoleInfo = {    name: &#39;芽衣&#39;,    level: &#39;20&#39;,    equipment: [&#39;村正&#39;, &#39;圣痕&#39;, &#39;大砍刀&#39;, &#39;高射炮&#39;, &#39;萝莉装&#39;, &#39;五速鞋&#39;],    blood: &#39;3500&#39;}// 将字典对象转化成字符串对象var gameRoleInfoString = JSON.stringify(gameRoleInfo)// writeFile后面跟三个值// 1.写入文件的路径// 2.写入的内容// 3.写入以后的回调函数fs.writeFile(&#39;data/game.txt&#39;,gameRoleInfoString,function(err){    if(err){        console.log(&#39;文件写入失败:&#39;,err)    }    else{        console.log(&#39;文件写入成功&#39;)    }})// readFile里面两个参数// 参数1.读取的文件路径// 参数2.读取文件以后的回调函数// 回调函数里面有两个参数// 参数1.读取失败以后的信息// 参数2.读取成功以后的信息fs.readFile(&#39;data/game.txt&#39;,function(err,data){    if(err){        console.log(&#39;读取失败&#39;,err)    }    else{        // JSON.parse 将字典格式的字符串转化为字典对象        // console.log(&#39;读取成功:&#39;+JSON.parse(data))        console.log(&#39;读取成功:&#39;)        console.log(JSON.parse(data))    }})</code></pre><h3 id="小项目练习"><a href="#小项目练习" class="headerlink" title="小项目练习"></a>小项目练习</h3><blockquote><p>小项目练习,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/8.30/%E5%91%98%E5%B7%A5%E4%BF%A1%E6%81%AF%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/info" target="_blank" rel="noopener" title="点击查看">员工信息管理</a>,<a href="https://github.com/zysxm/Learning.github.io/tree/master/Python/9.03/messagebox" target="_blank" rel="noopener" title="点击查看">留言板</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>前端</category>
      
    </categories>
    
    
    <tags>
      
      <tag>node.js</tag>
      
      <tag>html</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>node.js使用</title>
    <link href="undefined2018/08/29/2018-08-29-node.js%E4%BD%BF%E7%94%A8/"/>
    <url>2018/08/29/2018-08-29-node.js%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>简单的说 Node.js 就是运行在服务端的 JavaScript。</p><p>Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。</p><p>Node.js是一个事件驱动I/O服务端JavaScript环境，基于Google的V8引擎，V8引擎执行Javascript的速度非常快，性能非常好。</p><a id="more"></a><p>1.<a href="https://nodejs.org/dist/v4.4.3/node-v4.4.3-x64.msi" target="_blank" rel="noopener" title="node.js">node.js</a>,按照提示<code>next</code>,安装后在终端中键入 <code>node -v</code> 测试是否安装成功</p><blockquote><p><img src="/img/Pictures/node/1.png" srcset="/img/loading.gif" alt="示例图" title="点击查看大图"></p></blockquote><p>2.安装后注销/重启,使npm包管理工具生效</p><p>3.创建项目<code>nodebasic</code>(名称随意)</p><p>4.右键项目在终端中打开</p><blockquote><p><img src="/img/Pictures/node/2.png" srcset="/img/loading.gif" alt="示例图" title="点击查看大图"></p></blockquote><p>5.键入命令 <code>npm init</code>  ——-初始化项目</p><blockquote><p>a.根据提示输入内容,配置信息可全部回车<br>b.成功后自动创建package.json文件</p></blockquote><p>6.在项目下创建静态文件夹<code>public</code></p><blockquote><p><code>public</code>文件夹下创建<code>html</code>文件<code>index.html</code>(名称固定),内容如下:</p></blockquote><pre><code class="html">&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt;    &lt;title&gt;node.js第一个项目&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;!-- action 提交到的接口 --&gt;    &lt;form action=&quot;/book&quot;&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交书籍信息&quot;&gt;    &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><p>7.再次右键项目在终端中打开</p><blockquote><p>a.键入如下命令</p><pre><code>  npm install express ----安装Node.js web框架模块express以及所有依赖项</code></pre><p>b.如果提示网络错误,更改镜像源地址</p><pre><code>  npm config set registry http://registry.cnpmjs.org</code></pre></blockquote><p>8.项目下创建index.js,里面为服务端代码,内容如下:</p><blockquote><pre><code class="javascript">// 引入express模块  express模块为数据请求基础模块// 如果发生数据请求 那么一定需要使用这个模块var express = require(&#39;express&#39;)// 创建模块的一个实例化对象var web = express()// static 静态 让web对象使用工程中的静态资源 public文件夹web.use(express.static(&#39;public&#39;))// get 表示使用get方法 方法后面追加两个参数 // 参数1: 请求的接口// 参数2: 回调函数 //     回调函数里面有两个参数//     参数1: 前端从后端传的值//     参数2: 后端往前端传的值web.get(&#39;/book&#39;,function(req , res){    res.send(&#39;古今奇书&lt;&lt;聊斋志异&gt;&gt;&#39;)})// 让程序监听node端口web.listen(&#39;8080&#39;,function(){    console.log(&#39;服务器启动......&#39;)})</code></pre></blockquote><p>9.此时目录结构如下</p><blockquote><p><img src="/img/Pictures/node/3.png" srcset="/img/loading.gif" alt="img" title="点击查看大图"></p></blockquote><p>10.启动服务端并访问</p><blockquote><p>a.node index.js<br>b.地址栏:localhost:端口号(index.js中定义)<br>c.注意 : 修改服务端代码后 需重启服务器,同一个服务器程序,不能在多终端中启动</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>前端</category>
      
    </categories>
    
    
    <tags>
      
      <tag>node.js</tag>
      
      <tag>html</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scrapy_redis设置及分布式爬虫案例分析</title>
    <link href="undefined2018/08/16/2018-08-16-scrapy_redis%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/"/>
    <url>2018/08/16/2018-08-16-scrapy_redis%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>目录:</p><p><a href="#源码">源码</a></p><p><a href="#安装">安装</a></p><p><a href="#settings">settings</a></p><p><a href="#examples">examples</a></p><p><a href="#修改普通爬虫项目为分布式爬虫">修改scrapy普通爬虫项目为分布式爬虫</a></p><a id="more"></a><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><blockquote><h4 id="scrapy-redis两种获取方式"><a href="#scrapy-redis两种获取方式" class="headerlink" title="scrapy_redis两种获取方式"></a><a href="https://github.com/scrapy/scrapy" target="_blank" rel="noopener" title="进入详情页">scrapy_redis</a>两种获取方式</h4><blockquote><ul><li>安装git客户端,克隆<code>scrapy_redis</code>仓库到本地:</li></ul><pre><code class="bash">$ git clone &quot;https://github.com/scrapy/scrapy.git&quot;</code></pre><ul><li>直接下载项目压缩包,解压到本地</li></ul></blockquote><p>使用<code>Pycharm</code>打开项目</p></blockquote><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><blockquote><h4 id="在线安装scrapy-redis模块"><a href="#在线安装scrapy-redis模块" class="headerlink" title="在线安装scrapy_redis模块"></a>在线安装<code>scrapy_redis</code>模块</h4><pre><code class="cmd">\&gt; pip install scrapy_redis</code></pre></blockquote><h3 id="settings"><a href="#settings" class="headerlink" title="settings"></a>settings</h3><pre><code class="python">SPIDER_MODULES = [&#39;example.spiders&#39;]NEWSPIDER_MODULE = &#39;example.spiders&#39;# 使用scrapy_redis的去重类  不使用scrapy默认的去重类DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;# 使用srcpy_redis的调度器  不使用scrapy默认的调度器SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;# 控制爬虫是否允许暂停SCHEDULER_PERSIST = True# 队列形式 先进先出 先放入队列的请求先执行#SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.SpiderPriorityQueue&quot;# 栈形式  先进后出#SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.SpiderQueue&quot;#SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.SpiderStack&quot;ITEM_PIPELINES = {    &#39;example.pipelines.ExamplePipeline&#39;: 300,    # 使用redis数据库所要添加的管道,必须    &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400,}# log 日志 level 等级# LOG_LEVEL = &#39;DEBUG&#39;# Introduce an artifical delay to make use of parallelism. to speed up the# crawl.# 限制爬虫速度# DOWNLOAD_DELAY = 1# 域名为字符串 如果不写 默认为本机数据库的ipREDIS_HOST = &#39;127.0.0.1&#39;# 端口为数字REIDS_PORT = 6379</code></pre><h3 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h3><pre><code class="python">from scrapy.spiders import Rulefrom scrapy.linkextractors import LinkExtractorfrom scrapy_redis.spiders import RedisCrawlSpiderfrom ..items import ExampleItemclass MyCrawler(RedisCrawlSpider):    &quot;&quot;&quot;Spider that reads urls from redis queue (myspider:start_urls).&quot;&quot;&quot;    name = &#39;mycrawler_redis&#39;    # 推荐写法 类名: + start_urls    redis_key = &#39;mycrawler:start_urls&#39;    rules = (        # follow all links        Rule(LinkExtractor(), callback=&#39;parse_page&#39;, follow=True),        # Rule(LinkExtractor(),callback=&#39;fun&#39;)    )    # 初始化方法    def __init__(self, *args, **kwargs):        # 动态地定义允许的域名列表        # Dynamically define the allowed domains list.        domain = kwargs.pop(&#39;domain&#39;, &#39;&#39;)        self.allowed_domains = filter(None, domain.split(&#39;,&#39;))        # 固定写法,继承自当前方法的类名        super(MyCrawler, self).__init__(*args, **kwargs)    def parse_page(self, response):        # return {        #     &#39;name&#39;: response.css(&#39;title::text&#39;).extract_first(),        #     &#39;url&#39;: response.url,        # }        item = ExampleItem()        item[&#39;link&#39;] = response.url        yield item</code></pre><h3 id="修改普通爬虫项目为分布式爬虫"><a href="#修改普通爬虫项目为分布式爬虫" class="headerlink" title="修改普通爬虫项目为分布式爬虫"></a>修改普通爬虫项目为分布式爬虫</h3><blockquote><p>1.主爬虫文件<code>myspider.py</code>做如下修改,其他不做修改:</p></blockquote><pre><code class="python">import scrapyfrom ..items import MyItem # 导入数据模型from scrapy_redis.spiders import RedisSpider #导入scrapy_redis模块# 1.修改scrapy.spider为RedisSpiderclass MySpider(RedisSpider):    # 2.注释start_urls,添加redis_key    redis_key = &#39;myspider:start_urls&#39;    ...</code></pre><blockquote><p>2.<code>settings</code>添加如下代码:</p></blockquote><pre><code class="python"># 1.必须. 使用scrapy_redis的去重组件,在redis数据库里做去重DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;# 2.必须. 使用scrapy_redis的调度器,在redis里分配请求SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;# 3.可选. 在redis中保持scrapy-redis用到的各个队列,从而允许暂停和暂停后恢复,也就是不清理redis queuesSCHEDULER_PERSIST = True# redis服务器地址REDIS_HOST = &quot;127.0.0.1&quot;# redis服务器端口REDIS_PORT = 6379# 管道存储,将数据存储到redis里ITEM_PIPELINES = {    &#39;scrapy_redis.pipelines.RedisPipeline&#39;:100}</code></pre><blockquote><p>3.分发代码,选择一台机器作为服务器,切换到<code>redis</code>安装目录,修改配置文件<a href="https://zyboy.top/2018/08/14/2018-08-13-redis%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/" title="查看详情">参见</a>,如下命令开启<code>redis</code>数据库服务器</p></blockquote><pre><code class="cmd">\&gt; redis-server redis.windows.conf</code></pre><blockquote><p>4.客户端修改<code>settings.py</code>文件,将<code>REDIS_HOST</code>的值改为<code>redis</code>服务端的IP地址,在服务端打开命令行终端输入如下命令查看IP地址:</p></blockquote><pre><code class="cmd">\&gt; ipconfig</code></pre><p>或者在<code>settings.py</code>文件中添加如下代码指定服务器:</p><pre><code class="python"># 192.168.52.215 为服务器IP地址, 6397 为端口号REDIS_URL=&#39;redis://root:@192.168.52.215:6379&#39; </code></pre><p>如果不在<code>settings</code>文件中指定服务器ip,客户端也可以在<code>redis</code>配置文件中绑定服务器IP地址:</p><pre><code class="cmd">bind 192.168.52.215  ----绑定服务器IP</code></pre><blockquote><p>5.运行客户端爬虫程序,控制台可看到程序处于等待状态中(服务器端也可以开启爬虫程序,同样处于等待状态),此时在服务器端再次打开一个终端,连接<code>redis</code>数据库并向其中添加初始<code>url</code>:</p></blockquote><pre><code class="cmd">\&gt; redis-cli127.0.0.1:6379&gt; lpush myspider:start_urls http://www.......</code></pre><p>此时再次观察服务器终端,各个爬虫程序终端,可观察到程序正在爬取页面</p><blockquote><p>6.爬取到了页面,那么就要进行读取查看,可使用<code>Redis Desktop Manager</code>可视化工具连接数据库,so easy!</p></blockquote><!-- <meta http-equiv="refresh" content="2"> -->]]></content>
    
    
    
    <tags>
      
      <tag>scrapy</tag>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>redis数据库简单介绍</title>
    <link href="undefined2018/08/14/2018-08-13-redis%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
    <url>2018/08/14/2018-08-13-redis%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。</p></blockquote><a id="more"></a><blockquote><p>Redis 与其他 key - value 缓存产品有以下三个特点：</p><ul><li>Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。</li><li>Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。</li><li>Redis支持数据的备份，即master-slave模式的数据备份。</li></ul></blockquote><blockquote><p><code>redis</code>数据库下载安装</p></blockquote><ol><li>下载地址: <a href="https://github.com/MSOpenTech/redis/releases" target="_blank" rel="noopener" title="点击进入详情页">redis</a></li><li>解压安装包</li><li>将解压出来的文件夹改名为<code>redis</code></li><li>修改或添加<code>redis</code>数据库的启动配置文件<code>redis.windows.conf</code>如下内容:<pre><code class="cmd">\&gt; # bind 127.0.0.1  ------关闭绑定本地环回地址\&gt; protected-mode no ------关闭保护模块\&gt; daemonize yes     ------打开守护进程\&gt; Ctrl + s ------保存</code></pre></li><li><code>cd</code>到<code>redis</code>目录执行执行以下命令开启本地服务器监听(注意:启用本地配置文件启动<code>redis</code>服务端):<pre><code class="cmd">\&gt; redis-server redis.windows.conf</code></pre></li><li>打开另一个<code>cmd</code>终端<code>cd</code>到<code>redis</code>目录执行以下命令连接<code>redis</code>数据库并写入数据和取数据<pre><code class="cmd">\&gt; redis-cli -h 127.0.0.1\&gt; set task1 spider_baidu\&gt; set task2 spider_taobao\&gt; get task1`\&gt; get task2`</code></pre></li><li>配置环境变量</li><li>局域网下连接server服务器,需指定本地局域网下server服务器ip地址<pre><code class="cmd">\&gt; redis-cli -h 服务器ip</code></pre></li><li>这时就能访问<code>redis</code>服务器读写数据了</li></ol><!-- <meta http-equiv="refresh" content="1"> -->]]></content>
    
    
    
    <tags>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>phantomjs使用</title>
    <link href="undefined2018/08/11/2018-08-11-phantomjs%E4%BD%BF%E7%94%A8/"/>
    <url>2018/08/11/2018-08-11-phantomjs%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<pre><code class="python">from selenium import webdriver# 使用webkit无界面浏览器# 如果路径为exe启动程序的路径 那么该路径需要加一个rdriver = webdriver.PhantomJS(executable_path=r&#39;D:/phantomjs-2.1.1-windows/bin/phantomjs.exe&#39;)# 获取指定网页的数据driver.get(&#39;http://news.sohu.com/scroll/&#39;)print(driver.find_element_by_class_name(&#39;title&#39;).text)</code></pre><a id="more"></a><h3 id="以爬取淘宝搜索界面信息为例说明"><a href="#以爬取淘宝搜索界面信息为例说明" class="headerlink" title="以爬取淘宝搜索界面信息为例说明:"></a>以爬取淘宝搜索界面信息为例说明:</h3><blockquote><p><code>taobao.py</code></p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-import scrapyfrom selenium import webdriverfrom ..items import TaobaospiderItemclass TaobaoSpider(scrapy.Spider):    name = &#39;taobao&#39;    allowed_domains = [&#39;taobao.com&#39;]    start_urls = [&#39;https://s.taobao.com/search?q=%E7%AC%94%E8%AE%B0%E6%9C%AC%E7%94%B5%E8%84%91&amp;imgfile=&amp;js=1&amp;stats_click=search_radio_all%3A1&amp;initiative_id=staobaoz_20180809&amp;ie=utf8&#39;]    def __init__(self):        self.driver = webdriver.PhantomJS()    def parse(self, response):        print(&#39;================================&#39;)        print(response)        div_info = response.xpath(&#39;//div[@class=&quot;info-cont&quot;]&#39;)        print(&#39;======&#39;)        print(div_info)        for div in div_info:            title = div.xpath(&#39;.//div[@class=&quot;title-row &quot;]/a/text()&#39;).extract_first(&#39;&#39;)            price = div.xpath(&#39;.//div[@class=&quot;sale-row row&quot;]/div/span[2]/strong/text()&#39;).extract_first(&#39;&#39;)            print(&#39;---------------------------------&#39;)            print(title)            print(price)            item = TaobaospiderItem()            item[&#39;title&#39;] = title            item[&#39;price&#39;] = price            yield item    def closed(self, reason):        print(u&#39;爬虫关闭了, 原因：&#39;,reason)        self.driver.quit()</code></pre><blockquote><p><code>items.py</code></p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass TaobaospiderItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    title = scrapy.Field()    price = scrapy.Field()</code></pre><blockquote><p><code>middlewares.py</code></p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Define here the models for your spider middleware## See documentation in:# https://doc.scrapy.org/en/latest/topics/spider-middleware.htmlfrom scrapy import signalsfrom selenium import webdriverfrom scrapy.http.response.html import HtmlResponsefrom scrapy.http.response import Responseclass SeleniumSpiderMiddleware(object):    def process_request(self,request,spider):        # 当引擎从调度器取出request进行请求发送下载器之前        # 会先执行当前的爬虫中间件,在中间件里面使用selenium        # 请求这个request,拿到动态网站的数据,然后将请求返回给        # spider爬虫对象        print(&#39;++++++++++++++++++++++++++++++&#39;)        if spider.name == &#39;taobao&#39;:            # 使用爬虫文件的地址            spider.driver.get(request.url)            for x in range(1,12,2):                i = float(x) / 11                js = &#39;document.body.scrollTop=document.body.scrollHeight * %f&#39; % i                spider.driver.execute_script(js)            # 设置响应信息,响应的url为请求的url,响应的网页内容为请求网页的源码            # 响应的编码为utf-8 请求的信息为获取的信息            response = HtmlResponse(url=request.url,body=spider.driver.page_source,encoding=&#39;utf-8&#39;,request=request)            # 这个地方只能返回response            # 如果返回了response对象            # 那么可以直接跳过下载中间件            # 将response的值传递给引擎            # 引擎有传递给spider进行解析            print(&#39;+++++++++++++--------------------&#39;)            return responseclass TaobaospiderSpiderMiddleware(object):    # Not all methods need to be defined. If a method is not defined,    # scrapy acts as if the spider middleware does not modify the    # passed objects.    @classmethod    def from_crawler(cls, crawler):        # This method is used by Scrapy to create your spiders.        s = cls()        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)        return s    def process_spider_input(self, response, spider):        # Called for each response that goes through the spider        # middleware and into the spider.        # Should return None or raise an exception.        return None    def process_spider_output(self, response, result, spider):        # Called with the results returned from the Spider, after        # it has processed the response.        # Must return an iterable of Request, dict or Item objects.        for i in result:            yield i    def process_spider_exception(self, response, exception, spider):        # Called when a spider or process_spider_input() method        # (from other spider middleware) raises an exception.        # Should return either None or an iterable of Response, dict        # or Item objects.        pass    def process_start_requests(self, start_requests, spider):        # Called with the start requests of the spider, and works        # similarly to the process_spider_output() method, except        # that it doesn’t have a response associated.        # Must return only requests (not items).        for r in start_requests:            yield r    def spider_opened(self, spider):        spider.logger.info(&#39;Spider opened: %s&#39; % spider.name)class TaobaospiderDownloaderMiddleware(object):    # Not all methods need to be defined. If a method is not defined,    # scrapy acts as if the downloader middleware does not modify the    # passed objects.    @classmethod    def from_crawler(cls, crawler):        # This method is used by Scrapy to create your spiders.        s = cls()        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)        return s    def process_request(self, request, spider):        # Called for each request that goes through the downloader        # middleware.        # Must either:        # - return None: continue processing this request        # - or return a Response object        # - or return a Request object        # - or raise IgnoreRequest: process_exception() methods of        #   installed downloader middleware will be called        return None    def process_response(self, request, response, spider):        # Called with the response returned from the downloader.        # Must either;        # - return a Response object        # - return a Request object        # - or raise IgnoreRequest        return response    def process_exception(self, request, exception, spider):        # Called when a download handler or a process_request()        # (from other downloader middleware) raises an exception.        # Must either:        # - return None: continue processing this exception        # - return a Response object: stops process_exception() chain        # - return a Request object: stops process_exception() chain        pass    def spider_opened(self, spider):        spider.logger.info(&#39;Spider opened: %s&#39; % spider.name)</code></pre><blockquote><p><code>pipelines.py</code></p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Define your item pipelines here## Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport sqlite3class TaobaospiderPipeline(object):    def __init__(self):        self.con = sqlite3.connect(&#39;taobaoDB&#39;)        self.cursor = self.con.cursor()        self.cursor.execute(&#39;create table if not exists taobaoTable(title text,price text)&#39;)    def process_item(self, item, spider):        self.cursor.execute(&#39;insert into taobaoTable VALUES (&quot;{}&quot;,&quot;{}&quot;)&#39;.format(item[&#39;title&#39;],item[&#39;price&#39;]))        self.con.commit()        return item    def close_spider(self,spider):        self.cursor.close()        self.con.commit()</code></pre><blockquote><p><code>settings.py</code></p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Scrapy settings for taobaospider project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:##     https://doc.scrapy.org/en/latest/topics/settings.html#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#     https://doc.scrapy.org/en/latest/topics/spider-middleware.htmlBOT_NAME = &#39;taobaospider&#39;SPIDER_MODULES = [&#39;taobaospider.spiders&#39;]NEWSPIDER_MODULE = &#39;taobaospider.spiders&#39;# Crawl responsibly by identifying yourself (and your website) on the user-agent#USER_AGENT = &#39;taobaospider (+http://www.yourdomain.com)&#39;# Obey robots.txt rules# ROBOTSTXT_OBEY = True# Configure maximum concurrent requests performed by Scrapy (default: 16)#CONCURRENT_REQUESTS = 32# Configure a delay for requests for the same website (default: 0)# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs#DOWNLOAD_DELAY = 3# The download delay setting will honor only one of:#CONCURRENT_REQUESTS_PER_DOMAIN = 16#CONCURRENT_REQUESTS_PER_IP = 16# Disable cookies (enabled by default)#COOKIES_ENABLED = False# Disable Telnet Console (enabled by default)#TELNETCONSOLE_ENABLED = False# Override the default request headers:#DEFAULT_REQUEST_HEADERS = {#   &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,#   &#39;Accept-Language&#39;: &#39;en&#39;,#}# Enable or disable spider middlewares# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html# SPIDER_MIDDLEWARES = {#    &#39;taobaospider.middlewares.TaobaospiderSpiderMiddleware&#39;: 543,# }# Enable or disable downloader middlewares# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.htmlDOWNLOADER_MIDDLEWARES = {    &#39;taobaospider.middlewares.SeleniumSpiderMiddleware&#39;:1}# Enable or disable extensions# See https://doc.scrapy.org/en/latest/topics/extensions.html#EXTENSIONS = {#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,#}# Configure item pipelines# See https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = {   &#39;taobaospider.pipelines.TaobaospiderPipeline&#39;: 300,}# Enable and configure the AutoThrottle extension (disabled by default)# See https://doc.scrapy.org/en/latest/topics/autothrottle.html#AUTOTHROTTLE_ENABLED = True# The initial download delay#AUTOTHROTTLE_START_DELAY = 5# The maximum download delay to be set in case of high latencies#AUTOTHROTTLE_MAX_DELAY = 60# The average number of requests Scrapy should be sending in parallel to# each remote server#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# Enable showing throttling stats for every response received:#AUTOTHROTTLE_DEBUG = False# Enable and configure HTTP caching (disabled by default)# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings#HTTPCACHE_ENABLED = True#HTTPCACHE_EXPIRATION_SECS = 0#HTTPCACHE_DIR = &#39;httpcache&#39;#HTTPCACHE_IGNORE_HTTP_CODES = []#HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>scrapy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scrapy中间件</title>
    <link href="undefined2018/08/11/2018-08-11-scrapy%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    <url>2018/08/11/2018-08-11-scrapy%E4%B8%AD%E9%97%B4%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="中间件多用于设置请求头信息-请求数据等"><a href="#中间件多用于设置请求头信息-请求数据等" class="headerlink" title="中间件多用于设置请求头信息,请求数据等"></a>中间件多用于设置请求头信息,请求数据等</h3><a id="more"></a><pre><code class="python"># -*- coding: utf-8 -*-# Define here the models for your spider middleware## See documentation in:# https://doc.scrapy.org/en/latest/topics/spider-middleware.htmlfrom scrapy import signalsfrom fake_useragent import UserAgent# 爬虫中间件 设置在请求过程中的信息class BaiduspiderSpiderMiddleware(object):    # Not all methods need to be defined. If a method is not defined,    # scrapy acts as if the spider middleware does not modify the    # passed objects.    @classmethod    # 创建爬虫时调用    def from_crawler(cls, crawler):        print(&#39;爬虫创建&#39;)        # This method is used by Scrapy to create your spiders.        s = cls()        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)        return s    # 处理下载器返回的response对象    def process_spider_input(self, response, spider):        print(&#39;in函数被执行&#39;)        # Called for each response that goes through the spider        # middleware and into the spider.        # Should return None or raise an exception.        return None    # 当response对象被spider解析完成后,返回结果经过这个函数    def process_spider_output(self, response, result, spider):        print(&#39;out函数被执行&#39;)        # Called with the results returned from the Spider, after        # it has processed the response.        # Must return an iterable of Request, dict or Item objects.        for i in result:            yield i    # 当爬虫的中间件出现异常的时候被调用    def process_spider_exception(self, response, exception, spider):        print(&#39;爬虫出现异常&#39;)        # Called when a spider or process_spider_input() method        # (from other spider middleware) raises an exception.        # Should return either None or an iterable of Response, dict        # or Item objects.        pass    # 当爬虫开始的时候 从start_urls中发起request    def process_start_requests(self, start_requests, spider):        print(&#39;start_url请求启动&#39;)        # Called with the start requests of the spider, and works        # similarly to the process_spider_output() method, except        # that it doesn’t have a response associated.        # Must return only requests (not items).        for r in start_requests:            yield r    # 当爬虫程序启动的时候执行    def spider_opened(self, spider):        print(&#39;爬虫程序启动&#39;)        spider.logger.info(&#39;Spider opened: %s&#39; % spider.name)&quot;&quot;&quot;1.请求在开始的时候,会优先从settings里面找User-Agent,如果里面没有设置会有一个默认的&#39;User-Agent&#39;:&#39;scrapy&#39;2.设置User-Agent的另外一种方式 就是在这里设置scrapy内部会启动一个爬虫中间件 UserAgentMiddleware 设置请求信息的话需要在这里面设置,因此此中间件包含了一个&quot;&quot;&quot;class RandomUserAgentMiddleware(object):    def __init__(self,crawler):        super(RandomUserAgentMiddleware).__init__()        self.ua = UserAgent()        # 从配置文件settings中读取Random_ua_type值,如果不存在        # 则采用默认的random进行随机        self.ua_type = crawler.settings.get(&#39;RANDOM_UA_TYPE&#39;,&#39;random&#39;)    # 创建爬虫开始的方法    # 注意: 方法和参数必须和系统方法和参数保持一致    @classmethod    def from_crawler(cls,crawler):        # 返回当前response的对象        print(&#39;user-agent111111111111111111111111111&#39;)        return cls(crawler)    # 当进程进行请求的时候    def process_request(self,request,spider):        print(&#39;user-agent22222222222222222222222222222&#39;)        def user_agent():            return getattr(self.ua,self.ua_type)        request.headers.setdefault(&#39;User-Agent&#39;,user_agent())# 下载中间件 设置下载文件信息class BaiduspiderDownloaderMiddleware(object):    # Not all methods need to be defined. If a method is not defined,    # scrapy acts as if the downloader middleware does not modify the    # passed objects.    @classmethod    def from_crawler(cls, crawler):        # This method is used by Scrapy to create your spiders.        s = cls()        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)        return s    def process_request(self, request, spider):        # Called for each request that goes through the downloader        # middleware.        # Must either:        # - return None: continue processing this request        # - or return a Response object        # - or return a Request object        # - or raise IgnoreRequest: process_exception() methods of        #   installed downloader middleware will be called        return None    def process_response(self, request, response, spider):        # Called with the response returned from the downloader.        # Must either;        # - return a Response object        # - return a Request object        # - or raise IgnoreRequest        return response    def process_exception(self, request, exception, spider):        # Called when a download handler or a process_request()        # (from other downloader middleware) raises an exception.        # Must either:        # - return None: continue processing this exception        # - return a Response object: stops process_exception() chain        # - return a Request object: stops process_exception() chain        pass    def spider_opened(self, spider):        spider.logger.info(&#39;Spider opened: %s&#39; % spider.name)</code></pre><p><code>settings.py</code>文件中启用爬虫中间件</p><pre><code class="python"># Enable or disable spider middlewares# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html# 启用爬虫中间件SPIDER_MIDDLEWARES = {   &#39;baiduspider.middlewares.BaiduspiderSpiderMiddleware&#39;: 543,    &#39;baiduspider.middlewares.RandomUserAgentMiddleware&#39;:555}</code></pre><!-- <meta http-equiv="refresh" content="1"> -->]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>scrapy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scrapy帮助</title>
    <link href="undefined2018/08/11/2018-08-11-scrapy%E5%B8%AE%E5%8A%A9/"/>
    <url>2018/08/11/2018-08-11-scrapy%E5%B8%AE%E5%8A%A9/</url>
    
    <content type="html"><![CDATA[<blockquote><p>爬虫文件目录spider下,创建任意名称的<code>.py</code>文件写入如下内容,右键运行,可在pycharm中模拟命令行终端</p></blockquote><a id="more"></a><pre><code class="python">form scrapy import cmdline# blabla为爬虫文件的`name`值cmdline.execute(&quot;scrapy crawl blabla&quot;.split())</code></pre><blockquote><p>爬虫文件中的<code>name</code>,<code>allowed_domains</code>和<code>start_urls</code>:</p></blockquote><pre><code class="python">name = &#39;&#39;   --------------# 爬虫文件spider的名称allowed_domains = [&#39;&#39;] ----------------# 允许爬取的域名# 通常会修改爬虫程序的start_urlsstart_urls = [&#39;&#39;] -----------------# 开始爬取的网址,多个网址时,多线程进行请求</code></pre><!-- more --><blockquote><p><em>爬取网站过程</em></p><ol><li><code>items.py</code>文件定义数据模型,也就是用于存储主爬虫程序爬取的内容的字段</li><li>然后在爬虫文件中导入<code>from ..items import</code> 自动生成的爬虫item类</li><li>初始化数据模型对象,对数据模型字段赋值,<code>yield</code>逐条返回item</li><li>返回的<code>item</code>进入管道文件<code>pipelines</code>进行处理和返回,以及自定义管道</li><li><code>middlewares</code>中间件多用于请求数据,同时自定义设置请求头信息,自定义中间件</li><li><code>settings</code>文件用于配置项目信息,包括请求信息,管道调用和优先级,延迟delay,中间件等等</li></ol></blockquote><blockquote><p> 主爬虫文件中的<code>def parse(self,response)</code>函数</p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-import scrapyclass BaiduspiderSpider(scrapy.Spider):    # 爬虫名    name = &#39;baiduSpider&#39;    # 允许爬虫的范围    # allowed_domains = [&#39;baidu.com&#39;]    start_urls = [&#39;http://www.baidu.com/&#39;]    # response 为scrapy的默认下载中间请求start_urls后返回的结果    def parse(self, response):        # 请求的响应文本        # print(response.text)        # body为响应体        # print(response.body)        # 响应头        # print(response.headers)        # 获取当前状态        # print(response.status)        code = response.body.decode()        # 获取的response可直接使用xpath获取内容        result = response.xpath(&#39;.//div[@class=&quot;cover&quot;]/@cover-text&#39;)        # 仍能对其使用xpath取数据        result = result.xpath(......)        # 此时`result`为`[&#39;selector=&lt;...&gt;,data=&lt;...&gt;&#39;]`样式        result = result.extract()  # 使其转化为列表        result = result[0]    # 取第一个元素        # 也可以一步到位:        # retult = response.xpath(&#39;.//div[@class=&quot;cover&quot;]/@cover-text&#39;).extract_first(&#39;&#39;)        # 将items.py作为模块中导入其中的class类`MokoItem`        from ..items import MokoItem        for ul in ul_list:            # 初始化一个对象MokoItem类            item = MokoItem()            # 为数据模型中的字段赋值            item[&#39;title&#39;] = retult            # 逐个数据模型每个item对象            yield item</code></pre><blockquote><p>在控制台运行爬虫时可以同时保存返回的item为本地文件</p></blockquote><pre><code class="python"># 将文件存储为指定类型 支持四种数据类型text,json,xml,csvcrapy crwl meikong -o meikong.xml# 转换编码scrapy crawl meikong -o mei.json -s FEED_EXPORT_ENCODING=UTF-8</code></pre><blockquote><p><code>settings</code>部分配置说明</p></blockquote><pre><code class="python">BOT_NAME = &#39;baidu&#39;# 爬虫所在地SPIDER_MODULES = [&#39;baidu.spiders&#39;]NEWSPIDER_MODULE = &#39;baidu.spiders&#39;# 遵守爬虫协议ROBOTSTXT_OBEY = False# Configure maximum concurrent requests performed by Scrapy (default: 16)# 最大请求并发量 默认16# CONCURRENT_REQUESTS = 32# configure 配置 请求延迟# Configure a delay for requests for the same website (default: 0)# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs#DOWNLOAD_DELAY = 3# The download delay setting will honor only one of:#CONCURRENT_REQUESTS_PER_DOMAIN = 16#CONCURRENT_REQUESTS_PER_IP = 16# Disable cookies (enabled by default)# 是否使用cookie#COOKIES_ENABLED = False# 请求头信息,可添加用户表示User-AgentDEFAULT_REQUEST_HEADERS = {  &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,  &#39;Accept-Language&#39;: &#39;en&#39;}SPIDER_MIDDLEWARES = {        # 值越小,优先级越高,优先级越高,越先执行   &#39;baidu.middlewares.BaiduSpiderMiddleware&#39;: 543,}# 下载中间件OWNLOADER_MIDDLEWARES = {        # 值越小,优先级越高,优先级越高,越先执行   &#39;baidu.middlewares.BaiduDownloaderMiddleware&#39;: 543,}# Enable or disable extensions 是否进行扩展# See https://doc.scrapy.org/en/latest/topics/extensions.html#EXTENSIONS = {#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,#}ITEM_PIPELINES = {    # 值越小,优先级越高,优先级越高,越先执行   &#39;baidu.pipelines.BaiduPipeline&#39;: 1,}</code></pre><blockquote><p>主爬虫文件中图片和文件链接字段<code>[]</code>和回调函数<code>callback=</code>.</p><p>`settings.py’文件中使用系统文件,图片处理管道下载图片和文件.</p></blockquote><ul><li>spider.py<pre><code class="pthon"># -*- coding: utf-8 -*-import scrapyfrom ..items import ImagenetItem</code></pre></li></ul><p>class ImageSpider(scrapy.Spider):<br>    name = ‘image’<br>    allowed_domains = [‘pic.netbian.com’]<br>    # 请求最开始的url<br>    start_urls = [‘<a href="http://pic.netbian.com/4kmeishi/&#39;]" target="_blank" rel="noopener">http://pic.netbian.com/4kmeishi/&#39;]</a></p><pre><code>def parse(self, response):    # 根据响应来找到制定的内容 现在找的是img的src属性    img_list = response.xpath(&#39;//ul[@class=&quot;clearfix&quot;]/li/a/img/@src&#39;)    # print(img_list)    # 找到多个属性值 遍历    for img in img_list:        # 使用在items.py中定义的数据模型item        item = ImagenetItem()        img_src =&#39;http://pic.netbian.com&#39; + img.extract()        # print(img_src)        # 将下载地址放入数据模型中        # 下载地址要包在list当中        item[&#39;img_src&#39;] = [img_src]        item[&#39;fileUrl&#39;] = [fileUrl]        # 将数据传输给管道        yield item    next_url = response.xpath(&#39;//div[@class=&quot;page&quot;]/a[text()=&quot;下一页&quot;]/@href&#39;).extract()    if len(next_url) != 0:        url = &#39;http://pic.netbian.com&#39; + next_url[0]        # 将url传给scrapy.Request  得到的结果继续用self.parse继续处理        yield scrapy.Request(url,callback=self.parse)</code></pre><pre><code>* settings.py```pythonITEM_PIPELINES = {   # &#39;imageNet.pipelines.ImagenetPipeline&#39;: 300,   #  scrapy中专门负责图片下载的管道    &#39;scrapy.pipelines.images.ImagesPipeline&#39; : 1,    # 文件下载管道    &#39;scrapy.pipelines.files.FilesPipeline&#39;:2}# 体片的存储路径IMAGES_STORE = &#39;D:/imageDownLoad&#39;# 图片的下载地址 根据item中的字段来设置哪一个内容需要被下载IMAGES_URLS_FIELD = &#39;img_src&#39;FILES_STORE = &#39;d:/qishu/book/&#39;FILES_URLS_FIELD = &#39;fileUrl&#39;</code></pre><blockquote><p>自定义管道文件将items字段保存到<code>sqlite</code>数据库</p><p>注意<code>settings.py</code>文件中启用管道</p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Define your item pipelines here## Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport sqlite3class HongxiutianxiangPipeline(object):    def process_item(self, item, spider):        return itemclass HongXiuDBPipeline(object):    def open_spider(self,spider):        self.con = sqlite3.connect(&#39;hongxiuDB&#39;)        self.cursor = self.con.cursor()        self.cursor.execute(&#39;create table if not exists bookTable(name text,author text,img text,intro text)&#39;)        self.con.commit()    def process_item(self,item,spider):        print(&#39;--------------------------------------&#39;)        self.cursor.execute(&#39;insert into bookTable VALUES (&quot;{}&quot;,&quot;{}&quot;,&quot;{}&quot;,&quot;{}&quot;)&#39;.format(item[&#39;name&#39;],item[&#39;author&#39;],item[&#39;img&#39;],item[&#39;intro&#39;]))        self.con.commit()        return item    def close_spider(self,spider):        self.cursor.close()        self.con.close()</code></pre><blockquote><p>自定义管道保存items到json文件中</p></blockquote><pre><code class="python"># -*- coding: utf-8 -*-# Define your item pipelines here## Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html# 用来打开指定文件,并且对文件进行转码 防止出现乱码问题import codecsimport jsonimport osclass XiaoshuoPipeline(object):    def __init__(self):        # w 写文件        # w+读写文件 r+ 读写文件        # 前者读写文件 如果文件不存在则创建        # 后者读写文件 如果文件不存在 则抛出异常        self.file = codecs.open(filename=&#39;book.json&#39;,mode=&#39;w+&#39;,encoding=&#39;utf-8&#39;)        self.file.write(&#39;&quot;list&quot;:[&#39;)    # 如果想要将数据写入本地或者使用数据库的时候 这个方法需要保留    def process_item(self, item, spider):        # 将item对象转化成一个字典对象        res = dict(item)        # dumps将字典对象转化成字符串 ascii编码是否可用        # 如果直接将字典形式的数据写入到文件当中  会发生错误        # 所以需要将字典形式的值,转化为字符串写入文件        str  = json.dumps(res,ensure_ascii=False)        # 将数据写入到文件当中        self.file.write(str)        self.file.write(&#39;,\n&#39;)        return item    def open_spider(self,spider):        print(&#39;爬虫开始&#39;)    def close_spider(self,spider):        # 删除文件当中最后一个字符        # -1 表示偏移量        # SEEK_END 定位到文件最后一个字符        self.file.seek(-1,os.SEEK_END)        # 开始执行        self.file.truncate()        self.file.seek(-1,os.SEEK_END)        self.file.truncate()        self.file.write(&#39;]&#39;)        self.file.close()        print(&#39;爬虫结束&#39;)</code></pre><!-- <meta http-equiv="refresh" content="1"> -->]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>scrapy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>phantomjs环境配置</title>
    <link href="undefined2018/08/10/2018-08-09-phantomjs/"/>
    <url>2018/08/10/2018-08-09-phantomjs/</url>
    
    <content type="html"><![CDATA[<h3 id="phantomjs简介"><a href="#phantomjs简介" class="headerlink" title="phantomjs简介"></a>phantomjs简介</h3><p>有时，我们需要浏览器处理网页，但并不需要浏览，比如生成网页的截图、抓取网页数据等操作。PhantomJS的功能，就是提供一个浏览器环境的命令行接口，你可以把它看作一个“虚拟浏览器”，除了不能浏览，其他与正常浏览器一样。它的内核是WebKit引擎，不提供图形界面，只能在命令行下使用，我们可以用它完成一些特殊的用途。</p><a id="more"></a><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><blockquote><ol><li>下载<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener" title="详情页">phantomjs</a>,选择适合平台</li></ol></blockquote><blockquote><ol start="2"><li>解压压缩包到任意位置,例如我的:</li></ol></blockquote><p><img src="/img/Pictures/phantomjs/1.png" srcset="/img/loading.gif" alt="img" title="点击查看大图"></p><blockquote><ol start="3"><li>添加系统环境变量,将刚才解压后的路径添加进去,如图所示:</li></ol></blockquote><p><img src="/img/Pictures/phantomjs/2.png" srcset="/img/loading.gif" alt="img" title="点击查看大图"></p><blockquote><ol start="4"><li>使用:通常配合selenium使用</li></ol></blockquote><pre><code class="python">from selenium import webdriverdriver = webdriver.PhantomJS()driver.get(&#39;http://www.baidu.com&#39;)code = driver.page_sourceprint(code)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mysql安装</title>
    <link href="undefined2018/08/09/2018-08-08-mysql%E5%AE%89%E8%A3%85/"/>
    <url>2018/08/09/2018-08-08-mysql%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<p>Mysql安装详细步骤:</p><a id="more"></a><blockquote><ol><li>接受使用条款</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql2.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="2"><li>选择默认开发版即可</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql3.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="3"><li>安装路径默认即可</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql4.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="4"><li>安装产品及依赖组件先执行<code>Excute</code>再next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql5.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="5"><li>安装<code>Excute</code></li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql6.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="6"><li>next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql7.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="7"><li>创建密码</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql8.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="8"><li>默认next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql9.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="9"><li>finish</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql10.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="10"><li>next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql11.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="11"><li>登陆</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql12.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="12"><li>Excute</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql13.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="13"><li>finish</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql14.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="14"><li>next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql15.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="15"><li>next</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql16.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><blockquote><ol start="16"><li>静等片刻</li></ol></blockquote><p><img src="/img/Pictures/mysql/mysql17.png" srcset="/img/loading.gif" alt="img" title="双击查看大图"></p><!-- <meta http-equiv="refresh" content="1"> -->]]></content>
    
    
    
    <tags>
      
      <tag>mysql</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>xpath获取文本</title>
    <link href="undefined2018/08/04/2018-08-03-xpath%E8%8E%B7%E5%8F%96%E6%96%87%E6%9C%AC/"/>
    <url>2018/08/04/2018-08-03-xpath%E8%8E%B7%E5%8F%96%E6%96%87%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h3 id="以爬取百度贴吧content说明"><a href="#以爬取百度贴吧content说明" class="headerlink" title="以爬取百度贴吧content说明:"></a>以爬取百度贴吧content说明:</h3><ol><li>获取最外边标签,遍历内部所有子标签,获取标签文本<pre><code class="python">content =div.xpath(&#39;.//cc/div[@class=&quot;d_post_content j_d_post_content &quot;]//text()&#39;).extract()`</code></pre></li><li>正则去掉所有标签 &lt;.*?&gt;      re.sub(‘’,content)<pre><code class="python">content = div.xpath(&#39;.//cc/div[@class=&quot;d_post_content j_d_post_content &quot;]&#39;)</code></pre></li></ol><p>pattern = re.compile(‘&lt;.*?&gt;’)</p><p>content = pattern.sub(‘’,content)</p><pre><code>3. /text() 获取标签的文本   //text()获取标签以及子标签的文本```pythoncontent_list = div.xpath(&#39;.//div[@class=&quot;d_post_content j_d_post_content &quot;]//text()&#39;).extract()</code></pre><ol start="4"><li>使用xpath(‘string(.)’)这种方式获取所有文本 并且拼接<pre><code class="python"> content = div.xpath(&#39;.//div[@class=&quot;d_post_content j_d_post_content &quot;]&#39;).xpath(&#39;string(.)&#39;).extract()[0] + &#39;\n&#39;</code></pre>文本内容获取之后<code>print(content)</code>查看内容,如需处理格式,则如下:</li></ol><pre><code class="python">remove = re.compile(&#39;\s&#39;)content = &#39;&#39;for string in content_list:    string = remove.sub(&#39;&#39;,string)    content += stringself.f.write(content + &#39;\n&#39;)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>酷安</title>
    <link href="undefined2018/08/01/2018-07-31-%E9%85%B7%E5%AE%89/"/>
    <url>2018/08/01/2018-07-31-%E9%85%B7%E5%AE%89/</url>
    
    <content type="html"><![CDATA[<h2 id="爬取酷安app信息"><a href="#爬取酷安app信息" class="headerlink" title="爬取酷安app信息"></a>爬取酷安app信息</h2><a id="more"></a><pre><code class="python">from urllib.request import Request,urlopenfrom lxml import etreefrom fake_useragent import UserAgentimport xlwtclass Kuan(object):    def __init__(self):        self.base_url = &#39;https://www.coolapk.com/apk?p=642&#39;        self.headers = UserAgent()        self.workBook = None        self.sheet = None        self.record = 1    def spider(self):        self.createWorkBook()        self.get_code_with_url(self.base_url)        self.workBook.save(&#39;D:/酷安.xls&#39;)    def get_code_with_url(self,url):        headers = {            &#39;User-Agent&#39;:self.headers.random        }        print(headers)        request = Request(url,headers=headers)        response = urlopen(request).read().decode()        print(response)        code = etree.HTML(response)        items = code.xpath(&#39;//div[@class=&quot;app_left_list&quot;]/a&#39;)        for item in items:            title = item.xpath(&#39;.//p[@class=&quot;list_app_title&quot;]/text()&#39;)[0]            info = item.xpath(&#39;.//p[@class=&quot;list_app_info&quot;]//text()&#39;)            grade = info[0].split(&#39; &#39;)[0]            large = info[0].split(&#39; &#39;)[2]            download = info[1].split(&#39; &#39;)[0]            status = info[1].split(&#39; &#39;)[2]            # print(grade,large,download,status)            des = item.xpath(&#39;.//p[@class=&quot;list_app_description&quot;]/text()&#39;)            if len(des) == 0:                des = &#39;&#39;            else:                des = des[0]            list = [title,grade,large,download,status,des]            for index,item in enumerate(list):                self.sheet.write(self.record,index,item)            self.record += 1        self.next_page(code)    def next_page(self,code):        next_page_url = code.xpath(&#39;//ul[@class=&quot;pagination&quot;]/li[last()-1]/a/@href&#39;)[0]        if next_page_url == &#39;javascript:void(0);&#39;:            print(&#39;已经到最后一页了&#39;)            return        self.get_code_with_url(&#39;https://www.coolapk.com&#39;+next_page_url)    def createWorkBook(self):        list = [&#39;应用名称&#39;,&#39;评分&#39;,&#39;大小&#39;,&#39;下载量&#39;,&#39;更新状态&#39;,&#39;应用简介&#39;]        self.workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)        self.sheet = self.workBook.add_sheet(&#39;app&#39;)        for index,item in enumerate(list):            self.sheet.write(0,index,item)kuan = Kuan()kuan.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>selenium模拟自然人操作美食杰网站</title>
    <link href="undefined2018/08/01/2018-07-31-selenium%E6%A8%A1%E6%8B%9F%E8%87%AA%E7%84%B6%E4%BA%BA%E6%93%8D%E4%BD%9C%E7%BE%8E%E9%A3%9F%E6%9D%B0%E7%BD%91%E7%AB%99/"/>
    <url>2018/08/01/2018-07-31-selenium%E6%A8%A1%E6%8B%9F%E8%87%AA%E7%84%B6%E4%BA%BA%E6%93%8D%E4%BD%9C%E7%BE%8E%E9%A3%9F%E6%9D%B0%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Pycharm 控制浏览器打开美食杰网站:[<a href="https://www.meishij.net/]" target="_blank" rel="noopener">https://www.meishij.net/]</a><br>鼠标自动移动到<code>菜单大全</code>,点击<code>孕妇</code>标签,进入页面,滚动条自动下滑<br>同时控制台打印菜名</p></blockquote><a id="more"></a><pre><code class="python">from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.common.action_chains import ActionChainsimport timedriver = webdriver.Chrome()driver.get(&#39;https://www.meishij.net/&#39;)# 鼠标移动到菜谱大全order = driver.find_element_by_css_selector(&#39;.hasmore a&#39;)WebDriverWait(driver,10).until(lambda driver: order.is_displayed())action = ActionChains(driver).move_to_element(order).perform()# 鼠标点击孕妇标签women = driver.find_element_by_link_text(u&#39;孕妇&#39;)WebDriverWait(driver,10).until(lambda driver: women.is_displayed())women.click()for page in range(1,3):    print(&#39;正在查询第{}页&#39;.format(page))    # 设置滚动条每次滚动四分之一    for long in range(1,5):        x = float(long) / 4        js = &#39;document.documentElement.scrollTop = document.documentElement.scrollHeight * %f&#39; %x        driver.execute_script(js)        time.sleep(3)    # 获取菜名    titles = driver.find_elements_by_xpath(&#39;//div[@class=&quot;listtyle1&quot;]/a&#39;)    # titles = driver.find_elements_by_xpath(&#39;//div[@class=&quot;listtyle1&quot;]/a/@title&#39;)    for title in titles:        title = title.get_attribute(&#39;title&#39;)        print(title)    # 下一页    next_page = driver.find_element_by_link_text(u&#39;下一页&#39;).click()time.sleep(20)driver.quit()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git使用</title>
    <link href="undefined2018/07/31/2018-07-30-git%E4%BD%BF%E7%94%A8/"/>
    <url>2018/07/31/2018-07-30-git%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录:"></a>目录:</h1><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><ol><li><a href="#git的初始化">git的初始化</a><ul><li>Git配置</li></ul></li><li><a href="#获得一个Git仓库">获得一个Git仓库</a><ul><li>Clone一个仓库</li><li>初始化一个新的仓库</li></ul></li><li><a href="#正常的工作流程">正常的工作流程</a><ul><li>创建或修改文件</li><li>使用<code>git add</code>加入缓存区</li><li>使用<code>git commit</code>提交修改</li></ul></li><li><a href="#分支与合并">分支与合并</a><ul><li>创建分支</li><li>切换分支</li><li>合并分支</li><li>删除分支</li><li>撤销一个合并</li><li>快速向前合并</li></ul></li><li><a href="#Git日志">日志</a><ul><li>查看日志</li><li>日志统计</li><li>格式化日志</li><li>日志排序</li></ul></li><li><a href="#小结">小结</a></li></ol><a id="more"></a><h3 id="git的初始化"><a href="#git的初始化" class="headerlink" title="git的初始化"></a>git的初始化</h3><h4 id="git配置"><a href="#git配置" class="headerlink" title="git配置"></a>git配置</h4><p>配置全局变量,用户名和邮箱为提交<code>commit</code>时的签名</p><pre><code class="bash">`$ git config --global user.name &quot;zysxm&quot;` --zysxm 为github用户名`$ git config --global user.email &quot;451253127@qq.com&quot; ` --github注册邮箱</code></pre><p>配置完成后会在本地系统当前系统用户目录下创建<code>.gitconf</code>隐藏文件 可以输入<code>cat/echo .gitconfig</code>命令查看内容</p><pre><code class="bash">$ cat ~/.gitconfig[user]        name = zysxm        email = 451253127@qq.com上面的配置文件就是Git全局配置的文件，一般配置方法是:    git config --global &lt;配置名称&gt; &lt;配置的值&gt;配置局部变量(切换项目路径):    git config    命令不带--global选项来设置此命令会在当前目录创建`.git/config`,只针对当前项目的配置</code></pre><h3 id="获得一个Git仓库"><a href="#获得一个Git仓库" class="headerlink" title="获得一个Git仓库"></a>获得一个Git仓库</h3><pre><code>1. 从已有git仓库clone2. 新建仓库,进行版本控制</code></pre><h4 id="Clone一个仓库"><a href="#Clone一个仓库" class="headerlink" title="Clone一个仓库"></a>Clone一个仓库</h4><pre><code>git clone 仓库地址</code></pre><h4 id="初始化一个新的仓库"><a href="#初始化一个新的仓库" class="headerlink" title="初始化一个新的仓库"></a>初始化一个新的仓库</h4><pre><code class="bash">在已存在的目录`D:/newdir`下:    git initgit会输出:    Initialized empty Git repository in D:newdir/.git/输入`ls -la `出现`.git`目录,即初始化成功</code></pre><h3 id="正常的工作流程"><a href="#正常的工作流程" class="headerlink" title="正常的工作流程"></a>正常的工作流程</h3><pre><code>1. 创建,修改2. `git add` 添加创建或修改到本地缓存(index)3. `git commit`提交到本地代码库4. `git push`将本地代码库同步到远端</code></pre><h4 id="创建或修改文件"><a href="#创建或修改文件" class="headerlink" title="创建或修改文件"></a>创建或修改文件</h4><pre><code class="bash">增删改后使用`git status`查看当前git仓库状态文件处于`untarcked`状态时,使用`git add`加入缓存(index)</code></pre><h4 id="使用git-add加入缓存区"><a href="#使用git-add加入缓存区" class="headerlink" title="使用git add加入缓存区"></a>使用<code>git add</code>加入缓存区</h4><pre><code class="bash">将所有修改添加缓存区:    git add . 添加:    git add newfile查看状态:    git status此时已为`commit`做好准备使用`git diff`加上`--cached`参数,查看缓存区那些文件被修改,按`q`退出    git diff --cached不加`--cached`参数时,显示当前已做的所有更改</code></pre><h4 id="使用git-commit提交修改"><a href="#使用git-commit提交修改" class="headerlink" title="使用git commit提交修改"></a>使用<code>git commit</code>提交修改</h4><pre><code class="bash">加入缓存区中的文件使用`git commit`提交到本地仓库    git commit -m &quot;add newfile&quot;使新建的本地仓库关联远程仓库:    git remote add origin 远程仓库地址同步:    git push origin master</code></pre><h3 id="分支与合并"><a href="#分支与合并" class="headerlink" title="分支与合并"></a>分支与合并</h3><blockquote><p><code>Git</code>的分支<code>branch</code>是将代码提交到主线(master)之外,同时不会影响代码库主线.</p><p>分支的作用主要体现在多人合作开发,比如A码农负责一个独立功能需要一个月时间完成,就可以创建一个分支,只把该功能的代码提交到这个分支,而其他码农仍然可以继续使用主线开发,A码农每天的提交不会对他们造成任何影响.测试通过后再将功能分支提交到主线.</p></blockquote><h4 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a>创建分支</h4><blockquote><p>一个<code>Git</code>仓库可以维护多个开发分支,现在创建分支</p></blockquote><pre><code class="bash">`$ git branch my_branch`</code></pre><blockquote><p>运行<code>git branch</code>查看当前分支列表,以及目前开发环境处在哪个分支上:</p></blockquote><pre><code class="bash">45125@DESKTOP-I0F25I8 MINGW64 /d/Git (master)$ git branch* master  my_branch</code></pre><h4 id="切换分支"><a href="#切换分支" class="headerlink" title="切换分支"></a>切换分支</h4><blockquote><p><code>my_branch</code>是刚刚创建的分支,<code>master</code>是<code>Git</code>系统默认创建的主分支,<code>*</code>标识当前工作在哪个分支下,输入<code>git checkout 分支名</code> 可以切换到其他分支</p></blockquote><pre><code class="bash">45125@DESKTOP-I0F25I8 MINGW64 /d/Git (master)$ git checkout my_branchSwitched to branch &#39;my_branch&#39;# 切换成功45125@DESKTOP-I0F25I8 MINGW64 /d/Git (my_branch)$</code></pre><blockquote><p>切换到<code>my_branch</code>分支后,编辑文件,再提交(commit)改动,最后切换回<code>master</code>分支</p></blockquote><pre><code class="bash"># 修改文件$ echo &#39;hello git&#39; &gt; file1.txt# 查看当前状态$ git statusOn branch my_branch# 添加并提交file.text的修改$ git add file1.txt$ git commit -m &quot;update file1.text&quot;# 查看file1.text的内容$ cat file1.txthello git# 切换分支$ git checkout masterSwitched to branch &#39;master&#39;A       file1.txt45125@DESKTOP-I0F25I8 MINGW64 /d/Git (master)$</code></pre><blockquote><p>这时查看<code>file1.text</code>的内容,已经看不到在<code>my_branch</code>分支下做的改动.</p></blockquote><h4 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h4><blockquote><p>在<code>master</code>主分支下再做一些不同的修改,然后使用<code>merge</code>将<code>my_branch</code>分支合并到<code>master</code>主分支:</p></blockquote><pre><code class="bash"># 切换到主分支$ git checkout master# 将`my_branch`合并到`master`,&#39;-m&#39;仍然为注释$ git merge -m &#39;merge my_branch branch&#39; my_branch# 提交$ git add .$ git commit</code></pre><blockquote><p>当<code>master</code>主分支和<code>my_branch</code>分支同时更改了<code>hello.text</code>文件,合并显然会报错</p></blockquote><pre><code class="bash">$ git merge &quot;merge my_branch&quot; my_branchmerge: merge my_branch - not something we can merge</code></pre><blockquote><p>提示如下:</p></blockquote><pre><code class="bash">45125@DESKTOP-I0F25I8 MINGW64 /d/Git(master)$ git merge &quot;merge my_branch&quot; my_branchAuto-merging hello.txtCONFLICT (content): Merge conflict in hello.txt -----提示合并冲突Automatic merge failed; fix conflicts and then commit the result. --解决冲突并提交</code></pre><blockquote><p>此时用户显示为<code>master|MERGING</code>:</p></blockquote><pre><code class="bash">45125@DESKTOP-I0F25I8 MINGW64 /d/Git (master|MERGING)$</code></pre><blockquote><p>需要手动合并冲突文件,并添加到缓存<code>add</code>,然后提交<code>commit</code>才能继续后续执行操作.</p></blockquote><h4 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h4><blockquote><p>当合并完成后,不再需要分支<code>my_branch</code>,就可以删除分支:</p></blockquote><pre><code class="bash">45125@DESKTOP-I0F25I8 MINGW64 /d/Git_new (master)$ git branch -d my_branchDeleted branch my_branch (was 4be8b88).</code></pre><blockquote><p>需注意 <code>git branch -d</code> 只能删除已经别合并的分支,若要强制删除某个分支,使用<code>git branch -D</code></p></blockquote><h4 id="撤销一个合并"><a href="#撤销一个合并" class="headerlink" title="撤销一个合并"></a>撤销一个合并</h4><blockquote><p>回到合并之前的状态</p></blockquote><pre><code class="bash">$ git reset --hard HEAD^</code></pre><p>查看文件,即可发现回到合并前状态</p><h4 id="快速向前合并"><a href="#快速向前合并" class="headerlink" title="快速向前合并"></a>快速向前合并</h4><h3 id="Git日志"><a href="#Git日志" class="headerlink" title="Git日志"></a>Git日志</h3><h4 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h4><blockquote><p><code>git log</code> 命令显示所有的提交<code>commit</code></p></blockquote><pre><code class="bash">$ git log</code></pre><p>回车逐行显示,<code>q</code>退出</p><p><code>git log</code>选项很多,可使用<code>git help log</code>查看,例:找出所有从’v2.5’开始在fs目录下的所有Makefile的修改:</p><pre><code class="bash">$ git log v2.5.. Makefile fs/</code></pre><p><code>Git</code>会根据<code>git log</code>命令的参数,按时间顺序显示相关的提交(commit)</p><h4 id="日志统计"><a href="#日志统计" class="headerlink" title="日志统计"></a>日志统计</h4><blockquote><p>如果使用<code>--stat</code>选项使用<code>git log</code> 会打印详细的提交记录:</p></blockquote><pre><code class="bash">$ git log --stat</code></pre><p>此命令会显示每个提交中哪个文件被修改,这些文件添加或者修改了多少行内容</p><h4 id="格式化日志"><a href="#格式化日志" class="headerlink" title="格式化日志"></a>格式化日志</h4><blockquote><p>按自己要求格式化输出日志.<code>--pretty</code> 参数实现</p></blockquote><pre><code class="bash">$ git log --pretty=oneline# 输出内容--------a5a5ade971c5cbb55a81ad6536f68ea3827e3436 merge637423f27ac26924df3f10b3be67e2160c300af1 456d7380bc7f9ebb0686c1e8e905a2398593e0d14aa hehe...$ git log --pretty=short# 输出内容--------commit 4be8b88c059d6fd162b2ca7ed5911fc71b9e448aAuthor: zysxm &lt;451253127@qq.com&gt;    hahhah...</code></pre><p>其他参数<code>medium</code>,<code>full</code>,<code>fuller</code>,<code>email</code>或<code>raw</code>.</p><p>若都不符合自己需求,也可使用<code>--pretty=format</code>参数定义格式</p><p><code>--graph</code> 选项可以可视化提交图(commit graph),会用ASCII字符来画出一个很很漂亮的提交历史(commit history)线:</p><pre><code class="bash">$ git log --graph --pretty=oneline# 输出内容* 1a9d3e56192965adf42c374f9badf8d31d766526 (HEAD -&gt; master) add from master*   37ad7837625ff73d0839c5f15c1d38502fc58662 haha|\| * 4be8b88c059d6fd162b2ca7ed5911fc71b9e448a hahhah* | f432d9841ac64fd7d6e3846a747b5da719547901 modify hello.text* |   3b2b490122c7e4c93c2c691bf3e4bd51d1383239 merge|\ \| |/| * d1abf7b663fc50a08fcd2b2ded20d7ea9685b1ae update text* |   a5a5ade971c5cbb55a81ad6536f68ea3827e3436 merge...</code></pre><h4 id="日志排序"><a href="#日志排序" class="headerlink" title="日志排序"></a>日志排序</h4><blockquote><p>日志记录可以按不同的顺序来显示,可以指定<code>--topo-order</code>参数,让提交按拓扑顺序来显示(就是子提交在他们的父提交前显示):</p></blockquote><pre><code class="bash">git log --pretty=format:&#39;%h : %s&#39; --topo-order --graph</code></pre><p><code>--reverse</code>参数逆向显示所有提交日志</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>git config: 配置相关信息</li><li>git clone: 复制仓库</li><li>git init: 初始化仓库</li><li>git add: 添加更新内容到索引中</li><li>git diff: 比较内容</li><li>git status: 获取当前项目状态</li><li>git commit: 提交</li><li>git branch: 分支相关</li><li>git checkout: 切换分支</li><li>git merge: 合并分支</li><li>git reset: 恢复版本</li><li>git log: 查看日志</li></ul><!-- <meta http-equiv="refresh" content="2"> -->]]></content>
    
    
    
    <tags>
      
      <tag>Git</tag>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人人美剧影视库爬虫</title>
    <link href="undefined2018/07/31/2018-07-30-%E4%BA%BA%E4%BA%BA%E7%BE%8E%E5%89%A7%E5%BD%B1%E8%A7%86%E5%BA%93%E7%88%AC%E8%99%AB/"/>
    <url>2018/07/31/2018-07-30-%E4%BA%BA%E4%BA%BA%E7%BE%8E%E5%89%A7%E5%BD%B1%E8%A7%86%E5%BA%93%E7%88%AC%E8%99%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="字幕组影视库"><a href="#字幕组影视库" class="headerlink" title="字幕组影视库"></a>字幕组影视库</h2><a id="more"></a><pre><code class="python">from urllib.request import Request,urlopenfrom lxml import etreeimport xlwtclass Zimuzu(object):    def __init__(self):        self.base_url = &#39;http://www.zimuzu.io/resourcelist&#39;        self.headers = {            &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;        }        self.workBook = None        self.sheet = None        self.record = 1    def spider(self):        self.create_workBook()        self.get_data_with_url(self.base_url)        self.workBook.save(&#39;人人影视库.xls&#39;)    def get_data_with_url(self,url):        request = Request(url,headers=self.headers)        response = urlopen(request).read()        code = etree.HTML(response)        items = code.xpath(&#39;//div[@class=&quot;resource-showlist has-point&quot;]/ul/li[@class=&quot;clearfix&quot;]&#39;)        print(items)        for item in items:            point = item.xpath(&#39;.//span[@class=&quot;point&quot;]//text()&#39;)            if len(point) == 2:                point = point[0] + point [1]            point = &#39;&#39;            type = item.xpath(&#39;.//h3[@class=&quot;f14&quot;]/a/strong/text()&#39;)            if len(type) == 0:                type = &#39;&#39;            type = type[0]            name = item.xpath(&#39;.//h3[@class=&quot;f14&quot;]/a/text()&#39;)[0]            num = item.xpath(&#39;.//h3[@class=&quot;f14&quot;]/font[@class=&quot;f4&quot;]/text()&#39;)            if len(num)==0:                num = &#39;&#39;            num = num[0]            try:                self.sheet.write(self.record,0,point)                self.sheet.write(self.record,1,type)                self.sheet.write(self.record,2,name)                self.sheet.write(self.record,3,num)            except Exception as e:                print(&#39;写入失败&#39;,e)            finally:                self.record += 1        self.next_page_with_code(code)    def next_page_with_code(self,code):        next_page = code.xpath(&#39;//div[@class=&quot;pages&quot;]/div/a[last()-1]/@href&#39;)        print(next_page)        if len(next_page) == 0:            print(&#39;获取完毕&#39;)            return        self.get_data_with_url(&#39;http://www.zimuzu.io&#39;+ next_page[0])    def create_workBook(self):        self.workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)        self.sheet = self.workBook.add_sheet(&#39;movies&#39;)        self.sheet.write(0,0,&#39;影视评分&#39;)        self.sheet.write(0,1,&#39;影视类型&#39;)        self.sheet.write(0,2,&#39;影视名称&#39;)        self.sheet.write(0,3,&#39;更新进度&#39;)zimuzu = Zimuzu()zimuzu.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>selenium环境配置及使用</title>
    <link href="undefined2018/07/31/2018-07-30-selenium%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <url>2018/07/31/2018-07-30-selenium%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="selenium的特点"><a href="#selenium的特点" class="headerlink" title="selenium的特点:"></a>selenium的特点:</h2><ul><li>由程序控制浏览器进行操作,而不是手动操作浏览器</li><li>程序控制浏览器进行操作的时候,速度非常慢,所以要谨慎使用selenium</li><li>使用selenium控制浏览器的时候,需要下载浏览器对应的驱动程序</li><li>selenium为开源,免费,但是更新速度没有浏览器快,不是selenium<br> 更新慢,而是浏览器是更新快,注意selenium和浏览器对应关系</li></ul><a id="more"></a><h2 id="使用selenium之前进行环境配置"><a href="#使用selenium之前进行环境配置" class="headerlink" title="使用selenium之前进行环境配置"></a>使用selenium之前进行环境配置</h2><ol><li><p>下载<code>chromedriver</code>和<code>geckodriver</code></p><ul><li><p><a href="http://npm.taobao.org/mirrors/chromedriver/" target="_blank" rel="noopener" title="淘宝镜像源">chromedriver</a></p><blockquote><p>选择适合版本下载 附<a href="https://download.mozilla.org/?product=firefox-58.0&os=win&lang=zh-CN" target="_blank" rel="noopener" title="点击下载">FireFox_58_简中版</a><br><img src="/assets/img/Pictures/chromedriver.png" srcset="/img/loading.gif" alt="" title="效果"></p></blockquote></li><li><p><a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener" title="GitHub地址">geckodriver/releases</a></p><blockquote><p>选择对应平台下载<br><img src="/assets/img/Pictures/geckodriver1.png" srcset="/img/loading.gif" alt="" title="效果"><br><img src="/assets/img/Pictures/geckodriver2.png" srcset="/img/loading.gif" alt="" title="效果"></p></blockquote></li></ul></li><li><p>将下载好的<code>chromedriver.exe</code>和<code>geckodriver.exe</code>解压后放到<code>Anoconda</code>的安装目录中或者Python安装目录中,位置如下:</p></li></ol><blockquote><p><code>D:\Python\Anaconda\Scripts</code></p></blockquote><ol start="3"><li><p>重启Pycharm 等待加载完成</p></li><li><p><code>terminal</code>或者<code>cmd</code>安装<code>selenium</code></p></li></ol><blockquote><p><code>pip install selenium</code></p></blockquote><ol start="5"><li><p>在Pycharm中引入网页驱动并执行</p><pre><code class="python"> # 引入网页驱动 from selenium import webdriver import time # 使用网页驱动来运行google/firefox浏览器 driver = webdriver.Chrome() # 通过驱动来执行指定的网页 driver.get(&#39;http://www.baidu.com&#39;) # selenium.common.exceptions.NoSuchElementException: Message: Unable to locate element: [id=&quot;kw&quot;] # 错误原因:代码执行速度很快,但是浏览器响应很慢, # 代码执行到这的时候 浏览器里面的元素可能还没有加载完 # 所以报错找不到元素 # time.sleep(3) # 通过id查找 driver.find_element_by_id(&#39;kw&#39;).send_keys(&#39;selenium&#39;) # 通过name查找 属性 driver.find_element_by_name(&#39;wd&#39;).send_keys(&#39;csdn&#39;) # Unicode 若后面有中文 那么前面需要加一个u   r:禁止转义 driver.find_element_by_class_name(&#39;s_ipt&#39;).send_keys(u&#39;智游&#39;) # tag_name driver.find_element_by_tag_name(&#39;input&#39;).send_keys(&#39;...&#39;) # 前端 html css js # selector 选择器 driver.find_element_by_css_selector(&#39;#kw&#39;).send_keys(&#39;...&#39;) # 通过xpath语法定位一个元素 driver.find_element_by_xpath(&#39;//form[@id=&quot;form&quot;]/span/input[@id=&quot;kw&quot;]&#39;).send_keys(&#39;123&#39;) #link 链接 driver.find_element_by_link_text(&#39;贴吧&#39;) time.sleep(5) driver.close()</code></pre></li><li><p>selenium进阶</p><ul><li>按键操作</li></ul><pre><code class="python"># commom 共同的;公共的# keys 键from selenium.webdriver.common.keys import Keysfrom selenium import webdriverimport timedriver = webdriver.Chrome()driver.get(&#39;http://www.baidu.com&#39;)time.sleep(3)# 找到输入框 输入指定内容driver.find_element_by_id(&#39;kw&#39;).send_keys(&#39;selenium&#39;)time.sleep(3)# 全选输入框内的全部内容driver.find_element_by_id(&#39;kw&#39;).send_keys(Keys.CONTROL,&#39;a&#39;)time.sleep(3)driver.find_element_by_id(&#39;kw&#39;).send_keys(Keys.CONTROL,&#39;x&#39;)time.sleep(3)driver.find_element_by_id(&#39;kw&#39;).send_keys(u&#39;爬虫技巧&#39;)time.sleep(3)driver.find_element_by_id(&#39;su&#39;).click()time.sleep(5)# 退出浏览器driver.quit()</code></pre><ul><li>时间等待</li></ul><pre><code class="python"># selenium 由网页驱动驱使浏览器进行操作,速度慢时一大特点# 经常会出现代码执行完了 但是网页内容还没有加载完毕# 里面的标签还没有显示出来 如果这时候操作里面的标签# 就会爆出异常 NoSuchElementExpection# 解决的办法:时间休眠 time.sleep()# 不管页面的内容有没有加载完毕 一定要休眠指定秒数from selenium.webdriver.support.ui import WebDriverWaitfrom selenium import webdriverdriver = webdriver.Chrome()driver.get(&#39;http://www.baidu.com&#39;)# driver.find_element_by_id(&#39;kw&#39;).send_keys(&#39;hello world&#39;)button = driver.find_element_by_id(&#39;su&#39;)# WebDriverWait 网页等待# 值1: 等待的对象# 值2: 等待的时间# WebDriverWait经常和until not 一起使用 until: 直到...# lambda 匿名函数 is_displayed 是否已经显示is_visible = WebDriverWait(driver , 10).until(lambda driver: button.is_displayed())print(is_visible)button.click()# WebDriverWait() 和 time.sleep()# 1.都是让程序等待指定时间# 2.time的时间是固定的 时间长短不会随标签的加载速度而改变  # WebDriverWait时间是不固定的 等地啊多少时间要看标签的加载时间  # 和指定的固定的时间# 3.如果在指定的时间内,标签仍然没有加载出来,那么二者都会爆出异常# 隐性时间 implicility_wait</code></pre><ul><li>点击事件</li></ul><pre><code class="python">from selenium import webdriver# action 行动 chains 链from selenium.webdriver.common.action_chains import ActionChainsfrom selenium.webdriver.support.ui import WebDriverWaitimport timedriver = webdriver.Chrome()driver.get(&#39;http://www.baidu.com&#39;)# driver.find_element_by_class_name(&#39;index-logo-src&#39;)logo = driver.find_element_by_css_selector(&#39;#lg &gt; img&#39;)logo = driver.find_element_by_xpath(&#39;//div[@id=&quot;lg&quot;]/img[@class=&quot;index-logo-src&quot;]&#39;)# 双击事件ActionChains(driver).double_click(logo).perform()# context 上下文# content 内容# context_click 右击# 右键点击# WebDriverWait(driver,10).until(lambda driver : logo.is_displayed())# action = ActionChains(driver).context_click(logo)# 操作事件会跑到perform队列里面 perform实现# action.perform()# time.sleep(5)# more = driver.find_element_by_class_name(&#39;bri&#39;)# # 鼠标移动# WebDriverWait(driver,10).until(lambda driver: more.is_displayed())# action = ActionChains(driver).move_to_element(more).perform()</code></pre><ul><li>标签选择</li></ul><pre><code class="python">from selenium import webdriverfrom selenium.webdriver.common.by import Byimport os,timedriver = webdriver.Chrome()driver.get(&#39;file:///&#39; + os.path.abspath(&#39;4.index.html&#39;))# 同过标签名字找到指定标签# btns = driver.find_elements_by_tag_name(&#39;button&#39;)# print(btns)# 通过索引找到指定标签# btns[1].click()# for btn in btns:#     # 通过属性找到指定标签#     if btn.get_attribute(&#39;type&#39;) == &#39;button&#39;:#         btn.click()  # time.sleep(3)  # btn.click()# btn = driver.find_element_by_tag_name(&#39;button&#39;).click()# find_element_by_XX通过XX来找到所有标签中的第一个标签# find_elements_by_XX通过XX来找到所有符合的标签# 弹出指定的元素 如果不写索引 默认为最后一个# driver.find_elements_by_css_selector(&#39;button&#39;).pop(3).click()# [type=button] []里面为限制条件 限制选择的内容driver.find_elements_by_css_selector(&#39;button[type=button]&#39;).pop().click()# 通过...来找到指定的标签 ID,TAG,CLASS...driver.find_element(by=By.ID,value=&#39;pink&#39;).click()time.sleep(10)driver.quit()</code></pre><ul><li>window切换</li></ul><pre><code class="python">from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitimport timedriver = webdriver.Chrome()driver.get(&#39;http://www.baidu.com&#39;)# 获取当前的window对象current_window = driver.current_window_handle# 获取当前窗口编号和网页标题print(current_window,driver.title)driver.find_element_by_name(&#39;tj_trnews&#39;).click()news = WebDriverWait(driver,10).until(lambda driver:driver.find_element_by_css_selector(&#39;.hdline0 .a3&#39;))news.click()# 获取所有的窗口all_windows = driver.window_handlesprint(all_windows)for window in all_windows:  if window != current_window:      # switch 切换 to 到      driver.switch_to.window(window)# title = driver.find_element_by_tag_name(&#39;h1&#39;)time.sleep(2)title = driver.find_element_by_css_selector(&#39;.text_title h1&#39;).text# WebDriverWait(driver,10).until(lambda driver: title.is_displayed())print(title)time.sleep(3)# 关闭窗口driver.close()driver.switch_to.window(current_window)time.sleep(2)print(driver.find_element_by_css_selector(&#39;#footer span&#39;).text)</code></pre></li></ol><pre><code># 关闭浏览器# driver.quit()```* frame切换```pythonfrom selenium import webdriverimport osdriver = webdriver.Chrome()driver.get(&#39;file:///&#39; + os.path.abspath(&#39;6.outframe.html&#39;))# frame切换driver.switch_to.frame(driver.find_element_by_id(&#39;out&#39;))driver.switch_to.frame(&#39;in&#39;)driver.find_element_by_id(&#39;kw&#39;).send_keys(&#39;selenium&#39;)driver.find_element_by_id(&#39;su&#39;).click()```</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>奇书网</title>
    <link href="undefined2018/07/30/2018-07-30-%E5%A5%87%E4%B9%A6%E7%BD%91/"/>
    <url>2018/07/30/2018-07-30-%E5%A5%87%E4%B9%A6%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<p>奇书网小说爬取</p><blockquote><p>作业：奇书网 ，玄幻奇幻类小说<br>将小说名，点击次数，文件大小，书籍类型。更新日期，连载状态。<br>书籍作者，小说简介，下载地址存储到excel里面,</p><blockquote><p>实际获取中遇到一些坑 经常获取不到底部页面导航<br>解决:采用重传url获取本页内容</p></blockquote></blockquote><a id="more"></a><pre><code class="python">import xlwtfrom urllib.request import urlopen,Requestfrom bs4 import BeautifulSoupclass Qishu(object):    def __init__(self):        self.base_url = &#39;https://www.qisuu.la/soft/sort01/index_1.html&#39;        self.headers = {            &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;        }        self.workBook = None        self.sheet = None        self.record = 1    # 主爬虫程序    def spider(self):        self.create_workBook()        self.get_data_with_url(self.base_url)        # 创建excle工作簿        self.workBook.save(&#39;奇书小说.xls&#39;)    # 请求url获取信息    def get_data_with_url(self,url):        print(&#39;正在获取:{} ...&#39;.format(url))        # 请求url        request = Request(url, headers=self.headers)        try:            response = urlopen(request).read()        except Exception as e:            print(&#39;请求本页失败&#39;,e)            print(&#39;重新获取本页:{}...&#39;.format(url))            self.get_data_with_url(url)        # print(response)        soup = BeautifulSoup(response,&#39;lxml&#39;)        # 分支语句判断能否获取本页tspage,获取不到时        # 重新获取        tspage = soup.select(&#39;.tspage a&#39;)        if len(tspage) == 0:            print(&#39;重新获取本页:{}&#39;.format(url))            self.get_data_with_url(url)        else:            # 获取一页中的所有小说条目            items = soup.select(&#39;.listBox ul li&#39;)            for item in items:                # find获取每个条目中的第一个url链接即小说详情                url = item.find(&#39;a&#39;).get(&#39;href&#39;)                # print(url)                # 获取详情页信息-----------------------------------------                self.get_sub_data_with_url(&#39;https://www.qisuu.la&#39; + url)            # 获取下一页url            try:                tspage = soup.select(&#39;.tspage a&#39;)                if tspage[-2].get_text() != &#39;下一页&#39; :                    print(&#39;没有下一页了,亲...&#39;)                    return                next_page = tspage[-2].get(&#39;href&#39;)                page_url = &#39;https://www.qisuu.la&#39;+next_page                # print(page_url)                # 传递下一页url 回调函数                self.get_data_with_url(page_url)            except Exception as e:                print(&#39;获取失败:&#39;,e)    # 详情页    def get_sub_data_with_url(self,url):        request = Request(url, headers=self.headers)        response = urlopen(request).read()        # print(response.decode())        soup = BeautifulSoup(response, &#39;lxml&#39;)        # 获取指定信息        name = soup.select(&#39;.detail_right h1&#39;)[0].get_text()        infos = soup.select(&#39;.detail_right ul .small&#39;)        list = [name]        for index in range(0,6):            # 取文本并处理数据            option = infos[index].get_text().split(&#39;：&#39;)[1]            list.append(option)        show = soup.select(&#39;.showInfo p&#39;)[0].get_text()        url = soup.select(&#39;.showDown ul li script&#39;)[0].get_text().split(&quot;&#39;&quot;)[3]        list.append(show)        list.append(url)        # 写入excel表        for index,option in enumerate(list):            self.sheet.write(self.record,index,option)        # record 控制插入位置        self.record += 1    # 创建excel工作簿    def create_workBook(self):        self.workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)        self.sheet = self.workBook.add_sheet(&#39;小说&#39;)        # 插入头信息        for index,option in enumerate([&#39;小说名称&#39;,&#39;点击次数&#39;,&#39;文件大小&#39;,&#39;书籍类型&#39;,&#39;更新日期&#39;,&#39;连载状态&#39;,&#39;书籍作者&#39;,&#39;小说简介&#39;,&#39;下载地址&#39;]):            self.sheet.write(0,index,option)# 实例化对象qishu = Qishu()qishu.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>xpath,bs4,正则,数据请求方式</title>
    <link href="undefined2018/07/28/2018-07-27-xpath,bs4,%E6%AD%A3%E5%88%99,%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F/"/>
    <url>2018/07/28/2018-07-27-xpath,bs4,%E6%AD%A3%E5%88%99,%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h3 id="引用模块"><a href="#引用模块" class="headerlink" title="引用模块"></a>引用模块</h3><pre><code class="python"># xpathfrom lxml import etree# bs4from bs4 import BeautifulSoup# 正则import re# 数据请求from urllib.request import urlopen,Requestimport requests</code></pre><a id="more"></a><h3 id="xpath和bs4和正则的区别"><a href="#xpath和bs4和正则的区别" class="headerlink" title="xpath和bs4和正则的区别"></a>xpath和bs4和正则的区别</h3><pre><code class="python"># ---------------------------xpath和bs4和正则的区别--------------------------                # 总结:                # xpath与bs4z查找标签的方式不同 见33,42                # xpath和bs4都具有get方法用于取属性的值 其中xpath也可用 @+属性名 来取属性值                # xpath使用text()取文本  BeautifulSoup使用get_text()取文本                # 正则直接正面刚# 以网站美食杰为例 &#39;http://meishij.net&#39;说明headers = {    &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;}request = Request(&#39;http://meishij.net&#39;,headers=headers)response = urlopen(request)code = response.read().decode()# print(code)# xpath    # etree将响应的字符串code转化为html的标签类型,    # 通过xpath访问标签的方式匹配查找指定的内容    # root = etree.HTML(code)root = etree.HTML(code)print(type(root))title = root.xpath(&#39;//div[@class=&quot;nav&quot;]/ul/li&#39;)[0]# strong_text = title.xpath(&#39;.//a/strong/text()&#39;) #获取标签文本内容 返回字符串# strong_text = title.xpath(&#39;@class&#39;)#获取标签内属性内容 返回列表strong_text = title.get(&#39;class&#39;) # get 获取标签内属性内容print(strong_text)# bs4soup = BeautifulSoup(code,&#39;lxml&#39;)print(type(soup))# id唯一 使用 # + id内容来访问  class不唯一 使用. + class内容来访问# 返回列表 需要[0]来使用index = soup.select(&#39;.nav ul li a&#39;)[0]print(index)attr = index.get(&#39;class&#39;) #取属性的值print(attr)text = index.get_text() #取文本print(text)# 正则pattern = re.compile(r&#39;&lt;div class=&quot;nav&quot;&gt;.*?&lt;li class=&quot;current&quot;&gt;.*?&lt;strong&gt;(.*?)&lt;/strong&gt;&#39;,re.S)result = pattern.findall(code)print(result)</code></pre><h3 id="数据请求方式的区别"><a href="#数据请求方式的区别" class="headerlink" title="数据请求方式的区别"></a>数据请求方式的区别</h3><pre><code class="python"># -------------------------数据请求方式的区别------------------------# 1.requests# 请求数据方式不同response = requests.get(&#39;http://meishij.net&#39;,headers=headers)print(response.content) # bytesprint(response.text) # str 解码后# 2.urlopen Requestrequest = Request(&#39;http://meishij.net&#39;,headers=headers)response = urlopen(request).read() #bytesresponse = response.decode() #str 解码后print(response)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>豆瓣top250</title>
    <link href="undefined2018/07/27/2018-07-27-%E8%B1%86%E7%93%A3top250/"/>
    <url>2018/07/27/2018-07-27-%E8%B1%86%E7%93%A3top250/</url>
    
    <content type="html"><![CDATA[<h2 id="获取豆瓣影评top250信息存入excel表"><a href="#获取豆瓣影评top250信息存入excel表" class="headerlink" title="获取豆瓣影评top250信息存入excel表"></a>获取豆瓣影评top250信息存入excel表</h2><blockquote><p>使用xpath获取指定内容<br>from fake_useragent import UserAgent 随机生成User-Agent</p></blockquote><a id="more"></a><pre><code class="python">import xlwtimport requestsfrom lxml import etreefrom fake_useragent import UserAgentclass DBMovie(object):    def __init__(self):        self.base_url = &#39;http://movie.douban.com/top250&#39;        self.current_page = 1        self.headers = UserAgent()        self.workBook = None        self.sheet = None    def start_load_movie(self):        self.get_excel()        # 第一次调用该方法 url值可以不用传        self.get_code_with_url()        self.workBook.save(&#39;豆瓣top250.xls&#39;)    def get_excel(self):        self.workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)        self.sheet = self.workBook.add_sheet(&#39;电影排行榜&#39;)        self.sheet.write(0,0,&#39;排名&#39;)        self.sheet.write(0,1,&#39;影片&#39;)        self.sheet.write(0,2,&#39;导演和演员&#39;)        self.sheet.write(0,3,&#39;评分&#39;)        self.sheet.write(0,4,&#39;评论人次&#39;)        self.record = 1    def get_code_with_url(self,url=&#39;&#39;):        headers = {            &#39;User-Agent&#39;:self.headers.random        }        full_url = self.base_url + url        response = requests.get(full_url,headers=headers).text        # print(response)        code = etree.HTML(response)        item_div = code.xpath(&#39;//div[@class=&quot;item&quot;]&#39;)        # print(item_div)        for tag in item_div:            # .从当前节点开始取            # ..从父节点开始取            rank = tag.xpath(&#39;.//em[@class=&quot;&quot;]/text()&#39;)[0]            movie_name = tag.xpath(&#39;.//div[@class=&quot;hd&quot;]/a/span/text()&#39;)            name = &#39;&#39;            for movie in movie_name:                name += movie            director = tag.xpath(&#39;.//div[@class=&quot;bd&quot;]/p[@class=&quot;&quot;]/text()&#39;)[0]            director = director.strip(&#39;\n&#39;).replace(&#39; &#39;,&#39;&#39;)            star = tag.xpath(&#39;.//span[@class=&quot;rating_num&quot;]/text()&#39;)[0]            comment_num = tag.xpath(&#39;.//div[@class=&quot;star&quot;]/span[last()]/text()&#39;)[0]            comment_num = comment_num[0:-3]            # print(rank,name,director,star,comment_num)            self.sheet.write(self.record,0,rank)            self.sheet.write(self.record,1,name)            self.sheet.write(self.record,2,director)            self.sheet.write(self.record,3,star)            self.sheet.write(self.record,4,comment_num)            self.record += 1        self.get_next_page(code)    def get_next_page(self,code):        next_url = code.xpath(&#39;//span[@class=&quot;next&quot;]/a/@href&#39;)        if len(next_url) == 0:            print(&#39;已经是最后一页了&#39;)            return        self.get_code_with_url(next_url[0])movie = DBMovie()movie.start_load_movie()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>美食杰/BeautifulSoup</title>
    <link href="undefined2018/07/27/2018-07-26-%E7%BE%8E%E9%A3%9F%E6%9D%B0BeautifulSoup/"/>
    <url>2018/07/27/2018-07-26-%E7%BE%8E%E9%A3%9F%E6%9D%B0BeautifulSoup/</url>
    
    <content type="html"><![CDATA[<p>美食杰网站爬取封面图片,菜谱名及人气,以 “莴笋炒蛋（2047人气)” 形式存入数据库</p><a id="more"></a><pre><code class="python">from bs4 import BeautifulSoupfrom urllib.request import Request,urlopen,urlretrieveimport os,shutilimport sqlite3# 数据库操作class DBManager(object):    con = None    cursor = None    @classmethod    def createTable(cls):        cls.con = sqlite3.connect(&#39;Msj&#39;)        cls.cursor = cls.con.cursor()        cls.cursor.execute(&#39;create table if not exists food(title text,src text)&#39;)        cls.con.commit()    @classmethod    def insert_data(cls,title,src):        cls.cursor.execute(&#39;insert into food VALUES (&quot;{}&quot;,&quot;{}&quot;)&#39;.format(title,src))        cls.con.commit()    @classmethod    def closeDB(cls):        cls.cursor.close()        cls.con.close()class Msj(object):    def __init__(self):        self.base_url = &#39;https://www.meishij.net/chufang/diy/?&amp;page=1&#39;        self.DBmanager = DBManager()        self.pagedir = 1    def spider(self):        self.DBmanager.createTable()        if os.path.exists(&#39;D:/meishij&#39;):            shutil.rmtree(&#39;D:/meishij&#39;)        os.mkdir(&#39;D:/meishij&#39;)        os.chdir(&#39;D:/meishij&#39;)        self.get_code_with_url(self.base_url)        self.DBmanager.closeDB()    def get_code_with_url(self,url):        headers = {            &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0&#39;        }        request = Request(url,headers=headers)        response = urlopen(request)        try:            code = response.read().decode()            self.get_page_with_code(code)        except Exception as e:            print(&#39;获取失败:&#39;,e)    def get_page_with_code(self,code):        # print(code)        soup = BeautifulSoup(code,&#39;lxml&#39;)        info = soup.select(&#39;.clearfix img&#39;)        hots = soup.select(&#39;.c1 span&#39;)        print(info,hots)        os.mkdir(&#39;Page{}&#39;.format(self.pagedir))        os.chdir(&#39;Page{}&#39;.format(self.pagedir))        for data,hot in zip(info,hots):            title = data.get(&#39;alt&#39;).replace(&#39;|&#39;,&#39;&#39;)            src = data.get(&#39;src&#39;)            hot = hot.get_text().replace(&#39; &#39;,&#39;&#39;).split(&#39;论&#39;)[1]            print(title,src,hot)            self.DBmanager.insert_data(title+&#39;(&#39;+hot+&#39;)&#39;,src)            urlretrieve(src,title+&#39;(&#39;+hot+&#39;).jpg&#39;)        os.chdir(os.path.pardir)        next_page = soup.select(&#39;.next&#39;)        if next_page:            next_page = next_page[0]            next_page_url = next_page.get(&#39;href&#39;)            self.pagedir +=1            print(next_page)            self.get_code_with_url(next_page_url)msj = Msj()msj.spider()</code></pre><blockquote><p>老师:</p></blockquote><pre><code class="python">import requestsimport lxmlfrom bs4 import BeautifulSoupimport sqlite3from fake_useragent import UserAgentclass MeishiDB(object):    con = None    cursor = None    def openDB(self):        self.con = sqlite3.connect(&#39;meishiDB&#39;)        self.cursor = self.con.cursor()        self.cursor.execute(&#39;create table if not exists MeiShiTable (name text,src text)&#39;)        self.con.commit()    def add_info_to_db(self,name,src):        self.cursor.execute(&#39;insert into MeiShiTable VALUES (&quot;{}&quot;,&quot;{}&quot;)&#39;.format(name,src))        self.con.commit()    def close(self):        self.cursor.close()        self.con.close()class MeiShiJie(object):    def __init__(self):        self.headers = UserAgent()        self.DB = MeishiDB()    def spider(self):        # 打开数据库和创建数据表        self.DB.openDB()        code = self.get_code_with_url(&#39;https://www.meishij.net/chufang/diy/&#39;)        self.DB.close()    def get_code_with_url(self,url):        headers = {            &#39;User-Agent&#39;:self.headers.random        }        response = requests.get(url,headers=headers).text        # print(response)        code = BeautifulSoup(response,&#39;lxml&#39;)        self.get_content_with_code(code)    def get_content_with_code(self,code):        divList = code.select(&#39;div.listtyle1&#39;)        for div in divList:            img_alt = div.select(&#39;img&#39;)[0][&#39;alt&#39;]            print(img_alt)            img_src = div.select(&#39;img&#39;)[0][&#39;src&#39;]            print(img_src)            self.DB.add_info_to_db(img_alt,img_src)        self.get_next_page_with_code(code)    def get_next_page_with_code(self,code):        next_url = code.select(&#39;a.next&#39;)        if len(next_url) == 0:            print(&#39;最后一页&#39;)            return        self.get_code_with_url(next_url[0][&#39;href&#39;])meishi = MeiShiJie()meishi.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>美食杰模拟登陆</title>
    <link href="undefined2018/07/26/2018-07-25-%E7%BE%8E%E9%A3%9F%E6%9D%B0%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86/"/>
    <url>2018/07/26/2018-07-25-%E7%BE%8E%E9%A3%9F%E6%9D%B0%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86/</url>
    
    <content type="html"><![CDATA[<p>Charles抓包分析</p><blockquote><p>美食节 使用cookie模拟登陆 获取网页源码 使用xpath得到用户信息 存储到excel表格</p></blockquote><a id="more"></a><pre><code class="python">from http.cookiejar import CookieJar,LWPCookieJarfrom urllib.request import Request,build_opener,HTTPCookieProcessorfrom urllib.parse import urlencodefrom lxml import etreeimport xlwt# 获取cookiefileName = &#39;meishij.txt&#39;# 使用继承自CookieJar类的LWPCookieJar类实例化一个管理cookie的对象# 并传入file那么参数cookie_obj = LWPCookieJar(filename=fileName)# 实例化一个支持cookie的扩展类对象cookie_handler = HTTPCookieProcessor(cookie_obj)# 构建一个功能更多的请求url的方式opener = build_opener(cookie_handler)response = opener.open(&#39;https://i.meishi.cc/login.php?redirect=https%3A%2F%2Fi.meishi.cc%2Flogin.php%3Fac%3Dzhuce&#39;)# 保存cookiecookie_obj.save(ignore_discard=True,ignore_expires=True)# 使用本地cookie模拟用户登陆cookie = LWPCookieJar()cookie.load(&#39;meishij.txt&#39;)cookie_handler = HTTPCookieProcessor(cookie)opener = build_opener(cookie_handler)post_url = &#39;https://i.meishi.cc/login.php?redirect=https%3A%2F%2Fi.meishi.cc%2Flogin.php%3Fac%3Dzhuce&#39;headers = {    &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0&#39;}data = urlencode({    &#39;username&#39;:&#39;********&#39;, #用户名     &#39;password&#39;:&#39;********&#39;  #密码})request = Request(post_url,headers=headers)response = opener.open(request,bytes(data,encoding=&#39;utf-8&#39;))code = response.read().decode()root = etree.HTML(code)print(root)name_list = root.xpath(&#39;//div[@class=&quot;info1&quot;]/h1/text()[1]&#39;)name = name_list[0].replace(&#39;\t&#39;,&#39;&#39;)grade_list = root.xpath(&#39;//span[@class=&quot;info&quot;]/em[1]/a/text()&#39;)grade = grade_list[0]other_list = root.xpath(&#39;//span[@class=&quot;info&quot;]/em/text()&#39;)workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)sheet = workBook.add_sheet(&#39;info_meishij&#39;)sheet.write(0,0,name)sheet.write(0,1,grade)sheet.write(0,2,other_list[0])sheet.write(0,3,other_list[1])sheet.write(0,4,other_list[2])workBook.save(&#39;meishij.xls&#39;)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>前程无忧51Job</title>
    <link href="undefined2018/07/25/2018-07-24-%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A751Job/"/>
    <url>2018/07/25/2018-07-24-%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A751Job/</url>
    
    <content type="html"><![CDATA[<p>爬取前程无忧的岗位信息并存入Excel表<a id="more"></a></p><blockquote><p>爬取内容:</p><blockquote><p>岗位 公司名称 地址 薪资 发布时间</p></blockquote></blockquote><pre><code class="python">#https://search.51job.com/list/020000%252C030200,000000,0000,00,9,99,python,2,1.htmlfrom urllib.request import Request,urlopenfrom urllib.parse import quoteimport reimport string,jsonimport xlwtclass job_51(object):    def __init__(self,cityList,job):        self.cityList = cityList        self.job = job        self.cityName = &#39;&#39;        self.headers = {            &quot;User-Agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;        }        self.first_page = 1        self.total_page = 1        for city in cityList:            cityDigitDict = self.get_cityDigit()            if city == cityList[-1]:                self.cityName += cityDigitDict[city]            else:                self.cityName += cityDigitDict[city] + &#39;%252C&#39;            # print(self.cityName)    def get_cityDigit(self):        url = &#39;https://js.51jobcdn.com/in/js/2016/layer/area_array_c.js?20180319=&#39;        request = Request(url,headers=self.headers)        response = urlopen(request)        code = response.read().decode(&#39;gbk&#39;)        # print(code)        pattern = re.compile(r&#39;area=(.*?);&#39;,re.S)        result = pattern.findall(code)        result = json.loads(result[0])        dict = {}        for key,value in result.items():            key = key.strip(&#39;\r&#39;)            key = key.strip(&#39;\n&#39;)            dict[value] = key        return dict    def get_total_page(self,code):        pattern = re.compile(r&#39;&lt;span class=&quot;td&quot;&gt;共(.*?)页.*?&lt;/span&gt;&#39;,re.S)        result = pattern.findall(code)        return (int(result[0]))    def spider(self):        code = self.get_code_from_url(self.first_page)        # print(code)        self.total_page = self.get_total_page(code)        sheet,workBook = self.openExeclBook()        record = 1        for index in range(1,self.total_page+1):            code = self.get_code_from_url(index)            dataList = self.get_total_data(code)            for index in dataList:                sheet.write(record,0,index[0])                sheet.write(record,1,index[1])                sheet.write(record,2,index[2])                sheet.write(record,3,index[3])                sheet.write(record,4,index[4])                record +=1        workBook.save(&#39;python职位介绍表.xls&#39;)    def get_total_data(self,code):        pattern = re.compile(r&#39;&lt;div class=&quot;el&quot;.*?&lt;span.*?&lt;a target.*?&gt;(.*?)&lt;/a&gt;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&lt;span class.*?&gt;(.*?)&lt;/span&gt;.*?&lt;span class.*?&gt;(.*?)&lt;/span&gt;.*?&lt;span class.*?&gt;(.*?)&lt;/span&gt;&#39;,re.S)        pattern_space = re.compile(r&#39;\s&#39;, re.S)        result = pattern.findall(code)        list = []        for job, company, add, salary, time in result:            job = job.strip(&#39;\r\n&#39;)            job = re.sub(pattern_space, &#39;&#39;, job)            list.append([job, company, add, salary, time])        return list    def openExeclBook(self):        workBook = xlwt.Workbook(encoding=&#39;utf-8&#39;)        sheet = workBook.add_sheet(&#39;python职位表_51job&#39;)        sheet.write(0,0,&#39;职位名&#39;)        sheet.write(0,1,&#39;公司名&#39;)        sheet.write(0,2,&#39;工作地点&#39;)        sheet.write(0,3,&#39;薪资&#39;)        sheet.write(0,4,&#39;发布时间&#39;)        return sheet, workBook    def get_code_from_url(self,indexPage):        url = &#39;https://search.51job.com/list/{},000000,0000,00,9,99,{},2,{}.html&#39;.format(self.cityName,self.job,indexPage)        request = Request(url,headers=self.headers)        response = urlopen(request)        code = response.read().decode(&#39;gbk&#39;)        return codecityList = []while True:    city = input(&#39;请输入你要查询的城市,按E退出:&#39;)    if city == &#39;E&#39;:        break    cityList.append(city)job = input(&#39;请输入你要查询的职位:&#39;)spider = job_51(cityList,job)spider.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人人美剧懒加载</title>
    <link href="undefined2018/07/25/2018-07-24-%E4%BA%BA%E4%BA%BA%E7%BE%8E%E5%89%A7%E6%87%92%E5%8A%A0%E8%BD%BD/"/>
    <url>2018/07/25/2018-07-24-%E4%BA%BA%E4%BA%BA%E7%BE%8E%E5%89%A7%E6%87%92%E5%8A%A0%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<blockquote><p>这种做法效率有点低😂 还有点蠢……</p></blockquote><a id="more"></a><pre><code class="python"># http://www.zimuzu.io/articlefrom urllib.request import Request,urlopenimport reclass Zimuzu(object):    def __init__(self):        self.base_url = &#39;http://www.zimuzu.io/article/?page=&#39;        self.headers = {            &quot;User-Agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;        }        self.first_page = 1        self.total_page = 1    def spider(self):        code = self.get_code_from_url(self.first_page)        self.total_page = self.get_total_page(code)        self.get_data_from_code(code)        self.get_page_step()    def get_page_step(self):        index = 1        while True:            signal = input(&#39;回车翻页,按E退出&#39;)            if signal !=&quot;E&quot;:                code = self.get_code_from_url(index)                data = self.get_data_from_code(code)                index += 1                print(data)            else:                break    def get_data_from_code(self,code):        pattern = re.compile(r&#39;&lt;li class=&quot;clearfix&quot;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&lt;p&gt;(.*?)&lt;font.*?&gt;&#39;,re.S)        data = pattern.findall(code)        return data    def get_total_page(self,code):        pattern = re.compile(r&#39;&lt;div class=&quot;pages pages-padding&quot;&gt;.*?&lt;b&gt;.*?&lt;/b&gt;.*?&lt;/a&gt;.*?&lt;a href=.*?page=(.*?)&gt;.*?&lt;span&gt;&#39;,re.S)        result = pattern.findall(code)        result = result[0][0:-1]        return (int(result))    def get_code_from_url(self,indexPage):        url = self.base_url + str(indexPage)        request = Request(url ,headers=self.headers)        response = urlopen(request)        code = response.read().decode()        return codez = Zimuzu()z.spider()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>糗事百科爬虫</title>
    <link href="undefined2018/07/20/2018-07-19-%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91%E7%88%AC%E8%99%AB/"/>
    <url>2018/07/20/2018-07-19-%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91%E7%88%AC%E8%99%AB/</url>
    
    <content type="html"><![CDATA[<p>糗事百科爬虫</p><a id="more"></a><pre><code class="python">import  refrom urllib.request import Request,urlopenbase_url = &#39;https://www.qiushibaike.com/hot/page/&#39;headers = {    &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;}ip_list = [    &#39;223.241.116.66:18118&#39;,    &#39;125.120.200.49:6666&#39;,    &#39;221.228.17.172:8181&#39;,    &#39;122.72.18.34:80&#39;,    &#39;218.241.234.48:8080&#39;,    &#39;120.26.110.59:8080&#39;]def down_load_qiubai_info(pageIndex):    full_url = base_url + str(pageIndex) +&#39;/&#39;    request = Request(full_url,headers = headers)    response = urlopen(request)    # 获取对应网页的全部内容    # decode()解码    code = response.read().decode()    # 正则匹配的内容  从制定的开始位置  到全部内容结束    # 所以只需要指定开始的位置  不需要指定结束的位置    #    # 如果我们想要正则获取某一对标签里面的内容的时候    # 那么需要将这对标签对写完整 而且在想要获取的内容    # 上添加() 例如: &lt;h2&gt;(.*?)&lt;/h2&gt;    # pattern = re.compile(r&#39;&lt;div class=&quot;author clearfix&quot;&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div class=&quot;articleGender .*?Icon&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;a href=&quot;(.*?)&quot;&#39;,re.S)    # result = pattern.findall(code)    # # print(result)    # for name,age,article_url in result:    #     # 去掉开始和结尾的换行符    #     name = name.strip(&#39;\n&#39;)    #     article_url = article_url.strip(&#39;\n&#39;).replace(&#39;&lt;br/&gt;&#39;,&#39;&#39;)    #     print(&#39;作者是:&#39;,name)    #     print(&#39;年龄是:&#39;,age)    #     print(&#39;内容是:&#39;,article_url)    pattern = re.compile(r&#39;&lt;div class=&quot;author clearfix&quot;&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div class=&quot;articleGender.*?Icon&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;a.*?href=&quot;(.*?)&quot;.*?&gt;.*?&lt;div class=&quot;content&quot;&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;.*?&lt;div class=&quot;stats&quot;&gt;.*?&lt;i class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;span class=&quot;stats-comments&quot;&gt;.*?&lt;i class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&#39;,re.S)    result = pattern.findall(code)    for name,age,href,content,number,comments in result:        name = name.strip(&#39;\n&#39;)        age = age.strip(&#39;\n&#39;)        href = href.strip(&#39;\n&#39;)        content = content.strip(&#39;\n&#39;)        number = number.strip(&#39;\n&#39;)        comments = comments.strip(&#39;\n&#39;)        print(&#39;作者是:&#39;,name)        print(&#39;年龄是:&#39;,age)        print(&#39;详情是:&#39;,href)        print(&#39;内容是:&#39;,content)        print(&#39;好笑数:&#39;,number)        print(&#39;评论数:&#39;,comments)        if int(comments) !=0:            get_all_comment_with(href)        else:            print(&#39;该内容暂无评论&#39;)def get_all_comment_with(url):    detail_url = &#39;https://www.qiushibaike.com&#39; + url    print(detail_url)    request = Request(detail_url,headers=headers)    response = urlopen(request)    code = response.read().decode()    # print(code)    pattern = re.compile(r&#39;&lt;div class=&quot;replay&quot;.*?&lt;a href=&quot;(.*?)&quot; class=&quot;userlogin&quot;.*?title=&quot;(.*?)&quot;&gt;.*?&lt;span class=&quot;body&quot;&gt;(.*?)&lt;/span&gt;&#39;,re.S)    result = pattern.findall(code)    print(result)    print(&#39;---------------------------------------------------------------&#39;)for i in range(1,14):    down_load_qiubai_info(i)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bing壁纸爬取</title>
    <link href="undefined2018/07/19/2018-07-18-Bing%E5%A3%81%E7%BA%B8%E7%88%AC%E5%8F%96/"/>
    <url>2018/07/19/2018-07-18-Bing%E5%A3%81%E7%BA%B8%E7%88%AC%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<p>本次爬取采用<a href="https://github.com/xCss/bing" target="_blank" rel="noopener" title="GitHub">xCss</a>大大提供的接口</p><a id="more"></a><pre><code class="python">from urllib.request import urlopenimport timepname = 0day = 0while True:    day +=1    url = &#39;https://bing.ioliu.cn/v1?d={}&amp;w=1280&#39;.format(day)    response = urlopen(url)    responseStr = response.read()    pname +=1    with open(&#39;C:/Users/45125/Downloads/Bing_1280/{}.jpg&#39;.format(pname),&#39;wb&#39;)as f:        f.write(responseStr)        f.close()    time.sleep(0.5)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>B站视频信息爬取</title>
    <link href="undefined2018/07/18/2018-07-10-B%E7%AB%99%E8%A7%86%E9%A2%91%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96/"/>
    <url>2018/07/18/2018-07-10-B%E7%AB%99%E8%A7%86%E9%A2%91%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<p>B站视频信息爬取,分析b站视频信息得到的api接口</p><a id="more"></a><pre><code class="python">from urllib.request import urlopenfrom urllib.parse import quoteimport string,json,time# url:http://api.bilibili.com/x/web-interface/archive/stat?aid=10001# {&quot;code&quot;:0,&quot;message&quot;:&quot;0&quot;,&quot;ttl&quot;:1,&quot;data&quot;:{&quot;aid&quot;:10001,&quot;view&quot;:22760,&quot;danmaku&quot;:195,&quot;reply&quot;:191,&quot;favorite&quot;:173,&quot;coin&quot;:21,&quot;share&quot;:47,&quot;like&quot;:38,&quot;now_rank&quot;:0,&quot;his_rank&quot;:0,&quot;no_reprint&quot;:0,&quot;copyright&quot;:2}}i = 10001with open(&#39;bili.txt&#39;, &#39;a&#39;, encoding=&#39;utf-8&#39;)as f:    while i&gt;=1000:        i+=1        url = &#39;http://api.bilibili.com/x/web-interface/archive/stat?aid={}&#39;.format(i)        reponse = urlopen(url)        reponseStr = reponse.read()        reponseJson = json.loads(reponseStr)        print(reponseJson)        time.sleep(0.5)        if reponseJson[&#39;code&#39;] == 0:            biliList = []            for index in reponseJson[&#39;data&#39;].items():                biliList.append(str(index)+&#39;,&#39;)            biliList.pop()            biliList.append(&#39;\n&#39;)            for j in biliList:                f.write(str(j))            time.sleep(0.5)    f.close()</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>爬虫</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>markdown_sync</title>
    <link href="undefined2018/07/17/2018-07-13-markdown_sync/"/>
    <url>2018/07/17/2018-07-13-markdown_sync/</url>
    
    <content type="html"><![CDATA[<p>该文件用来测试和展示书写README的各种markdown语法。<a id="more"></a>GitHub的markdown语法在标准的markdown语法基础上做了扩充，称之为<code>GitHub Flavored Markdown</code>。简称<code>GFM</code>，GFM在GitHub上有广泛应用，除了README文件外，issues和wiki均支持markdown语法。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#横线">横线</a></li><li><a href="#标题">标题</a></li><li><a href="#文本">文本</a><ul><li>普通文本</li><li>单行文本</li><li>多行文本</li><li>文字高亮</li><li>换行</li><li>斜体</li><li>粗体</li><li>删除线</li></ul></li><li><a href="#图片">图片</a><ul><li>来源于网络的图片</li><li>GitHub仓库中的图片</li></ul></li><li><a href="#链接">链接</a> <ul><li>文字超链接<ul><li>链接外部URL</li><li>链接本仓库里的URL</li></ul></li><li>锚点</li><li><a href="#图片链接">图片链接</a></li></ul></li><li><a href="#列表">列表</a><ul><li>无序列表</li><li>有序列表</li><li>复选框列表</li></ul></li><li><a href="#块引用">块引用</a></li><li><a href="#代码高亮">代码高亮</a></li><li><a href="#表格">表格</a> </li><li><a href="#表情">表情</a></li><li><a href="#diff语法">diff语法</a></li></ul><h3 id="横线"><a href="#横线" class="headerlink" title="横线"></a>横线</h3><hr><p>***、—、___可以显示横线效果</p><hr><hr><hr><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><h3 id="普通文本"><a href="#普通文本" class="headerlink" title="普通文本"></a>普通文本</h3><p>这是一段普通的文本</p><h3 id="单行文本"><a href="#单行文本" class="headerlink" title="单行文本"></a>单行文本</h3><pre><code>Hello,大家好，我是果冻虾仁。</code></pre><p>在一行开头加入1个Tab或者4个空格。</p><h3 id="文本块"><a href="#文本块" class="headerlink" title="文本块"></a>文本块</h3><h4 id="语法1"><a href="#语法1" class="headerlink" title="语法1"></a>语法1</h4><p>在连续几行的文本开头加入1个Tab或者4个空格。</p><pre><code>欢迎到访很高兴见到您祝您，早上好，中午好，下午好，晚安</code></pre><h4 id="语法2"><a href="#语法2" class="headerlink" title="语法2"></a>语法2</h4><p>使用一对各三个的反引号：</p><pre><code>欢迎到访我是C++码农你可以在知乎、CSDN、简书搜索【果冻虾仁】找到我</code></pre><p>该语法也可以实现代码高亮，见<a href="#代码高亮">代码高亮</a></p><h3 id="文字高亮"><a href="#文字高亮" class="headerlink" title="文字高亮"></a>文字高亮</h3><p>文字高亮功能能使行内部分文字高亮，使用一对反引号。<br>语法：</p><pre><code>`linux` `网络编程` `socket` `epoll` </code></pre><p>效果：<code>linux</code> <code>网络编程</code> <code>socket</code> <code>epoll</code></p><p>也适合做一篇文章的tag</p><h4 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h4><p>直接回车不能换行，<br>可以在上一行文本后面补两个空格，<br>这样下一行的文本就换行了。</p><p>或者就是在两行文本直接加一个空行。</p><p>也能实现换行效果，不过这个行间距有点大。</p><h4 id="斜体、粗体、删除线"><a href="#斜体、粗体、删除线" class="headerlink" title="斜体、粗体、删除线"></a>斜体、粗体、删除线</h4><table><thead><tr><th>语法</th><th>效果</th></tr></thead><tbody><tr><td><code>*斜体1*</code></td><td><em>斜体1</em></td></tr><tr><td><code>_斜体2_</code></td><td><em>斜体2</em></td></tr><tr><td><code>**粗体1**</code></td><td><strong>粗体1</strong></td></tr><tr><td><code>__粗体2__</code></td><td><strong>粗体2</strong></td></tr><tr><td><code>这是一个 ~~删除线~~</code></td><td>这是一个 <del>删除线</del></td></tr><tr><td><code>***斜粗体1***</code></td><td><strong><em>斜粗体1</em></strong></td></tr><tr><td><code>___斜粗体2___</code></td><td><strong><em>斜粗体2</em></strong></td></tr><tr><td><code>***~~斜粗体删除线1~~***</code></td><td><strong><em><del>斜粗体删除线1</del></em></strong></td></tr><tr><td><code>~~***斜粗体删除线2***~~</code></td><td><del><strong><em>斜粗体删除线2</em></strong></del></td></tr></tbody></table><pre><code>斜体、粗体、删除线可混合使用</code></pre><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>基本格式：</p><pre><code>![alt](URL title)</code></pre><p>alt和title即对应HTML中的alt和title属性（都可省略）：</p><ul><li>alt表示图片显示失败时的替换文本</li><li>title表示鼠标悬停在图片时的显示文本（注意这里要加引号）</li></ul><p>URL即图片的url地址，如果引用本仓库中的图片，直接使用<strong>相对路径</strong>就可了，如果引用其他github仓库中的图片要注意格式，即：<code>仓库地址/raw/分支名/图片路径</code>，如：</p><pre><code>https://github.com/guodongxiaren/ImageCache/raw/master/Logo/foryou.gif</code></pre><table><thead><tr><th>#</th><th>语法</th><th>效果</th></tr></thead><tbody><tr><td>1</td><td><code>![baidu](http://www.baidu.com/img/bdlogo.gif &quot;百度logo&quot;)</code></td><td><img src="http://www.baidu.com/img/bdlogo.gif" srcset="/img/loading.gif" alt="baidu" title="百度logo"></td></tr><tr><td>2</td><td><code>![][foryou]</code></td><td><img src="https://github.com/guodongxiaren/ImageCache/raw/master/Logo/foryou.gif" srcset="/img/loading.gif" alt=""></td></tr></tbody></table><p>注意例2的写法使用了<strong>URL标识符</strong>的形式，在<a href="#链接">链接</a>一节有介绍。</p><blockquote><p>在文末有foryou的定义：</p></blockquote><pre><code>[foryou]:https://github.com/guodongxiaren/ImageCache/raw/master/Logo/foryou.gif</code></pre><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><h3 id="链接外部URL"><a href="#链接外部URL" class="headerlink" title="链接外部URL"></a>链接外部URL</h3><table><thead><tr><th>#</th><th>语法</th><th>效果</th></tr></thead><tbody><tr><td>1</td><td><code>[我的博客](http://blog.csdn.net/guodongxiaren &quot;悬停显示&quot;)</code></td><td><a href="http://blog.csdn.net/guodongxiaren" target="_blank" rel="noopener" title="悬停显示">我的博客</a></td></tr><tr><td>2</td><td><code>[我的知乎][zhihu]</code></td><td><a href="https://www.zhihu.com/people/jellywong" target="_blank" rel="noopener" title="我的知乎，欢迎关注">我的知乎</a></td></tr></tbody></table><p>语法2由两部分组成：</p><ul><li>第一部分使用两个中括号，[ ]里的标识符（本例中zhihu），可以是数字，字母等的组合，标识符上下对应就行了（<strong>姑且称之为URL标识符</strong>）</li><li>第二部分标记实际URL。</li></ul><blockquote><p>使用URL标识符能达到复用的目的，一般把全文所有的URL标识符统一放在文章末尾，这样看起来比较干净。</p><blockquote><p>URL标识符是我起的名字，不知道是否准确。囧。。</p></blockquote></blockquote><h3 id="链接本仓库里的URL"><a href="#链接本仓库里的URL" class="headerlink" title="链接本仓库里的URL"></a>链接本仓库里的URL</h3><table><thead><tr><th>语法</th><th>效果</th></tr></thead><tbody><tr><td><code>[我的简介](/example/profile.md)</code></td><td><a href="/example/profile.md">我的简介</a></td></tr><tr><td><code>[example](./example)</code></td><td><a href="./example">example</a></td></tr></tbody></table><h3 id="图片链接"><a href="#图片链接" class="headerlink" title="图片链接"></a>图片链接</h3><p>给图片加链接的本质是混合图片显示语法和普通的链接语法。普通的链接中[ ]内部是链接要显示的文本，而图片链接[ ]里面则是要显示的图片。<br>直接混合两种语法当然可以，但是十分啰嗦，为此我们可以使用URL标识符的形式。</p><table><thead><tr><th>#</th><th>语法</th><th align="center">效果</th></tr></thead><tbody><tr><td>1</td><td><code>[![weibo-logo]](http://weibo.com/linpiaochen)</code></td><td align="center"><a href="http://weibo.com/linpiaochen" target="_blank" rel="noopener"><img src="/img/weibo.png" srcset="/img/loading.gif" alt="weibo-logo" title="点击图片进入我的微博"></a></td></tr><tr><td>2</td><td><code>[![](/img/zhihu.png &quot;我的知乎，欢迎关注&quot;)][zhihu]</code></td><td align="center"><a href="https://www.zhihu.com/people/jellywong" target="_blank" rel="noopener" title="我的知乎，欢迎关注"><img src="/img/zhihu.png" srcset="/img/loading.gif" alt="" title="我的知乎，欢迎关注"></a></td></tr><tr><td>3</td><td><code>[![csdn-logo]][csdn]</code></td><td align="center"><a href="http://blog.csdn.net/guodongxiaren" target="_blank" rel="noopener" title="我的博客"><img src="/img/csdn.png" srcset="/img/loading.gif" alt="csdn-logo" title="我的CSDN博客"></a></td></tr></tbody></table><p>因为图片本身和链接本身都支持URL标识符的形式，所以图片链接也可以很简洁（见例3）。<br>注意，此时鼠标悬停时显示的文字是图片的title，而非链接本身的title了。</p><blockquote><p>本文URL标识符都放置于文末</p></blockquote><h3 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a>锚点</h3><p>其实呢，每一个标题都是一个锚点，和HTML的锚点（<code>#</code>）类似，比如我们 </p><table><thead><tr><th>语法</th><th>效果</th></tr></thead><tbody><tr><td><code>[回到顶部](#readme)</code></td><td><a href="#readme">回到顶部</a></td></tr></tbody></table><p>不过要注意，标题中的英文字母都被转化为<strong>小写字母</strong>了。</p><blockquote><p>以前GitHub对中文支持的不好，所以中文标题不能正确识别为锚点，但是现在已经没问题啦！</p></blockquote><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><ul><li>昵称：果冻虾仁</li></ul><ul><li>别名：隔壁老王</li></ul><ul><li>英文名：Jelly</li></ul><h3 id="多级无序列表"><a href="#多级无序列表" class="headerlink" title="多级无序列表"></a>多级无序列表</h3><ul><li>编程语言<ul><li>脚本语言<ul><li>Python</li></ul></li></ul></li></ul><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><h4 id="一般效果"><a href="#一般效果" class="headerlink" title="一般效果"></a>一般效果</h4><p>就是在数字后面加一个点，再加一个空格。不过看起来起来可能不够明显。<br>面向对象的三个基本特征：</p><ol><li>封装</li><li>继承</li><li>多态</li></ol><h4 id="多级有序列表"><a href="#多级有序列表" class="headerlink" title="多级有序列表"></a>多级有序列表</h4><p>和无序列表一样，有序列表也有多级结构：  </p><ol><li>这是一级的有序列表，数字1还是1<ol><li>这是二级的有序列表，阿拉伯数字在显示的时候变成了罗马数字<ol><li>这是三级的有序列表，数字在显示的时候变成了英文字母</li></ol></li></ol></li></ol><h3 id="复选框列表"><a href="#复选框列表" class="headerlink" title="复选框列表"></a>复选框列表</h3><ul><li><input checked="" disabled="" type="checkbox"> 需求分析</li><li><input checked="" disabled="" type="checkbox"> 系统设计</li><li><input checked="" disabled="" type="checkbox"> 详细设计</li><li><input disabled="" type="checkbox"> 编码</li><li><input disabled="" type="checkbox"> 测试</li><li><input disabled="" type="checkbox"> 交付</li></ul><p>您可以使用这个功能来标注某个项目各项任务的完成情况。</p><blockquote><p>Tip:</p><blockquote><p>在GitHub的<strong>issue</strong>中使用该语法是可以实时点击复选框来勾选或解除勾选的，而无需修改issue原文。</p></blockquote></blockquote><h2 id="块引用"><a href="#块引用" class="headerlink" title="块引用"></a>块引用</h2><h3 id="常用于引用文本"><a href="#常用于引用文本" class="headerlink" title="常用于引用文本"></a>常用于引用文本</h3><h4 id="文本摘自《深入理解计算机系统》P27"><a href="#文本摘自《深入理解计算机系统》P27" class="headerlink" title="文本摘自《深入理解计算机系统》P27"></a>文本摘自《深入理解计算机系统》P27</h4><p>　令人吃惊的是，在哪种字节顺序是合适的这个问题上，人们表现得非常情绪化。实际上术语“little endian”（小端）和“big endian”（大端）出自Jonathan Swift的《格利佛游记》一书，其中交战的两个派别无法就应该从哪一端打开一个半熟的鸡蛋达成一致。因此，争论沦为关于社会政治的争论。只要选择了一种规则并且始终如一的坚持，其实对于哪种字节排序的选择都是任意的。</p><blockquote><p><strong>“端”（endian）的起源</strong><br>以下是Jonathan Swift在1726年关于大小端之争历史的描述：<br>“……下面我要告诉你的是，Lilliput和Blefuscu这两大强国在过去36个月里一直在苦战。战争开始是由于以下的原因：我们大家都认为，吃鸡蛋前，原始的方法是打破鸡蛋较大的一端，可是当今的皇帝的祖父小时候吃鸡蛋，一次按古法打鸡蛋时碰巧将一个手指弄破了，因此他的父亲，当时的皇帝，就下了一道敕令，命令全体臣民吃鸡蛋时打破较小的一端，违令者重罚。”</p></blockquote><h3 id="块引用有多级结构"><a href="#块引用有多级结构" class="headerlink" title="块引用有多级结构"></a>块引用有多级结构</h3><blockquote><p>数据结构</p><blockquote><p>树</p><blockquote><p>二叉树</p><blockquote><p>平衡二叉树</p><blockquote><p>满二叉树</p></blockquote></blockquote></blockquote></blockquote></blockquote><h2 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h2><p>在三个反引号后面加上编程语言的名字，另起一行开始写代码，最后一行再加上三个反引号。</p><pre><code class="Java">public static void main(String[]args){} //Java</code></pre><pre><code class="c">int main(int argc, char *argv[]) //C</code></pre><pre><code class="Bash">echo &quot;hello GitHub&quot; #Bash</code></pre><pre><code class="javascript">document.getElementById(&quot;myH1&quot;).innerHTML=&quot;Welcome to my Homepage&quot;; //javascipt</code></pre><pre><code class="cpp">string &amp;operator+(const string&amp; A,const string&amp; B) //cpp</code></pre><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><table><thead><tr><th>表头1</th><th>表头2</th></tr></thead><tbody><tr><td>表格单元</td><td>表格单元</td></tr><tr><td>表格单元</td><td>表格单元</td></tr></tbody></table><table><thead><tr><th>表头1</th><th>表头2</th></tr></thead><tbody><tr><td>表格单元</td><td>表格单元</td></tr><tr><td>表格单元</td><td>表格单元</td></tr></tbody></table><h3 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h3><p>表格可以指定对齐方式</p><table><thead><tr><th align="left">左对齐</th><th align="center">居中</th><th align="right">右对齐</th></tr></thead><tbody><tr><td align="left">col 3 is</td><td align="center">some wordy text</td><td align="right">$1600</td></tr><tr><td align="left">col 2 is</td><td align="center">centered</td><td align="right">$12</td></tr><tr><td align="left">zebra stripes</td><td align="center">are neat</td><td align="right">$1</td></tr></tbody></table><h3 id="混合其他语法"><a href="#混合其他语法" class="headerlink" title="混合其他语法"></a>混合其他语法</h3><p>表格单元中的内容可以和其他大多数GFM语法配合使用，如：  </p><h4 id="使用普通文本的删除线，斜体等效果"><a href="#使用普通文本的删除线，斜体等效果" class="headerlink" title="使用普通文本的删除线，斜体等效果"></a>使用普通文本的删除线，斜体等效果</h4><table><thead><tr><th>名字</th><th>描述</th></tr></thead><tbody><tr><td>Help</td><td><del>Display the</del> help window.</td></tr><tr><td>Close</td><td><em>Closes</em> a window</td></tr></tbody></table><h4 id="表格中嵌入图片（链接）"><a href="#表格中嵌入图片（链接）" class="headerlink" title="表格中嵌入图片（链接）"></a>表格中嵌入图片（链接）</h4><p>其实前面介绍图片显示、图片链接的时候为了清晰就是放在在表格中显示的。</p><table><thead><tr><th>图片</th><th>描述</th></tr></thead><tbody><tr><td><img src="http://www.baidu.com/img/bdlogo.gif" srcset="/img/loading.gif" alt="baidu" title="百度logo"></td><td>百度</td></tr></tbody></table><h2 id="表情"><a href="#表情" class="headerlink" title="表情"></a>表情</h2><p>Github的Markdown语法支持添加emoji表情，输入不同的符号码（两个冒号包围的字符）可以显示出不同的表情。</p><p>比如<code>:blush:</code>，可以显示:blush:。</p><p>具体每一个表情的符号码，可以查询GitHub的官方网页<a href="http://www.emoji-cheat-sheet.com" target="_blank" rel="noopener">http://www.emoji-cheat-sheet.com</a>。</p><p>但是这个网页每次都打开<strong>奇慢</strong>。。所以我整理到了本repo中，大家可以直接在此查看<a href="./emoji.md">emoji</a>。</p><h2 id="diff语法"><a href="#diff语法" class="headerlink" title="diff语法"></a>diff语法</h2><p>版本控制的系统中都少不了diff的功能，即展示一个文件内容的增加与删除。<br>GFM中可以显示的展示diff效果。使用绿色表示新增，红色表示删除。</p><p>其语法与代码高亮类似，只是在三个反引号后面写diff，<br>并且其内容中，以 <code>+</code>开头表示新增，<code>-</code>开头表示删除。</p><p>效果如下：</p><pre><code class="diff">+ 鸟宿池边树，僧敲月下门- 鸟宿池边树，僧推月下门</code></pre><hr>]]></content>
    
    
    
    <tags>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python常见错误</title>
    <link href="undefined2018/07/17/2018-07-16-python%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    <url>2018/07/17/2018-07-16-python%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="python常见错误"><a href="#python常见错误" class="headerlink" title="python常见错误"></a>python常见错误</h2><p>1.SyntaxError: ‘return’ outside function</p><pre><code class="python">while True:    count += 1    if count ==20:        return</code></pre><blockquote><p>语法错误:return 不能在方法以外执行</p><blockquote><p>解决:将return放进方法体中</p></blockquote></blockquote> <a id="more"></a><p>2.IndentationError: expected an indented block</p><pre><code class="python">if count == 0:print(count)else:  print(&#39;nothing&#39;)</code></pre><blockquote><p>缩进错误:未正确缩进</p><blockquote><p>解决:正确缩进</p></blockquote></blockquote><p>3.IndexError: string index out of range</p><pre><code class="python">content = &#39;hello&#39;print(content[10])</code></pre><blockquote><p>索引错误:字符串超出范围</p><blockquote><p>解决:查看字符串长度,索引要小于字符串长度</p></blockquote></blockquote><p>4.ValueError: substring not found</p><pre><code class="python">content = &#39;Hello World&#39;result = content.index(&#39;z&#39;)print(result)</code></pre><blockquote><p>值错误:子字符串未找到</p><blockquote><p>解决://</p></blockquote></blockquote><p>5.AttributeError: ‘tuple’ object has no attribute ‘remove’</p><pre><code class="python">tp1 = ((),[],{},1,2,3,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,3.14,True)print(tp1)</code></pre><blockquote><p>attribute 属性 object 对象</p><blockquote><p>属性错误</p></blockquote></blockquote><p>6.KeyError: ‘fond’</p><pre><code class="python">dict1 = {    &#39;name&#39;:&#39;张三&#39;,    &#39;age&#39;:17,    &#39;friend&#39;:[&#39;李四&#39;,&#39;王五&#39;,&#39;赵六&#39;,&#39;冯七&#39;]}print(dict1[&#39;fond&#39;])</code></pre><blockquote><p>key错误 没有指定的键值’fond’</p></blockquote><p>7.TypeError: pop expected at least 1 arguments, got 0</p><pre><code class="python">dict1 = {&#39;a&#39;:1,&#39;b&#39;:2}dict1.pop()</code></pre><blockquote><p>arguments 参数  expected 期望 at least 至少</p><blockquote><p>类型错误:pop方法希望得到至少一个参数 但现在参数为0</p></blockquote></blockquote><p>8.SyntaxError: invalid syntax</p><pre><code class="python">if True    print(1)</code></pre><blockquote><p>语法错误</p><blockquote><p>解决:True后面加上:</p></blockquote></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>