<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="">
  <meta name="author" content="Zhangyu">
  <meta name="keywords" content="">
  <title>scrapy帮助 ~ zyboy</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >
<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>zyboy</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/default2.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期六, 八月 11日 2018, 8:30 晚上
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    1.8k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      8 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <blockquote>
<p>爬虫文件目录spider下,创建任意名称的<code>.py</code>文件写入如下内容,右键运行,可在pycharm中模拟命令行终端</p>
</blockquote>
<a id="more"></a>

<pre><code class="python">form scrapy import cmdline

# blabla为爬虫文件的`name`值
cmdline.execute(&quot;scrapy crawl blabla&quot;.split())</code></pre>
<blockquote>
<p>爬虫文件中的<code>name</code>,<code>allowed_domains</code>和<code>start_urls</code>:</p>
</blockquote>
<pre><code class="python">name = &#39;&#39;   --------------# 爬虫文件spider的名称
allowed_domains = [&#39;&#39;] ----------------# 允许爬取的域名
# 通常会修改爬虫程序的start_urls
start_urls = [&#39;&#39;] -----------------# 开始爬取的网址,多个网址时,多线程进行请求</code></pre>
<!-- more -->

<blockquote>
<p><em>爬取网站过程</em></p>
<ol>
<li><code>items.py</code>文件定义数据模型,也就是用于存储主爬虫程序爬取的内容的字段</li>
<li>然后在爬虫文件中导入<code>from ..items import</code> 自动生成的爬虫item类</li>
<li>初始化数据模型对象,对数据模型字段赋值,<code>yield</code>逐条返回item</li>
<li>返回的<code>item</code>进入管道文件<code>pipelines</code>进行处理和返回,以及自定义管道</li>
<li><code>middlewares</code>中间件多用于请求数据,同时自定义设置请求头信息,自定义中间件</li>
<li><code>settings</code>文件用于配置项目信息,包括请求信息,管道调用和优先级,延迟delay,中间件等等</li>
</ol>
</blockquote>
<blockquote>
<p> 主爬虫文件中的<code>def parse(self,response)</code>函数</p>
</blockquote>
<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy


class BaiduspiderSpider(scrapy.Spider):
    # 爬虫名
    name = &#39;baiduSpider&#39;
    # 允许爬虫的范围
    # allowed_domains = [&#39;baidu.com&#39;]
    start_urls = [&#39;http://www.baidu.com/&#39;]

    # response 为scrapy的默认下载中间请求start_urls后返回的结果
    def parse(self, response):
        # 请求的响应文本
        # print(response.text)
        # body为响应体
        # print(response.body)
        # 响应头
        # print(response.headers)
        # 获取当前状态
        # print(response.status)
        code = response.body.decode()
        # 获取的response可直接使用xpath获取内容
        result = response.xpath(&#39;.//div[@class=&quot;cover&quot;]/@cover-text&#39;)
        # 仍能对其使用xpath取数据
        result = result.xpath(......)
        # 此时`result`为`[&#39;selector=&lt;...&gt;,data=&lt;...&gt;&#39;]`样式
        result = result.extract()  # 使其转化为列表
        result = result[0]    # 取第一个元素
        # 也可以一步到位:
        # retult = response.xpath(&#39;.//div[@class=&quot;cover&quot;]/@cover-text&#39;).extract_first(&#39;&#39;)

        # 将items.py作为模块中导入其中的class类`MokoItem`
        from ..items import MokoItem
        for ul in ul_list:
            # 初始化一个对象MokoItem类
            item = MokoItem()
            # 为数据模型中的字段赋值
            item[&#39;title&#39;] = retult

            # 逐个数据模型每个item对象
            yield item</code></pre>
<blockquote>
<p>在控制台运行爬虫时可以同时保存返回的item为本地文件</p>
</blockquote>
<pre><code class="python"># 将文件存储为指定类型 支持四种数据类型text,json,xml,csv
crapy crwl meikong -o meikong.xml

# 转换编码
scrapy crawl meikong -o mei.json -s FEED_EXPORT_ENCODING=UTF-8</code></pre>
<blockquote>
<p><code>settings</code>部分配置说明</p>
</blockquote>
<pre><code class="python">BOT_NAME = &#39;baidu&#39;

# 爬虫所在地
SPIDER_MODULES = [&#39;baidu.spiders&#39;]
NEWSPIDER_MODULE = &#39;baidu.spiders&#39;

# 遵守爬虫协议
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
# 最大请求并发量 默认16
# CONCURRENT_REQUESTS = 32

# configure 配置 请求延迟
# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
# 是否使用cookie
#COOKIES_ENABLED = False

# 请求头信息,可添加用户表示User-Agent
DEFAULT_REQUEST_HEADERS = {
  &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
  &#39;Accept-Language&#39;: &#39;en&#39;
}

SPIDER_MIDDLEWARES = {
        # 值越小,优先级越高,优先级越高,越先执行
   &#39;baidu.middlewares.BaiduSpiderMiddleware&#39;: 543,
}
# 下载中间件
OWNLOADER_MIDDLEWARES = {
        # 值越小,优先级越高,优先级越高,越先执行
   &#39;baidu.middlewares.BaiduDownloaderMiddleware&#39;: 543,
}

# Enable or disable extensions 是否进行扩展
# See https://doc.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,
#}

ITEM_PIPELINES = {
    # 值越小,优先级越高,优先级越高,越先执行
   &#39;baidu.pipelines.BaiduPipeline&#39;: 1,
}
</code></pre>
<blockquote>
<p>主爬虫文件中图片和文件链接字段<code>[]</code>和回调函数<code>callback=</code>.</p>
<p>`settings.py’文件中使用系统文件,图片处理管道下载图片和文件.</p>
</blockquote>
<ul>
<li>spider.py<pre><code class="pthon"># -*- coding: utf-8 -*-
import scrapy
from ..items import ImagenetItem
</code></pre>
</li>
</ul>
<p>class ImageSpider(scrapy.Spider):<br>    name = ‘image’<br>    allowed_domains = [‘pic.netbian.com’]<br>    # 请求最开始的url<br>    start_urls = [‘<a href="http://pic.netbian.com/4kmeishi/&#39;]" target="_blank" rel="noopener">http://pic.netbian.com/4kmeishi/&#39;]</a></p>
<pre><code>def parse(self, response):
    # 根据响应来找到制定的内容 现在找的是img的src属性
    img_list = response.xpath(&#39;//ul[@class=&quot;clearfix&quot;]/li/a/img/@src&#39;)
    # print(img_list)
    # 找到多个属性值 遍历
    for img in img_list:
        # 使用在items.py中定义的数据模型item
        item = ImagenetItem()
        img_src =&#39;http://pic.netbian.com&#39; + img.extract()
        # print(img_src)
        # 将下载地址放入数据模型中
        # 下载地址要包在list当中
        item[&#39;img_src&#39;] = [img_src]
        item[&#39;fileUrl&#39;] = [fileUrl]
        # 将数据传输给管道
        yield item

    next_url = response.xpath(&#39;//div[@class=&quot;page&quot;]/a[text()=&quot;下一页&quot;]/@href&#39;).extract()
    if len(next_url) != 0:
        url = &#39;http://pic.netbian.com&#39; + next_url[0]
        # 将url传给scrapy.Request  得到的结果继续用self.parse继续处理
        yield scrapy.Request(url,callback=self.parse)</code></pre><pre><code>
* settings.py
```python
ITEM_PIPELINES = {
   # &#39;imageNet.pipelines.ImagenetPipeline&#39;: 300,
   #  scrapy中专门负责图片下载的管道
    &#39;scrapy.pipelines.images.ImagesPipeline&#39; : 1,
    # 文件下载管道
    &#39;scrapy.pipelines.files.FilesPipeline&#39;:2
}
# 体片的存储路径
IMAGES_STORE = &#39;D:/imageDownLoad&#39;
# 图片的下载地址 根据item中的字段来设置哪一个内容需要被下载
IMAGES_URLS_FIELD = &#39;img_src&#39;

FILES_STORE = &#39;d:/qishu/book/&#39;
FILES_URLS_FIELD = &#39;fileUrl&#39;</code></pre><blockquote>
<p>自定义管道文件将items字段保存到<code>sqlite</code>数据库</p>
<p>注意<code>settings.py</code>文件中启用管道</p>
</blockquote>
<pre><code class="python"># -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
import sqlite3

class HongxiutianxiangPipeline(object):
    def process_item(self, item, spider):
        return item

class HongXiuDBPipeline(object):
    def open_spider(self,spider):
        self.con = sqlite3.connect(&#39;hongxiuDB&#39;)
        self.cursor = self.con.cursor()
        self.cursor.execute(&#39;create table if not exists bookTable(name text,author text,img text,intro text)&#39;)
        self.con.commit()
    def process_item(self,item,spider):
        print(&#39;--------------------------------------&#39;)
        self.cursor.execute(&#39;insert into bookTable VALUES (&quot;{}&quot;,&quot;{}&quot;,&quot;{}&quot;,&quot;{}&quot;)&#39;.format(item[&#39;name&#39;],item[&#39;author&#39;],item[&#39;img&#39;],item[&#39;intro&#39;]))
        self.con.commit()
        return item
    def close_spider(self,spider):
        self.cursor.close()
        self.con.close()</code></pre>
<blockquote>
<p>自定义管道保存items到json文件中</p>
</blockquote>
<pre><code class="python"># -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html

# 用来打开指定文件,并且对文件进行转码 防止出现乱码问题
import codecs
import json
import os
class XiaoshuoPipeline(object):
    def __init__(self):
        # w 写文件
        # w+读写文件 r+ 读写文件
        # 前者读写文件 如果文件不存在则创建
        # 后者读写文件 如果文件不存在 则抛出异常
        self.file = codecs.open(filename=&#39;book.json&#39;,mode=&#39;w+&#39;,encoding=&#39;utf-8&#39;)
        self.file.write(&#39;&quot;list&quot;:[&#39;)
    # 如果想要将数据写入本地或者使用数据库的时候 这个方法需要保留
    def process_item(self, item, spider):
        # 将item对象转化成一个字典对象
        res = dict(item)
        # dumps将字典对象转化成字符串 ascii编码是否可用
        # 如果直接将字典形式的数据写入到文件当中  会发生错误
        # 所以需要将字典形式的值,转化为字符串写入文件
        str  = json.dumps(res,ensure_ascii=False)
        # 将数据写入到文件当中
        self.file.write(str)
        self.file.write(&#39;,\n&#39;)
        return item
    def open_spider(self,spider):
        print(&#39;爬虫开始&#39;)
    def close_spider(self,spider):
        # 删除文件当中最后一个字符
        # -1 表示偏移量
        # SEEK_END 定位到文件最后一个字符
        self.file.seek(-1,os.SEEK_END)
        # 开始执行
        self.file.truncate()

        self.file.seek(-1,os.SEEK_END)
        self.file.truncate()

        self.file.write(&#39;]&#39;)
        self.file.close()
        print(&#39;爬虫结束&#39;)
</code></pre>
<!-- <meta http-equiv="refresh" content="1"> -->
            <hr>
          </div>
          <br>
          <div>
            <p>
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
                
                  <a class="hover-with-bg" href="/tags/scrapy">scrapy</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "scrapy帮助&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>
